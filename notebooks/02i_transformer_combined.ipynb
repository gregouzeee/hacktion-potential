{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 02i : Transformer Combined\n",
    "## Dropout + Bruit Gaussien + Feasibility + Clustering Hiérarchique 3 zones + 5-Fold 30 Epochs\n",
    "\n",
    "Ce notebook combine toutes les améliorations des notebooks précédents :\n",
    "\n",
    "1. **Spike dropout (15%)** + **Bruit gaussien (std=0.5)** sur les waveforms (02d)\n",
    "2. **Feasibility loss** : pénalise les prédictions hors du couloir du U-maze (02d_bis)\n",
    "3. **Classification hiérarchique 3 zones** + régression conditionnelle (02h)\n",
    "4. **5-fold cross-validation**, 30 epochs max, early stopping patience=7\n",
    "\n",
    "**Architecture** : SpikeTransformerHierarchical\n",
    "- Backbone partagé (CNN encoders + Transformer 2 layers)\n",
    "- Classification 3 zones (gauche / haut / droite)\n",
    "- 3 heads de régression conditionnelle (mu + sigma par zone)\n",
    "- Prédiction de distance curviligne d (tâche auxiliaire)\n",
    "- Mélange pondéré par softmax pour la prédiction finale\n",
    "\n",
    "**Loss combinée** :\n",
    "```\n",
    "L = CrossEntropy(zone) + Σ GaussianNLL(par zone, masqué) + λ_d * MSE(d) + λ_feas * Feasibility\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports et configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Reproductibilité\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Device\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device('mps')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "print(f'Device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_DIR = os.path.join(os.path.abspath('..'), 'data')\n",
    "\n",
    "PARQUET_NAME = \"M1199_PAG_stride4_win108_test.parquet\"\n",
    "JSON_NAME = \"M1199_PAG.json\"\n",
    "\n",
    "PARQUET_FILE = os.path.join(LOCAL_DIR, PARQUET_NAME)\n",
    "JSON_FILE = os.path.join(LOCAL_DIR, JSON_NAME)\n",
    "\n",
    "if not os.path.exists(PARQUET_FILE):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Données introuvables dans {LOCAL_DIR}/\\n\"\n",
    "        f\"Lancez d'abord: python download_data.py\"\n",
    "    )\n",
    "\n",
    "print(f\"Chargement depuis {LOCAL_DIR}/\")\n",
    "df = pd.read_parquet(PARQUET_FILE)\n",
    "with open(JSON_FILE, \"r\") as f:\n",
    "    params = json.load(f)\n",
    "\n",
    "print(f\"Shape: {df.shape}\")\n",
    "\n",
    "nGroups = params['nGroups']\n",
    "nChannelsPerGroup = [params[f'group{g}']['nChannels'] for g in range(nGroups)]\n",
    "print(f\"nGroups={nGroups}, nChannelsPerGroup={nChannelsPerGroup}\")\n",
    "\n",
    "# Filtrage speedMask\n",
    "speed_masks = np.array([x[0] for x in df['speedMask']])\n",
    "df_moving = df[speed_masks].reset_index(drop=True)\n",
    "print(f'Exemples en mouvement : {len(df_moving)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Géométrie du U-maze, distance curviligne et labels de zone\n",
    "\n",
    "**Squelette central** : 3 segments formant un U\n",
    "1. (0.15, 0) → (0.15, 0.90) — bras gauche\n",
    "2. (0.15, 0.90) → (0.85, 0.90) — couloir haut\n",
    "3. (0.85, 0.90) → (0.85, 0) — bras droit\n",
    "\n",
    "**3 zones** basées sur la distance curviligne d :\n",
    "- Gauche (classe 0) : d < seuil_1\n",
    "- Haut (classe 1) : seuil_1 ≤ d < seuil_2\n",
    "- Droite (classe 2) : d ≥ seuil_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Squelette du U ---\n",
    "SKELETON_SEGMENTS = np.array([\n",
    "    [0.15, 0.0, 0.15, 0.90],   # Segment 1 : bras gauche (bas → haut)\n",
    "    [0.15, 0.90, 0.85, 0.90],  # Segment 2 : couloir haut (gauche → droite)\n",
    "    [0.85, 0.90, 0.85, 0.0],   # Segment 3 : bras droit (haut → bas)\n",
    "])\n",
    "\n",
    "CORRIDOR_HALF_WIDTH = 0.15\n",
    "\n",
    "SEGMENT_LENGTHS = np.array([\n",
    "    np.sqrt((s[2]-s[0])**2 + (s[3]-s[1])**2) for s in SKELETON_SEGMENTS\n",
    "])\n",
    "TOTAL_LENGTH = SEGMENT_LENGTHS.sum()\n",
    "CUMULATIVE_LENGTHS = np.concatenate([[0], np.cumsum(SEGMENT_LENGTHS)])\n",
    "\n",
    "# Seuils de zone\n",
    "D_LEFT_END = CUMULATIVE_LENGTHS[1] / TOTAL_LENGTH\n",
    "D_RIGHT_START = CUMULATIVE_LENGTHS[2] / TOTAL_LENGTH\n",
    "\n",
    "N_ZONES = 3\n",
    "ZONE_NAMES = ['Gauche', 'Haut', 'Droite']\n",
    "\n",
    "print(f'Longueurs des segments : {SEGMENT_LENGTHS}')\n",
    "print(f'Longueur totale du U : {TOTAL_LENGTH:.2f}')\n",
    "print(f'Seuils : Gauche d<{D_LEFT_END:.4f}, Haut {D_LEFT_END:.4f}-{D_RIGHT_START:.4f}, Droite d>={D_RIGHT_START:.4f}')\n",
    "\n",
    "\n",
    "def project_point_on_segment(px, py, x1, y1, x2, y2):\n",
    "    \"\"\"Projette un point (px, py) sur le segment [(x1,y1), (x2,y2)].\"\"\"\n",
    "    dx, dy = x2 - x1, y2 - y1\n",
    "    seg_len_sq = dx**2 + dy**2\n",
    "    if seg_len_sq < 1e-12:\n",
    "        return 0.0, np.sqrt((px - x1)**2 + (py - y1)**2), x1, y1\n",
    "    t = ((px - x1) * dx + (py - y1) * dy) / seg_len_sq\n",
    "    t = np.clip(t, 0.0, 1.0)\n",
    "    proj_x = x1 + t * dx\n",
    "    proj_y = y1 + t * dy\n",
    "    dist = np.sqrt((px - proj_x)**2 + (py - proj_y)**2)\n",
    "    return t, dist, proj_x, proj_y\n",
    "\n",
    "\n",
    "def compute_curvilinear_distance(x, y):\n",
    "    \"\"\"Distance curviligne normalisée d ∈ [0, 1] le long du U.\"\"\"\n",
    "    best_dist = np.inf\n",
    "    best_d = 0.0\n",
    "    for i, (x1, y1, x2, y2) in enumerate(SKELETON_SEGMENTS):\n",
    "        t, dist, _, _ = project_point_on_segment(x, y, x1, y1, x2, y2)\n",
    "        if dist < best_dist:\n",
    "            best_dist = dist\n",
    "            best_d = (CUMULATIVE_LENGTHS[i] + t * SEGMENT_LENGTHS[i]) / TOTAL_LENGTH\n",
    "    return best_d\n",
    "\n",
    "\n",
    "def compute_distance_to_skeleton(x, y):\n",
    "    \"\"\"Distance minimale du point (x, y) au squelette du U.\"\"\"\n",
    "    best_dist = np.inf\n",
    "    for x1, y1, x2, y2 in SKELETON_SEGMENTS:\n",
    "        _, dist, _, _ = project_point_on_segment(x, y, x1, y1, x2, y2)\n",
    "        best_dist = min(best_dist, dist)\n",
    "    return best_dist\n",
    "\n",
    "\n",
    "def d_to_zone(d):\n",
    "    \"\"\"Convertit une distance curviligne d en label de zone (0, 1, 2).\"\"\"\n",
    "    if d < D_LEFT_END:\n",
    "        return 0\n",
    "    elif d < D_RIGHT_START:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "\n",
    "# --- Calcul de d et zone_labels pour tous les exemples ---\n",
    "positions = np.array([[x[0], x[1]] for x in df_moving['pos']], dtype=np.float32)\n",
    "curvilinear_d = np.array([\n",
    "    compute_curvilinear_distance(x, y) for x, y in positions\n",
    "], dtype=np.float32)\n",
    "zone_labels = np.array([d_to_zone(d) for d in curvilinear_d], dtype=np.int64)\n",
    "\n",
    "print(f'\\nd curviligne : min={curvilinear_d.min():.4f}, max={curvilinear_d.max():.4f}, mean={curvilinear_d.mean():.4f}')\n",
    "print(f'\\nDistribution des zones :')\n",
    "for z in range(N_ZONES):\n",
    "    count = (zone_labels == z).sum()\n",
    "    print(f'  {ZONE_NAMES[z]:8s} (classe {z}) : {count} ({count / len(zone_labels):.1%})')\n",
    "\n",
    "dist_to_skel = np.array([compute_distance_to_skeleton(x, y) for x, y in positions])\n",
    "print(f'\\nDistance au squelette : mean={dist_to_skel.mean():.4f}, max={dist_to_skel.max():.4f}')\n",
    "print(f'  % dans le couloir (dist < {CORRIDOR_HALF_WIDTH}) : {(dist_to_skel < CORRIDOR_HALF_WIDTH).mean():.1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualisation : d + classification 3 zones ---\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 6))\n",
    "\n",
    "# 1. Squelette\n",
    "axes[0].scatter(positions[:, 0], positions[:, 1], c='lightgray', s=1, alpha=0.3)\n",
    "for x1, y1, x2, y2 in SKELETON_SEGMENTS:\n",
    "    axes[0].plot([x1, x2], [y1, y2], 'r-', linewidth=3)\n",
    "axes[0].set_xlabel('X'); axes[0].set_ylabel('Y')\n",
    "axes[0].set_title('Squelette du U sur les positions')\n",
    "axes[0].set_aspect('equal')\n",
    "\n",
    "# 2. d coloré\n",
    "sc = axes[1].scatter(positions[:, 0], positions[:, 1], c=curvilinear_d, s=1, alpha=0.5, cmap='viridis')\n",
    "plt.colorbar(sc, ax=axes[1], label='d (distance curviligne normalisée)')\n",
    "axes[1].set_xlabel('X'); axes[1].set_ylabel('Y')\n",
    "axes[1].set_title('Distance curviligne d le long du U')\n",
    "axes[1].set_aspect('equal')\n",
    "\n",
    "# 3. Classification 3 zones\n",
    "zone_colors = ['blue', 'green', 'red']\n",
    "for z in range(N_ZONES):\n",
    "    mask_z = zone_labels == z\n",
    "    axes[2].scatter(positions[mask_z, 0], positions[mask_z, 1], c=zone_colors[z], s=1, alpha=0.3, label=f'{ZONE_NAMES[z]}')\n",
    "axes[2].set_xlabel('X'); axes[2].set_ylabel('Y')\n",
    "axes[2].set_title(f'Classification 3 zones')\n",
    "axes[2].legend(markerscale=10)\n",
    "axes[2].set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Histogramme de d avec seuils\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.hist(curvilinear_d, bins=100, alpha=0.7, edgecolor='black', linewidth=0.3)\n",
    "ax.axvline(x=D_LEFT_END, color='blue', linestyle='--', linewidth=2, label=f'Gauche/Haut ({D_LEFT_END:.3f})')\n",
    "ax.axvline(x=D_RIGHT_START, color='red', linestyle='--', linewidth=2, label=f'Haut/Droite ({D_RIGHT_START:.3f})')\n",
    "ax.set_xlabel('d (distance curviligne)'); ax.set_ylabel('Count')\n",
    "ax.set_title('Distribution de d + seuils de zone')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocessing : reconstruction de la séquence chronologique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_sequence(row, nGroups, nChannelsPerGroup, max_seq_len=128):\n",
    "    \"\"\"Reconstruit la séquence chronologique de spikes.\"\"\"\n",
    "    groups = row['groups']\n",
    "    length = min(len(groups), max_seq_len)\n",
    "    waveforms = {}\n",
    "    for g in range(nGroups):\n",
    "        nCh = nChannelsPerGroup[g]\n",
    "        raw = row[f'group{g}']\n",
    "        waveforms[g] = raw.reshape(-1, nCh, 32)\n",
    "    seq_waveforms = []\n",
    "    seq_shank_ids = []\n",
    "    for t in range(length):\n",
    "        g = int(groups[t])\n",
    "        idx = int(row[f'indices{g}'][t])\n",
    "        if idx > 0 and idx <= waveforms[g].shape[0]:\n",
    "            seq_waveforms.append((waveforms[g][idx - 1], g))\n",
    "            seq_shank_ids.append(g)\n",
    "    return seq_waveforms, seq_shank_ids\n",
    "\n",
    "wf, sids = reconstruct_sequence(df_moving.iloc[0], nGroups, nChannelsPerGroup)\n",
    "print(f'Premier exemple : {len(wf)} spikes, shanks={set(sids)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset PyTorch (avec zone_label et d curviligne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 128\n",
    "MAX_CHANNELS = max(nChannelsPerGroup)  # 6\n",
    "\n",
    "class SpikeSequenceDataset(Dataset):\n",
    "    def __init__(self, dataframe, nGroups, nChannelsPerGroup, curvilinear_d, zone_labels, max_seq_len=MAX_SEQ_LEN):\n",
    "        self.df = dataframe\n",
    "        self.nGroups = nGroups\n",
    "        self.nChannelsPerGroup = nChannelsPerGroup\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.targets = np.array([[x[0], x[1]] for x in dataframe['pos']], dtype=np.float32)\n",
    "        self.curvilinear_d = curvilinear_d.astype(np.float32)\n",
    "        self.zone_labels = zone_labels.astype(np.int64)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        seq, shank_ids = reconstruct_sequence(row, self.nGroups, self.nChannelsPerGroup, self.max_seq_len)\n",
    "        seq_len = len(seq)\n",
    "        if seq_len == 0:\n",
    "            seq_len = 1\n",
    "            waveforms = np.zeros((1, MAX_CHANNELS, 32), dtype=np.float32)\n",
    "            shank_ids_arr = np.array([0], dtype=np.int64)\n",
    "        else:\n",
    "            waveforms = np.zeros((seq_len, MAX_CHANNELS, 32), dtype=np.float32)\n",
    "            shank_ids_arr = np.array(shank_ids, dtype=np.int64)\n",
    "            for t, (wf, g) in enumerate(seq):\n",
    "                nCh = wf.shape[0]\n",
    "                waveforms[t, :nCh, :] = wf\n",
    "        return {\n",
    "            'waveforms': torch.from_numpy(waveforms),\n",
    "            'shank_ids': torch.from_numpy(shank_ids_arr),\n",
    "            'seq_len': seq_len,\n",
    "            'target': torch.from_numpy(self.targets[idx]),\n",
    "            'd': torch.tensor(self.curvilinear_d[idx], dtype=torch.float32),\n",
    "            'zone': torch.tensor(self.zone_labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    max_len = max(item['seq_len'] for item in batch)\n",
    "    batch_size = len(batch)\n",
    "    waveforms = torch.zeros(batch_size, max_len, MAX_CHANNELS, 32)\n",
    "    shank_ids = torch.zeros(batch_size, max_len, dtype=torch.long)\n",
    "    mask = torch.ones(batch_size, max_len, dtype=torch.bool)\n",
    "    targets = torch.stack([item['target'] for item in batch])\n",
    "    d_targets = torch.stack([item['d'] for item in batch])\n",
    "    zone_targets = torch.stack([item['zone'] for item in batch])\n",
    "    for i, item in enumerate(batch):\n",
    "        sl = item['seq_len']\n",
    "        waveforms[i, :sl] = item['waveforms']\n",
    "        shank_ids[i, :sl] = item['shank_ids']\n",
    "        mask[i, :sl] = False\n",
    "    return {\n",
    "        'waveforms': waveforms, 'shank_ids': shank_ids, 'mask': mask,\n",
    "        'targets': targets, 'd_targets': d_targets, 'zone_targets': zone_targets\n",
    "    }\n",
    "\n",
    "print('Dataset et collate_fn définis.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feasibility Loss\n",
    "\n",
    "Pénalise les prédictions (x, y) hors du couloir du U. Distance différentiable au squelette :\n",
    "- `penalty = ReLU(distance - corridor_half_width)²`\n",
    "- Nulle dans le couloir, quadratique au-delà"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeasibilityLoss(nn.Module):\n",
    "    \"\"\"Pénalise les prédictions (x, y) qui tombent hors du couloir du U.\"\"\"\n",
    "    \n",
    "    def __init__(self, skeleton_segments, corridor_half_width):\n",
    "        super().__init__()\n",
    "        self.register_buffer('segments', torch.tensor(skeleton_segments, dtype=torch.float32))\n",
    "        self.corridor_half_width = corridor_half_width\n",
    "    \n",
    "    def forward(self, xy_pred):\n",
    "        px, py = xy_pred[:, 0], xy_pred[:, 1]\n",
    "        distances = []\n",
    "        for i in range(self.segments.shape[0]):\n",
    "            x1, y1, x2, y2 = self.segments[i]\n",
    "            dx, dy = x2 - x1, y2 - y1\n",
    "            seg_len_sq = dx**2 + dy**2\n",
    "            t = ((px - x1) * dx + (py - y1) * dy) / (seg_len_sq + 1e-8)\n",
    "            t = t.clamp(0.0, 1.0)\n",
    "            proj_x, proj_y = x1 + t * dx, y1 + t * dy\n",
    "            dist = torch.sqrt((px - proj_x)**2 + (py - proj_y)**2 + 1e-8)\n",
    "            distances.append(dist)\n",
    "        distances = torch.stack(distances, dim=1)\n",
    "        min_dist = distances.min(dim=1).values\n",
    "        return torch.relu(min_dist - self.corridor_half_width).pow(2).mean()\n",
    "\n",
    "\n",
    "# Test\n",
    "feas_loss = FeasibilityLoss(SKELETON_SEGMENTS, CORRIDOR_HALF_WIDTH)\n",
    "in_corridor = torch.tensor([[0.15, 0.5], [0.85, 0.3], [0.5, 0.90]], dtype=torch.float32)\n",
    "outside = torch.tensor([[0.5, 0.3], [0.5, 0.5], [0.5, 0.0]], dtype=torch.float32)\n",
    "print(f'Pénalité (dans le couloir) : {feas_loss(in_corridor).item():.6f}')\n",
    "print(f'Pénalité (hors couloir)    : {feas_loss(outside).item():.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Architecture du modèle\n",
    "\n",
    "**SpikeTransformerHierarchical** avec :\n",
    "- Spike dropout (15%) + Gaussian noise (std=0.5) en data augmentation\n",
    "- Backbone partagé : CNN encoders par shank + Transformer 2 layers\n",
    "- Classification 3 zones (softmax)\n",
    "- 3 heads de régression conditionnelle (mu + log_sigma par zone)\n",
    "- Head de distance curviligne d (Sigmoid)\n",
    "- Mélange pondéré à l'inférence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpikeEncoder(nn.Module):\n",
    "    def __init__(self, n_channels, embed_dim):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(n_channels, 32, kernel_size=5, padding=2), nn.ReLU(),\n",
    "            nn.Conv1d(32, embed_dim, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv(x).squeeze(-1)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=256):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "class SpikeTransformerHierarchical(nn.Module):\n",
    "    \"\"\"Transformer hiérarchique : classification 3 zones + régression conditionnelle.\n",
    "    Avec spike dropout et bruit gaussien en data augmentation.\"\"\"\n",
    "    \n",
    "    def __init__(self, nGroups, nChannelsPerGroup, n_zones=3, embed_dim=64, nhead=4,\n",
    "                 num_layers=2, dropout=0.2, spike_dropout=0.15, noise_std=0.5,\n",
    "                 max_channels=MAX_CHANNELS):\n",
    "        super().__init__()\n",
    "        self.nGroups = nGroups\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_zones = n_zones\n",
    "        self.max_channels = max_channels\n",
    "        self.spike_dropout = spike_dropout\n",
    "        self.noise_std = noise_std\n",
    "        \n",
    "        self.spike_encoders = nn.ModuleList([\n",
    "            SpikeEncoder(max_channels, embed_dim) for _ in range(nGroups)\n",
    "        ])\n",
    "        self.shank_embedding = nn.Embedding(nGroups, embed_dim)\n",
    "        self.pos_encoding = PositionalEncoding(embed_dim)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, nhead=nhead, dim_feedforward=embed_dim * 4,\n",
    "            dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer, num_layers=num_layers, enable_nested_tensor=False\n",
    "        )\n",
    "        \n",
    "        # Classification 3 zones\n",
    "        self.cls_head = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim), nn.ReLU(), nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim, n_zones)\n",
    "        )\n",
    "        \n",
    "        # Régression par zone\n",
    "        self.mu_heads = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(embed_dim, embed_dim), nn.ReLU(), nn.Dropout(dropout),\n",
    "                nn.Linear(embed_dim, 2)\n",
    "            ) for _ in range(n_zones)\n",
    "        ])\n",
    "        self.log_sigma_heads = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(embed_dim, embed_dim), nn.ReLU(), nn.Dropout(dropout),\n",
    "                nn.Linear(embed_dim, 2)\n",
    "            ) for _ in range(n_zones)\n",
    "        ])\n",
    "        \n",
    "        # Distance curviligne\n",
    "        self.d_head = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim), nn.ReLU(), nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim, 1), nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def _apply_spike_dropout(self, mask):\n",
    "        if not self.training or self.spike_dropout <= 0:\n",
    "            return mask\n",
    "        drop_mask = torch.rand_like(mask.float()) < self.spike_dropout\n",
    "        active = ~mask\n",
    "        new_drops = drop_mask & active\n",
    "        remaining = active & ~new_drops\n",
    "        all_dropped = remaining.sum(dim=1) == 0\n",
    "        if all_dropped.any():\n",
    "            new_drops[all_dropped] = False\n",
    "        return mask | new_drops\n",
    "    \n",
    "    def _apply_waveform_noise(self, waveforms):\n",
    "        if not self.training or self.noise_std <= 0:\n",
    "            return waveforms\n",
    "        return waveforms + torch.randn_like(waveforms) * self.noise_std\n",
    "    \n",
    "    def _encode(self, waveforms, shank_ids, mask):\n",
    "        \"\"\"Shared backbone : encode → transformer → pooling.\"\"\"\n",
    "        batch_size, seq_len = waveforms.shape[:2]\n",
    "        mask = self._apply_spike_dropout(mask)\n",
    "        waveforms = self._apply_waveform_noise(waveforms)\n",
    "        \n",
    "        embeddings = torch.zeros(batch_size, seq_len, self.embed_dim, device=waveforms.device)\n",
    "        for g in range(self.nGroups):\n",
    "            group_mask = (shank_ids == g) & (~mask)\n",
    "            if group_mask.any():\n",
    "                embeddings[group_mask] = self.spike_encoders[g](waveforms[group_mask])\n",
    "        \n",
    "        embeddings = embeddings + self.shank_embedding(shank_ids)\n",
    "        embeddings = self.pos_encoding(embeddings)\n",
    "        encoded = self.transformer(embeddings, src_key_padding_mask=mask)\n",
    "        \n",
    "        active_mask = (~mask).unsqueeze(-1).float()\n",
    "        pooled = (encoded * active_mask).sum(dim=1) / (active_mask.sum(dim=1) + 1e-8)\n",
    "        return pooled\n",
    "    \n",
    "    def forward(self, waveforms, shank_ids, mask):\n",
    "        pooled = self._encode(waveforms, shank_ids, mask)\n",
    "        cls_logits = self.cls_head(pooled)\n",
    "        mus = [head(pooled) for head in self.mu_heads]\n",
    "        sigmas = [torch.exp(head(pooled)) for head in self.log_sigma_heads]\n",
    "        d_pred = self.d_head(pooled)\n",
    "        return cls_logits, mus, sigmas, d_pred\n",
    "    \n",
    "    def predict(self, waveforms, shank_ids, mask):\n",
    "        \"\"\"Prédiction combinée via mélange pondéré par softmax.\"\"\"\n",
    "        cls_logits, mus, sigmas, d_pred = self.forward(waveforms, shank_ids, mask)\n",
    "        probs = torch.softmax(cls_logits, dim=1)  # (batch, 3)\n",
    "        \n",
    "        mu_stack = torch.stack(mus, dim=1)       # (batch, 3, 2)\n",
    "        sigma_stack = torch.stack(sigmas, dim=1)  # (batch, 3, 2)\n",
    "        p = probs.unsqueeze(-1)                   # (batch, 3, 1)\n",
    "        \n",
    "        mu = (p * mu_stack).sum(dim=1)  # (batch, 2)\n",
    "        var_combined = (p * (sigma_stack ** 2 + mu_stack ** 2)).sum(dim=1) - mu ** 2\n",
    "        sigma = torch.sqrt(var_combined.clamp(min=1e-8))\n",
    "        \n",
    "        return mu, sigma, probs, d_pred\n",
    "\n",
    "\n",
    "model = SpikeTransformerHierarchical(nGroups, nChannelsPerGroup)\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Modèle créé : {n_params:,} paramètres')\n",
    "print(f'Têtes : classification 3 zones + 3 régressions conditionnelles + d curviligne')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Split et DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split temporel 90/10\n",
    "split_idx = int(len(df_moving) * 0.9)\n",
    "df_train_full = df_moving.iloc[:split_idx].reset_index(drop=True)\n",
    "df_test = df_moving.iloc[split_idx:].reset_index(drop=True)\n",
    "d_train_full = curvilinear_d[:split_idx]\n",
    "d_test = curvilinear_d[split_idx:]\n",
    "zone_train_full = zone_labels[:split_idx]\n",
    "zone_test = zone_labels[split_idx:]\n",
    "\n",
    "print(f'Train : {len(df_train_full)} | Test : {len(df_test)}')\n",
    "for z in range(N_ZONES):\n",
    "    print(f'  {ZONE_NAMES[z]:8s} — train: {(zone_train_full == z).sum()}, test: {(zone_test == z).sum()}')\n",
    "\n",
    "N_FOLDS = 5\n",
    "kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=41)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "test_dataset = SpikeSequenceDataset(df_test, nGroups, nChannelsPerGroup, d_test, zone_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=0)\n",
    "print(f'Test: {len(test_dataset)} exemples, {len(test_loader)} batches')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Entraînement (5-Fold Cross-Validation, 30 epochs)\n",
    "\n",
    "**Hyperparamètres** :\n",
    "- embed_dim=64, nhead=4, 2 layers, dropout=0.2\n",
    "- spike_dropout=15%, noise_std=0.5\n",
    "- AdamW (lr=1e-3, weight_decay=1e-4)\n",
    "- OneCycleLR, 30 epochs, patience=7\n",
    "\n",
    "**Loss combinée** :\n",
    "```\n",
    "L = CrossEntropy(zone) + Σ GaussianNLL(par zone) + 1.0 * MSE(d) + 10.0 * Feasibility\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparamètres\n",
    "EMBED_DIM = 64\n",
    "NHEAD = 4\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.2\n",
    "SPIKE_DROPOUT = 0.15\n",
    "NOISE_STD = 0.5\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "EPOCHS = 30\n",
    "PATIENCE = 7\n",
    "LAMBDA_D = 1.0\n",
    "LAMBDA_FEAS = 10.0\n",
    "\n",
    "print(f'=== Configuration ===' )\n",
    "print(f'  Architecture : embed_dim={EMBED_DIM}, nhead={NHEAD}, layers={NUM_LAYERS}, dropout={DROPOUT}')\n",
    "print(f'  Data augmentation : spike dropout={SPIKE_DROPOUT:.0%}, gaussian noise std={NOISE_STD}')\n",
    "print(f'  Loss : CrossEntropy(cls) + GaussianNLL(par zone) + {LAMBDA_D}*MSE(d) + {LAMBDA_FEAS}*Feasibility')\n",
    "print(f'  Training : {EPOCHS} epochs, patience={PATIENCE}, LR={LR}, {N_FOLDS} folds')\n",
    "print(f'  Device : {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, scheduler, criterion_ce, criterion_nll, criterion_d, feas_loss, device):\n",
    "    model.train()\n",
    "    totals = {'loss': 0, 'cls': 0, 'pos': 0, 'd': 0, 'feas': 0, 'correct': 0, 'n': 0, 'batches': 0}\n",
    "    \n",
    "    for batch in loader:\n",
    "        wf = batch['waveforms'].to(device)\n",
    "        sid = batch['shank_ids'].to(device)\n",
    "        mask = batch['mask'].to(device)\n",
    "        targets = batch['targets'].to(device)\n",
    "        d_targets = batch['d_targets'].to(device)\n",
    "        zone_targets = batch['zone_targets'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        cls_logits, mus, sigmas, d_pred = model(wf, sid, mask)\n",
    "        \n",
    "        loss_cls = criterion_ce(cls_logits, zone_targets)\n",
    "        \n",
    "        loss_pos = torch.tensor(0.0, device=device)\n",
    "        for z in range(N_ZONES):\n",
    "            zmask = (zone_targets == z)\n",
    "            if zmask.any():\n",
    "                loss_pos = loss_pos + criterion_nll(\n",
    "                    mus[z][zmask], targets[zmask], sigmas[z][zmask] ** 2\n",
    "                )\n",
    "        \n",
    "        loss_d = criterion_d(d_pred.squeeze(-1), d_targets)\n",
    "        \n",
    "        probs = torch.softmax(cls_logits, dim=1).unsqueeze(-1)\n",
    "        mu_stack = torch.stack(mus, dim=1)\n",
    "        mu_combined = (probs * mu_stack).sum(dim=1)\n",
    "        loss_feas = feas_loss(mu_combined)\n",
    "        \n",
    "        loss = loss_cls + loss_pos + LAMBDA_D * loss_d + LAMBDA_FEAS * loss_feas\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        totals['loss'] += loss.item()\n",
    "        totals['cls'] += loss_cls.item()\n",
    "        totals['pos'] += loss_pos.item()\n",
    "        totals['d'] += loss_d.item()\n",
    "        totals['feas'] += loss_feas.item()\n",
    "        with torch.no_grad():\n",
    "            totals['correct'] += (cls_logits.argmax(dim=1) == zone_targets).sum().item()\n",
    "            totals['n'] += len(zone_targets)\n",
    "        totals['batches'] += 1\n",
    "    \n",
    "    nb = totals['batches']\n",
    "    return {k: totals[k] / nb for k in ['loss', 'cls', 'pos', 'd', 'feas']}, totals['correct'] / totals['n']\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(model, loader, criterion_ce, criterion_nll, criterion_d, feas_loss, device):\n",
    "    model.eval()\n",
    "    totals = {'loss': 0, 'cls': 0, 'pos': 0, 'd': 0, 'feas': 0, 'correct': 0, 'n': 0, 'batches': 0}\n",
    "    all_mu, all_sigma, all_probs, all_d, all_targets, all_d_targets, all_zone_targets = [], [], [], [], [], [], []\n",
    "    \n",
    "    for batch in loader:\n",
    "        wf = batch['waveforms'].to(device)\n",
    "        sid = batch['shank_ids'].to(device)\n",
    "        mask = batch['mask'].to(device)\n",
    "        targets = batch['targets'].to(device)\n",
    "        d_targets = batch['d_targets'].to(device)\n",
    "        zone_targets = batch['zone_targets'].to(device)\n",
    "        \n",
    "        mu, sigma, probs, d_pred = model.predict(wf, sid, mask)\n",
    "        cls_logits, mus, sigmas_z, _ = model(wf, sid, mask)\n",
    "        \n",
    "        loss_cls = criterion_ce(cls_logits, zone_targets)\n",
    "        loss_pos = torch.tensor(0.0, device=device)\n",
    "        for z in range(N_ZONES):\n",
    "            zmask = (zone_targets == z)\n",
    "            if zmask.any():\n",
    "                loss_pos = loss_pos + criterion_nll(mus[z][zmask], targets[zmask], sigmas_z[z][zmask] ** 2)\n",
    "        loss_d = criterion_d(d_pred.squeeze(-1), d_targets)\n",
    "        loss_feas = feas_loss(mu)\n",
    "        loss = loss_cls + loss_pos + LAMBDA_D * loss_d + LAMBDA_FEAS * loss_feas\n",
    "        \n",
    "        totals['loss'] += loss.item(); totals['cls'] += loss_cls.item()\n",
    "        totals['pos'] += loss_pos.item(); totals['d'] += loss_d.item()\n",
    "        totals['feas'] += loss_feas.item()\n",
    "        totals['correct'] += (cls_logits.argmax(dim=1) == zone_targets).sum().item()\n",
    "        totals['n'] += len(zone_targets); totals['batches'] += 1\n",
    "        \n",
    "        all_mu.append(mu.cpu().numpy()); all_sigma.append(sigma.cpu().numpy())\n",
    "        all_probs.append(probs.cpu().numpy()); all_d.append(d_pred.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy()); all_d_targets.append(d_targets.cpu().numpy())\n",
    "        all_zone_targets.append(zone_targets.cpu().numpy())\n",
    "    \n",
    "    nb = totals['batches']\n",
    "    losses = {k: totals[k] / nb for k in ['loss', 'cls', 'pos', 'd', 'feas']}\n",
    "    acc = totals['correct'] / totals['n']\n",
    "    arrays = [np.concatenate(a) for a in [all_mu, all_sigma, all_probs, all_d, all_targets, all_d_targets, all_zone_targets]]\n",
    "    return losses, acc, arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boucle d'entraînement 5-Fold\n",
    "fold_results = []\n",
    "all_train_losses = {}\n",
    "all_val_losses = {}\n",
    "all_train_losses_detail = {}  # Pour les sous-losses\n",
    "all_val_losses_detail = {}\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(df_train_full)):\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'FOLD {fold+1}/{N_FOLDS}')\n",
    "    print(f'{\"=\"*60}')\n",
    "    \n",
    "    df_ft = df_train_full.iloc[train_idx].reset_index(drop=True)\n",
    "    df_fv = df_train_full.iloc[val_idx].reset_index(drop=True)\n",
    "    \n",
    "    ds_t = SpikeSequenceDataset(df_ft, nGroups, nChannelsPerGroup, d_train_full[train_idx], zone_train_full[train_idx])\n",
    "    ds_v = SpikeSequenceDataset(df_fv, nGroups, nChannelsPerGroup, d_train_full[val_idx], zone_train_full[val_idx])\n",
    "    dl_t = DataLoader(ds_t, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=0)\n",
    "    dl_v = DataLoader(ds_v, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=0)\n",
    "    \n",
    "    print(f'  Train: {len(ds_t)}, Val: {len(ds_v)}')\n",
    "    \n",
    "    model = SpikeTransformerHierarchical(\n",
    "        nGroups, nChannelsPerGroup, n_zones=N_ZONES,\n",
    "        embed_dim=EMBED_DIM, nhead=NHEAD, num_layers=NUM_LAYERS,\n",
    "        dropout=DROPOUT, spike_dropout=SPIKE_DROPOUT, noise_std=NOISE_STD\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=LR, epochs=EPOCHS, steps_per_epoch=len(dl_t))\n",
    "    criterion_ce = nn.CrossEntropyLoss()\n",
    "    criterion_nll = nn.GaussianNLLLoss()\n",
    "    criterion_d = nn.MSELoss()\n",
    "    feas_loss_fn = FeasibilityLoss(SKELETON_SEGMENTS, CORRIDOR_HALF_WIDTH).to(DEVICE)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    train_losses, val_losses = [], []\n",
    "    train_detail, val_detail = [], []\n",
    "    model_path = f'../outputs/best_transformer_02i_fold{fold+1}.pt'\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        t_losses, t_acc = train_epoch(model, dl_t, optimizer, scheduler, criterion_ce, criterion_nll, criterion_d, feas_loss_fn, DEVICE)\n",
    "        v_losses, v_acc, _ = eval_epoch(model, dl_v, criterion_ce, criterion_nll, criterion_d, feas_loss_fn, DEVICE)\n",
    "        \n",
    "        train_losses.append(t_losses['loss'])\n",
    "        val_losses.append(v_losses['loss'])\n",
    "        train_detail.append(t_losses)\n",
    "        val_detail.append(v_losses)\n",
    "        \n",
    "        if epoch % 5 == 0 or epoch == EPOCHS - 1:\n",
    "            print(f'  Epoch {epoch+1:02d}/{EPOCHS} | Train: {t_losses[\"loss\"]:.4f} (cls={t_losses[\"cls\"]:.4f} pos={t_losses[\"pos\"]:.4f} d={t_losses[\"d\"]:.5f} feas={t_losses[\"feas\"]:.6f} acc={t_acc:.1%}) | Val: {v_losses[\"loss\"]:.4f} (acc={v_acc:.1%})')\n",
    "        \n",
    "        if v_losses['loss'] < best_val_loss:\n",
    "            best_val_loss = v_losses['loss']\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f'  Early stopping a epoch {epoch+1}')\n",
    "                break\n",
    "    \n",
    "    all_train_losses[fold] = train_losses\n",
    "    all_val_losses[fold] = val_losses\n",
    "    all_train_losses_detail[fold] = train_detail\n",
    "    all_val_losses_detail[fold] = val_detail\n",
    "    \n",
    "    # Évaluation sur la validation de ce fold\n",
    "    model.load_state_dict(torch.load(model_path, map_location=DEVICE, weights_only=True))\n",
    "    _, val_acc, (val_mu, val_sigma, val_probs, val_d_pred, val_targets, val_d_targets, val_zone_targets) = eval_epoch(\n",
    "        model, dl_v, criterion_ce, criterion_nll, criterion_d, feas_loss_fn, DEVICE\n",
    "    )\n",
    "    val_eucl = np.sqrt(((val_targets - val_mu) ** 2).sum(axis=1))\n",
    "    val_d_mae = np.abs(val_d_targets - val_d_pred.squeeze()).mean()\n",
    "    \n",
    "    val_dist_to_skel = np.array([compute_distance_to_skeleton(val_mu[i, 0], val_mu[i, 1]) for i in range(len(val_mu))])\n",
    "    pct_outside = (val_dist_to_skel > CORRIDOR_HALF_WIDTH).mean()\n",
    "    \n",
    "    fold_results.append({\n",
    "        'fold': fold + 1, 'best_val_loss': best_val_loss,\n",
    "        'val_eucl_mean': val_eucl.mean(),\n",
    "        'val_r2_x': r2_score(val_targets[:, 0], val_mu[:, 0]),\n",
    "        'val_r2_y': r2_score(val_targets[:, 1], val_mu[:, 1]),\n",
    "        'val_d_mae': val_d_mae, 'val_cls_acc': val_acc, 'val_pct_outside': pct_outside,\n",
    "        'epochs': len(train_losses),\n",
    "    })\n",
    "    print(f'  => Eucl={val_eucl.mean():.4f} | R2: X={fold_results[-1][\"val_r2_x\"]:.4f} Y={fold_results[-1][\"val_r2_y\"]:.4f} | d_MAE={val_d_mae:.4f} | cls={val_acc:.1%} | hors={pct_outside:.1%}')\n",
    "\n",
    "# Résumé\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print(f'RESUME CROSS-VALIDATION ({N_FOLDS} folds)')\n",
    "print(f'{\"=\"*60}')\n",
    "for r in fold_results:\n",
    "    print(f'  Fold {r[\"fold\"]}: Eucl={r[\"val_eucl_mean\"]:.4f} | R2_X={r[\"val_r2_x\"]:.4f} R2_Y={r[\"val_r2_y\"]:.4f} | d_MAE={r[\"val_d_mae\"]:.4f} | cls={r[\"val_cls_acc\"]:.1%} | hors={r[\"val_pct_outside\"]:.1%} | epochs={r[\"epochs\"]}')\n",
    "\n",
    "print(f'\\n  Moyenne : Eucl={np.mean([r[\"val_eucl_mean\"] for r in fold_results]):.4f} (+/- {np.std([r[\"val_eucl_mean\"] for r in fold_results]):.4f})')\n",
    "print(f'            R2_X={np.mean([r[\"val_r2_x\"] for r in fold_results]):.4f} | R2_Y={np.mean([r[\"val_r2_y\"] for r in fold_results]):.4f}')\n",
    "print(f'            cls={np.mean([r[\"val_cls_acc\"] for r in fold_results]):.1%} | hors={np.mean([r[\"val_pct_outside\"] for r in fold_results]):.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualisation de l'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Courbes de loss totale par fold ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, N_FOLDS))\n",
    "\n",
    "for fold in range(N_FOLDS):\n",
    "    axes[0].plot(all_train_losses[fold], color=colors[fold], linewidth=1.5, label=f'Fold {fold+1}')\n",
    "    axes[1].plot(all_val_losses[fold], color=colors[fold], linewidth=1.5, label=f'Fold {fold+1}')\n",
    "\n",
    "axes[0].set_xlabel('Epoch'); axes[0].set_ylabel('Loss totale')\n",
    "axes[0].set_title('Train Loss par fold'); axes[0].legend(); axes[0].grid(True, alpha=0.3)\n",
    "axes[1].set_xlabel('Epoch'); axes[1].set_ylabel('Loss totale')\n",
    "axes[1].set_title('Validation Loss par fold'); axes[1].legend(); axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Décomposition des sous-losses (moyenne sur les folds) ---\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "loss_keys = ['cls', 'pos', 'd', 'feas']\n",
    "loss_titles = ['CrossEntropy (classification)', 'GaussianNLL (régression)', 'MSE (distance curviligne d)', 'Feasibility (hors labyrinthe)']\n",
    "\n",
    "for ax_idx, (key, title) in enumerate(zip(loss_keys, loss_titles)):\n",
    "    ax = axes[ax_idx // 2, ax_idx % 2]\n",
    "    for fold in range(N_FOLDS):\n",
    "        train_vals = [d[key] for d in all_train_losses_detail[fold]]\n",
    "        val_vals = [d[key] for d in all_val_losses_detail[fold]]\n",
    "        ax.plot(train_vals, color=colors[fold], linewidth=1, alpha=0.5)\n",
    "        ax.plot(val_vals, color=colors[fold], linewidth=1.5, linestyle='--')\n",
    "    # Legend\n",
    "    ax.plot([], [], 'k-', linewidth=1, label='Train')\n",
    "    ax.plot([], [], 'k--', linewidth=1.5, label='Val')\n",
    "    ax.set_xlabel('Epoch'); ax.set_ylabel('Loss')\n",
    "    ax.set_title(title); ax.legend(); ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Évaluation finale sur le test set (ensemble 5 folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_ce = nn.CrossEntropyLoss()\n",
    "criterion_nll = nn.GaussianNLLLoss()\n",
    "criterion_d = nn.MSELoss()\n",
    "feas_loss_fn = FeasibilityLoss(SKELETON_SEGMENTS, CORRIDOR_HALF_WIDTH).to(DEVICE)\n",
    "\n",
    "all_fold_mu, all_fold_sigma, all_fold_probs, all_fold_d = [], [], [], []\n",
    "\n",
    "for fold in range(N_FOLDS):\n",
    "    model = SpikeTransformerHierarchical(\n",
    "        nGroups, nChannelsPerGroup, n_zones=N_ZONES,\n",
    "        embed_dim=EMBED_DIM, nhead=NHEAD, num_layers=NUM_LAYERS,\n",
    "        dropout=DROPOUT, spike_dropout=SPIKE_DROPOUT, noise_std=NOISE_STD\n",
    "    ).to(DEVICE)\n",
    "    model.load_state_dict(torch.load(f'../outputs/best_transformer_02i_fold{fold+1}.pt', map_location=DEVICE, weights_only=True))\n",
    "    \n",
    "    _, fold_acc, (fold_mu, fold_sigma, fold_probs, fold_d, y_test, d_test_targets, zone_test_targets) = eval_epoch(\n",
    "        model, test_loader, criterion_ce, criterion_nll, criterion_d, feas_loss_fn, DEVICE\n",
    "    )\n",
    "    all_fold_mu.append(fold_mu); all_fold_sigma.append(fold_sigma)\n",
    "    all_fold_probs.append(fold_probs); all_fold_d.append(fold_d)\n",
    "    fold_eucl = np.sqrt(((y_test - fold_mu) ** 2).sum(axis=1))\n",
    "    print(f'Fold {fold+1}: Eucl={fold_eucl.mean():.4f}, cls_acc={fold_acc:.1%}')\n",
    "\n",
    "# Ensemble\n",
    "all_fold_mu = np.stack(all_fold_mu)\n",
    "all_fold_sigma = np.stack(all_fold_sigma)\n",
    "all_fold_probs = np.stack(all_fold_probs)\n",
    "all_fold_d = np.stack(all_fold_d)\n",
    "\n",
    "y_pred = all_fold_mu.mean(axis=0)\n",
    "d_pred_ensemble = all_fold_d.mean(axis=0).squeeze()\n",
    "probs_ensemble = all_fold_probs.mean(axis=0)\n",
    "zone_pred = probs_ensemble.argmax(axis=1)\n",
    "\n",
    "# Sigma ensemble (loi de la variance totale)\n",
    "mean_var = (all_fold_sigma ** 2).mean(axis=0)\n",
    "var_mu = all_fold_mu.var(axis=0)\n",
    "y_sigma = np.sqrt(mean_var + var_mu)\n",
    "\n",
    "# Métriques\n",
    "mse_x = mean_squared_error(y_test[:, 0], y_pred[:, 0])\n",
    "mse_y = mean_squared_error(y_test[:, 1], y_pred[:, 1])\n",
    "mae_x = mean_absolute_error(y_test[:, 0], y_pred[:, 0])\n",
    "mae_y = mean_absolute_error(y_test[:, 1], y_pred[:, 1])\n",
    "r2_x = r2_score(y_test[:, 0], y_pred[:, 0])\n",
    "r2_y = r2_score(y_test[:, 1], y_pred[:, 1])\n",
    "eucl_errors = np.sqrt(((y_test - y_pred) ** 2).sum(axis=1))\n",
    "d_mae = np.abs(d_test_targets - d_pred_ensemble).mean()\n",
    "d_r2 = r2_score(d_test_targets, d_pred_ensemble)\n",
    "cls_accuracy = (zone_pred == zone_test_targets).mean()\n",
    "\n",
    "# Distance au squelette\n",
    "test_dist_to_skel = np.array([compute_distance_to_skeleton(y_pred[i, 0], y_pred[i, 1]) for i in range(len(y_pred))])\n",
    "pct_outside = (test_dist_to_skel > CORRIDOR_HALF_WIDTH).mean()\n",
    "\n",
    "zone_confusion = zone_pred != zone_test_targets\n",
    "zone_confusion_rate = zone_confusion.mean()\n",
    "\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print(f'02i Combined — Ensemble ({N_FOLDS} folds)')\n",
    "print(f'{\"=\"*60}')\n",
    "print(f'  MSE  : X={mse_x:.5f}, Y={mse_y:.5f}')\n",
    "print(f'  MAE  : X={mae_x:.4f}, Y={mae_y:.4f}')\n",
    "print(f'  R²   : X={r2_x:.4f}, Y={r2_y:.4f}')\n",
    "print(f'  Eucl : mean={eucl_errors.mean():.4f}, median={np.median(eucl_errors):.4f}, p90={np.percentile(eucl_errors, 90):.4f}')\n",
    "print(f'\\n  d curviligne : MAE={d_mae:.4f}, R²={d_r2:.4f}')\n",
    "print(f'  Classification zone : accuracy={cls_accuracy:.1%}')\n",
    "print(f'  Hors labyrinthe : {pct_outside:.1%}')\n",
    "print(f'  Confusion de zone : {zone_confusion_rate:.1%}')\n",
    "\n",
    "print(f'\\n  Erreur par zone :')\n",
    "for z in range(N_ZONES):\n",
    "    zmask = zone_test_targets == z\n",
    "    if zmask.any():\n",
    "        z_acc = (zone_pred[zmask] == z).mean()\n",
    "        print(f'    {ZONE_NAMES[z]:8s} : Eucl={eucl_errors[zmask].mean():.4f} | cls_acc={z_acc:.1%} ({zmask.sum()} pts)')\n",
    "\n",
    "print(f'\\n  Sigma moyen : X={y_sigma[:, 0].mean():.4f}, Y={y_sigma[:, 1].mean():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Visualisations standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Scatter pred vs true (X, Y, d) ---\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "axes[0].scatter(y_test[:, 0], y_pred[:, 0], s=1, alpha=0.3)\n",
    "axes[0].plot([0, 1], [0, 1], 'r--', linewidth=2)\n",
    "axes[0].set_xlabel('True X'); axes[0].set_ylabel('Predicted X')\n",
    "axes[0].set_title(f'Position X (R²={r2_x:.3f})'); axes[0].set_aspect('equal')\n",
    "\n",
    "axes[1].scatter(y_test[:, 1], y_pred[:, 1], s=1, alpha=0.3)\n",
    "axes[1].plot([0, 1], [0, 1], 'r--', linewidth=2)\n",
    "axes[1].set_xlabel('True Y'); axes[1].set_ylabel('Predicted Y')\n",
    "axes[1].set_title(f'Position Y (R²={r2_y:.3f})'); axes[1].set_aspect('equal')\n",
    "\n",
    "axes[2].scatter(d_test_targets, d_pred_ensemble, s=1, alpha=0.3)\n",
    "axes[2].plot([0, 1], [0, 1], 'r--', linewidth=2)\n",
    "axes[2].set_xlabel('True d'); axes[2].set_ylabel('Predicted d')\n",
    "axes[2].set_title(f'Distance curviligne d (R²={d_r2:.3f})'); axes[2].set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Points prédits vs vrais avec incertitude (500 premiers points) ---\n",
    "segment = slice(0, 500)\n",
    "seg_idx = np.arange(500)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Points 2D colorés par ordre chronologique\n",
    "colors_pts = np.arange(500)\n",
    "axes[0, 0].scatter(y_test[segment, 0], y_test[segment, 1], c=colors_pts, cmap='winter', s=8, alpha=0.6, label='Vraie position')\n",
    "sc = axes[0, 0].scatter(y_pred[segment, 0], y_pred[segment, 1], c=colors_pts, cmap='autumn', s=8, alpha=0.6, marker='x', label='Prediction (mu)')\n",
    "for x1, y1, x2, y2 in SKELETON_SEGMENTS:\n",
    "    axes[0, 0].plot([x1, x2], [y1, y2], 'k--', linewidth=1, alpha=0.3)\n",
    "axes[0, 0].set_xlabel('X'); axes[0, 0].set_ylabel('Y')\n",
    "axes[0, 0].set_title('Positions (500 premiers points test)')\n",
    "axes[0, 0].legend(); axes[0, 0].set_aspect('equal')\n",
    "cbar = plt.colorbar(sc, ax=axes[0, 0])\n",
    "cbar.set_label('Index temporel')\n",
    "\n",
    "# 2. Position X avec bande d'incertitude\n",
    "axes[0, 1].plot(seg_idx, y_test[segment, 0], 'b-', label='Vrai X', linewidth=1.5)\n",
    "axes[0, 1].plot(seg_idx, y_pred[segment, 0], 'r-', alpha=0.7, label='Prediction (mu)', linewidth=1)\n",
    "axes[0, 1].fill_between(seg_idx, \n",
    "                         y_pred[segment, 0] - 2 * y_sigma[segment, 0],\n",
    "                         y_pred[segment, 0] + 2 * y_sigma[segment, 0],\n",
    "                         alpha=0.2, color='red', label='Incertitude (2 sigma)')\n",
    "axes[0, 1].set_xlabel('Index'); axes[0, 1].set_ylabel('Position X')\n",
    "axes[0, 1].set_title('Position X avec incertitude'); axes[0, 1].legend()\n",
    "\n",
    "# 3. Position Y avec bande d'incertitude\n",
    "axes[1, 0].plot(seg_idx, y_test[segment, 1], 'b-', label='Vrai Y', linewidth=1.5)\n",
    "axes[1, 0].plot(seg_idx, y_pred[segment, 1], 'r-', alpha=0.7, label='Prediction (mu)', linewidth=1)\n",
    "axes[1, 0].fill_between(seg_idx,\n",
    "                         y_pred[segment, 1] - 2 * y_sigma[segment, 1],\n",
    "                         y_pred[segment, 1] + 2 * y_sigma[segment, 1],\n",
    "                         alpha=0.2, color='red', label='Incertitude (2 sigma)')\n",
    "axes[1, 0].set_xlabel('Index'); axes[1, 0].set_ylabel('Position Y')\n",
    "axes[1, 0].set_title('Position Y avec incertitude'); axes[1, 0].legend()\n",
    "\n",
    "# 4. Calibration incertitude vs erreur\n",
    "sigma_mean = (y_sigma[:, 0] + y_sigma[:, 1]) / 2\n",
    "axes[1, 1].scatter(sigma_mean, eucl_errors, s=1, alpha=0.3)\n",
    "axes[1, 1].set_xlabel('Sigma moyen predit'); axes[1, 1].set_ylabel('Erreur euclidienne reelle')\n",
    "axes[1, 1].set_title('Calibration : incertitude vs erreur')\n",
    "sigma_range = np.linspace(0, sigma_mean.max(), 100)\n",
    "axes[1, 1].plot(sigma_range, 2 * sigma_range, 'r--', label='y = 2*sigma', linewidth=1.5)\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calibration\n",
    "in_1sigma = np.mean(eucl_errors < sigma_mean)\n",
    "in_2sigma = np.mean(eucl_errors < 2 * sigma_mean)\n",
    "in_3sigma = np.mean(eucl_errors < 3 * sigma_mean)\n",
    "print(f'Calibration de l\\'incertitude :')\n",
    "print(f'  Erreur < 1*sigma : {in_1sigma:.1%} (attendu ~39% pour gaussienne 2D)')\n",
    "print(f'  Erreur < 2*sigma : {in_2sigma:.1%} (attendu ~86%)')\n",
    "print(f'  Erreur < 3*sigma : {in_3sigma:.1%} (attendu ~99%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Visualisations spécifiques : Feasibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Heatmaps : erreur + sigma + distance au squelette ---\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 7))\n",
    "\n",
    "nbins = 20\n",
    "x_edges = np.linspace(0, 1, nbins + 1)\n",
    "y_edges = np.linspace(0, 1, nbins + 1)\n",
    "\n",
    "for ax_idx, (title, values, cmap, pos_for_binning) in enumerate([\n",
    "    ('Erreur euclidienne moyenne', eucl_errors, 'RdYlGn_r', y_test),\n",
    "    ('Sigma moyen predit', (y_sigma[:, 0] + y_sigma[:, 1]) / 2, 'RdYlGn_r', y_test),\n",
    "    ('Distance au squelette (predictions)', test_dist_to_skel, 'Reds', y_pred)\n",
    "]):\n",
    "    val_map = np.full((nbins, nbins), np.nan)\n",
    "    count_map = np.zeros((nbins, nbins))\n",
    "    for i in range(len(pos_for_binning)):\n",
    "        xi = np.clip(np.searchsorted(x_edges, pos_for_binning[i, 0]) - 1, 0, nbins - 1)\n",
    "        yi = np.clip(np.searchsorted(y_edges, pos_for_binning[i, 1]) - 1, 0, nbins - 1)\n",
    "        if np.isnan(val_map[yi, xi]):\n",
    "            val_map[yi, xi] = 0\n",
    "        val_map[yi, xi] += values[i]\n",
    "        count_map[yi, xi] += 1\n",
    "    mean_map = np.where(count_map > 0, val_map / count_map, np.nan)\n",
    "    im = axes[ax_idx].imshow(mean_map, origin='lower', aspect='equal', cmap=cmap, extent=[0, 1, 0, 1])\n",
    "    axes[ax_idx].set_xlabel('X'); axes[ax_idx].set_ylabel('Y')\n",
    "    axes[ax_idx].set_title(title); plt.colorbar(im, ax=axes[ax_idx])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Prédictions hors labyrinthe : {pct_outside:.1%}')\n",
    "print(f'Distance moyenne au squelette : {test_dist_to_skel.mean():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Scatter des prédictions avec couleur = distance au squelette ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# 1. Prédictions colorées par distance au squelette\n",
    "sc = axes[0].scatter(y_pred[:, 0], y_pred[:, 1], c=test_dist_to_skel, cmap='Reds', s=2, alpha=0.5, vmin=0, vmax=0.3)\n",
    "for x1, y1, x2, y2 in SKELETON_SEGMENTS:\n",
    "    axes[0].plot([x1, x2], [y1, y2], 'b-', linewidth=2, alpha=0.8)\n",
    "axes[0].set_xlabel('X'); axes[0].set_ylabel('Y')\n",
    "axes[0].set_title('Prédictions colorées par distance au squelette')\n",
    "axes[0].set_aspect('equal')\n",
    "plt.colorbar(sc, ax=axes[0], label='Distance au squelette')\n",
    "\n",
    "# 2. Histogramme de la distance au squelette\n",
    "axes[1].hist(test_dist_to_skel, bins=50, alpha=0.7, edgecolor='white')\n",
    "axes[1].axvline(CORRIDOR_HALF_WIDTH, color='red', linestyle='--', linewidth=2, label=f'Seuil couloir ({CORRIDOR_HALF_WIDTH})')\n",
    "axes[1].set_xlabel('Distance au squelette')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title(f'Distribution des distances (hors : {pct_outside:.1%})')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Visualisations spécifiques : Clustering hiérarchique 3 zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Trajectoire + zone probabilities ---\n",
    "segment = slice(0, 500)\n",
    "seg_idx = np.arange(500)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Trajectoire 2D colorée par zone prédite\n",
    "zone_colors_map = ['blue', 'green', 'red']\n",
    "for z in range(N_ZONES):\n",
    "    zmask = zone_pred == z\n",
    "    axes[0, 0].scatter(y_pred[zmask, 0], y_pred[zmask, 1], c=zone_colors_map[z], s=2, alpha=0.3, label=f'{ZONE_NAMES[z]}')\n",
    "for x1, y1, x2, y2 in SKELETON_SEGMENTS:\n",
    "    axes[0, 0].plot([x1, x2], [y1, y2], 'k--', linewidth=1, alpha=0.5)\n",
    "axes[0, 0].set_xlabel('X'); axes[0, 0].set_ylabel('Y')\n",
    "axes[0, 0].set_title('Prédictions colorées par zone')\n",
    "axes[0, 0].legend(markerscale=10); axes[0, 0].set_aspect('equal')\n",
    "\n",
    "# 2. Probabilités de zone dans le temps (500 pts)\n",
    "for z in range(N_ZONES):\n",
    "    axes[0, 1].plot(seg_idx, probs_ensemble[segment, z], label=ZONE_NAMES[z], linewidth=1)\n",
    "axes[0, 1].plot(seg_idx, zone_test_targets[segment], 'k--', alpha=0.3, label='Vrai (0/1/2)')\n",
    "axes[0, 1].set_xlabel('Index'); axes[0, 1].set_ylabel('Probabilité')\n",
    "axes[0, 1].set_title('P(zone) vs temps (500 pts)'); axes[0, 1].legend()\n",
    "axes[0, 1].set_ylim(-0.05, 1.05)\n",
    "\n",
    "# 3. Distance curviligne prédite vs vraie\n",
    "axes[1, 0].plot(seg_idx, d_test_targets[segment], 'b-', label='Vrai d', linewidth=1.5)\n",
    "axes[1, 0].plot(seg_idx, d_pred_ensemble[segment], 'r-', alpha=0.7, label='Pred d', linewidth=1)\n",
    "axes[1, 0].axhline(y=D_LEFT_END, color='blue', linestyle=':', alpha=0.5, label=f'Seuil gauche ({D_LEFT_END:.3f})')\n",
    "axes[1, 0].axhline(y=D_RIGHT_START, color='red', linestyle=':', alpha=0.5, label=f'Seuil droite ({D_RIGHT_START:.3f})')\n",
    "axes[1, 0].set_xlabel('Index'); axes[1, 0].set_ylabel('d curviligne')\n",
    "axes[1, 0].set_title('Distance curviligne d (500 pts)'); axes[1, 0].legend()\n",
    "\n",
    "# 4. Carte confusion (correct/erreur de zone)\n",
    "correct = ~zone_confusion\n",
    "axes[1, 1].scatter(y_test[correct, 0], y_test[correct, 1], c='green', s=1, alpha=0.2, label=f'Correct ({correct.mean():.1%})')\n",
    "if zone_confusion.any():\n",
    "    axes[1, 1].scatter(y_test[zone_confusion, 0], y_test[zone_confusion, 1], c='red', s=5, alpha=0.8, label=f'Erreur ({zone_confusion_rate:.1%})')\n",
    "for x1, y1, x2, y2 in SKELETON_SEGMENTS:\n",
    "    axes[1, 1].plot([x1, x2], [y1, y2], 'k--', linewidth=1, alpha=0.3)\n",
    "axes[1, 1].set_xlabel('X'); axes[1, 1].set_ylabel('Y')\n",
    "axes[1, 1].set_title('Confusion de zone (rouge = erreur)'); axes[1, 1].legend(markerscale=5)\n",
    "axes[1, 1].set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Heatmap zone prédite + erreur par zone ---\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 7))\n",
    "\n",
    "nbins = 20\n",
    "x_edges = np.linspace(0, 1, nbins + 1)\n",
    "y_edges = np.linspace(0, 1, nbins + 1)\n",
    "\n",
    "for ax_idx, (title, values, cmap) in enumerate([\n",
    "    ('Zone prédite (argmax)', zone_pred.astype(float), 'RdYlBu'),\n",
    "    ('Erreur euclidienne', eucl_errors, 'RdYlGn_r'),\n",
    "    ('Confiance de zone (max prob)', probs_ensemble.max(axis=1), 'RdYlGn')\n",
    "]):\n",
    "    val_map = np.full((nbins, nbins), np.nan)\n",
    "    count_map = np.zeros((nbins, nbins))\n",
    "    for i in range(len(y_test)):\n",
    "        xi = np.clip(np.searchsorted(x_edges, y_test[i, 0]) - 1, 0, nbins - 1)\n",
    "        yi = np.clip(np.searchsorted(y_edges, y_test[i, 1]) - 1, 0, nbins - 1)\n",
    "        if np.isnan(val_map[yi, xi]):\n",
    "            val_map[yi, xi] = 0\n",
    "        val_map[yi, xi] += values[i]\n",
    "        count_map[yi, xi] += 1\n",
    "    mean_map = np.where(count_map > 0, val_map / count_map, np.nan)\n",
    "    im = axes[ax_idx].imshow(mean_map, origin='lower', aspect='equal', cmap=cmap, extent=[0, 1, 0, 1])\n",
    "    axes[ax_idx].set_title(title)\n",
    "    axes[ax_idx].set_xlabel('X'); axes[ax_idx].set_ylabel('Y')\n",
    "    plt.colorbar(im, ax=axes[ax_idx])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Confusion matrix 3 classes ---\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(zone_test_targets, zone_pred)\n",
    "cm_norm = cm.astype(float) / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Matrice de confusion brute\n",
    "im0 = axes[0].imshow(cm, cmap='Blues')\n",
    "for i in range(N_ZONES):\n",
    "    for j in range(N_ZONES):\n",
    "        axes[0].text(j, i, f'{cm[i, j]}', ha='center', va='center', fontsize=14,\n",
    "                     color='white' if cm[i, j] > cm.max() / 2 else 'black')\n",
    "axes[0].set_xticks(range(N_ZONES)); axes[0].set_xticklabels(ZONE_NAMES)\n",
    "axes[0].set_yticks(range(N_ZONES)); axes[0].set_yticklabels(ZONE_NAMES)\n",
    "axes[0].set_xlabel('Zone prédite'); axes[0].set_ylabel('Zone vraie')\n",
    "axes[0].set_title('Matrice de confusion (comptes)')\n",
    "plt.colorbar(im0, ax=axes[0])\n",
    "\n",
    "# Matrice de confusion normalisée\n",
    "im1 = axes[1].imshow(cm_norm, cmap='Blues', vmin=0, vmax=1)\n",
    "for i in range(N_ZONES):\n",
    "    for j in range(N_ZONES):\n",
    "        axes[1].text(j, i, f'{cm_norm[i, j]:.1%}', ha='center', va='center', fontsize=14,\n",
    "                     color='white' if cm_norm[i, j] > 0.5 else 'black')\n",
    "axes[1].set_xticks(range(N_ZONES)); axes[1].set_xticklabels(ZONE_NAMES)\n",
    "axes[1].set_yticks(range(N_ZONES)); axes[1].set_yticklabels(ZONE_NAMES)\n",
    "axes[1].set_xlabel('Zone prédite'); axes[1].set_ylabel('Zone vraie')\n",
    "axes[1].set_title('Matrice de confusion (normalisée)')\n",
    "plt.colorbar(im1, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Accuracy globale : {cls_accuracy:.1%}')\n",
    "for z in range(N_ZONES):\n",
    "    print(f'  {ZONE_NAMES[z]:8s} : recall={cm_norm[z, z]:.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Visualisations spécifiques : Dropout et bruit gaussien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Analyse de la variance inter-folds (effet du dropout/noise) ---\n",
    "# La variance entre les 5 folds reflète l'incertitude épistémique,\n",
    "# amplifiée par le dropout et le bruit gaussien pendant l'entraînement.\n",
    "\n",
    "fold_std_mu = all_fold_mu.std(axis=0)  # (n_test, 2)\n",
    "fold_std_x = fold_std_mu[:, 0]\n",
    "fold_std_y = fold_std_mu[:, 1]\n",
    "fold_std_eucl = np.sqrt(fold_std_x**2 + fold_std_y**2)\n",
    "\n",
    "# Incertitude aleatoric (mean sigma) vs epistemic (fold variance)\n",
    "aleatoric = (all_fold_sigma ** 2).mean(axis=0)  # E[sigma²]\n",
    "aleatoric_mean = np.sqrt(aleatoric).mean(axis=1)\n",
    "epistemic = np.sqrt(var_mu).mean(axis=1)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Variance inter-folds par position\n",
    "sc = axes[0, 0].scatter(y_test[:, 0], y_test[:, 1], c=fold_std_eucl, cmap='Reds', s=2, alpha=0.5)\n",
    "axes[0, 0].set_xlabel('X'); axes[0, 0].set_ylabel('Y')\n",
    "axes[0, 0].set_title('Variance inter-folds (épistémique)')\n",
    "axes[0, 0].set_aspect('equal')\n",
    "plt.colorbar(sc, ax=axes[0, 0], label='Std des mu entre folds')\n",
    "\n",
    "# 2. Aleatoric vs Epistemic\n",
    "axes[0, 1].scatter(aleatoric_mean, epistemic, s=1, alpha=0.3)\n",
    "max_val = max(aleatoric_mean.max(), epistemic.max())\n",
    "axes[0, 1].plot([0, max_val], [0, max_val], 'r--', label='y=x')\n",
    "axes[0, 1].set_xlabel('Incertitude aléatoire (mean sigma)')\n",
    "axes[0, 1].set_ylabel('Incertitude épistémique (std mu entre folds)')\n",
    "axes[0, 1].set_title('Aléatoire vs Épistémique')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# 3. Distribution des deux types d'incertitude\n",
    "axes[1, 0].hist(aleatoric_mean, bins=50, alpha=0.7, label='Aléatoire', color='steelblue')\n",
    "axes[1, 0].hist(epistemic, bins=50, alpha=0.7, label='Épistémique', color='coral')\n",
    "axes[1, 0].set_xlabel('Incertitude'); axes[1, 0].set_ylabel('Count')\n",
    "axes[1, 0].set_title('Distribution des incertitudes'); axes[1, 0].legend()\n",
    "\n",
    "# 4. Erreur vs incertitude totale (aleatoric + epistemic)\n",
    "total_uncertainty = np.sqrt(aleatoric_mean**2 + epistemic**2)\n",
    "axes[1, 1].scatter(total_uncertainty, eucl_errors, s=1, alpha=0.3)\n",
    "u_range = np.linspace(0, total_uncertainty.max(), 100)\n",
    "axes[1, 1].plot(u_range, 2 * u_range, 'r--', label='y = 2*sigma_total')\n",
    "axes[1, 1].set_xlabel('Incertitude totale (sqrt(aleatoric² + epistemic²))')\n",
    "axes[1, 1].set_ylabel('Erreur euclidienne')\n",
    "axes[1, 1].set_title('Calibration totale'); axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "corr_alea, _ = spearmanr(aleatoric_mean, eucl_errors)\n",
    "corr_epis, _ = spearmanr(epistemic, eucl_errors)\n",
    "corr_total, _ = spearmanr(total_uncertainty, eucl_errors)\n",
    "print(f'Corrélation (Spearman) incertitude vs erreur :')\n",
    "print(f'  Aléatoire  : {corr_alea:.3f}')\n",
    "print(f'  Épistémique: {corr_epis:.3f}')\n",
    "print(f'  Totale     : {corr_total:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Accord entre les 5 folds ---\n",
    "# Pour chaque point, on regarde si les 5 folds prédisent la même zone\n",
    "fold_zone_preds = np.stack([fp.argmax(axis=1) for fp in all_fold_probs])  # (5, n_test)\n",
    "zone_agreement = np.zeros(len(y_test))\n",
    "for i in range(len(y_test)):\n",
    "    most_common = np.bincount(fold_zone_preds[:, i], minlength=3).max()\n",
    "    zone_agreement[i] = most_common / N_FOLDS\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# 1. Accord de zone spatial\n",
    "sc = axes[0].scatter(y_test[:, 0], y_test[:, 1], c=zone_agreement, cmap='RdYlGn', s=2, alpha=0.5, vmin=0.4, vmax=1.0)\n",
    "for x1, y1, x2, y2 in SKELETON_SEGMENTS:\n",
    "    axes[0].plot([x1, x2], [y1, y2], 'k--', linewidth=1, alpha=0.5)\n",
    "axes[0].set_xlabel('X'); axes[0].set_ylabel('Y')\n",
    "axes[0].set_title('Accord inter-folds sur la zone (5 folds)')\n",
    "axes[0].set_aspect('equal')\n",
    "plt.colorbar(sc, ax=axes[0], label='% accord (5 folds)')\n",
    "\n",
    "# 2. Histogramme d'accord\n",
    "unique_vals = [0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "counts = [(zone_agreement == v).sum() for v in unique_vals]\n",
    "labels = ['1/5', '2/5', '3/5', '4/5', '5/5']\n",
    "bars = axes[1].bar(labels, counts, color=['red', 'orange', 'yellow', 'lightgreen', 'green'])\n",
    "for bar, c in zip(bars, counts):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 20, f'{c}', ha='center')\n",
    "axes[1].set_xlabel('Accord (sur 5 folds)'); axes[1].set_ylabel('Nombre de points')\n",
    "axes[1].set_title('Distribution de l\\'accord de zone entre folds')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Accord parfait (5/5) : {(zone_agreement == 1.0).mean():.1%}')\n",
    "print(f'Accord majoritaire (>=3/5) : {(zone_agreement >= 0.6).mean():.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Visualisations spécifiques : Incertitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Distribution des sigma ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(y_sigma[:, 0], bins=50, alpha=0.7, color='steelblue', edgecolor='white')\n",
    "axes[0].axvline(y_sigma[:, 0].mean(), color='red', linestyle='--', label=f'Moyenne={y_sigma[:, 0].mean():.4f}')\n",
    "axes[0].set_xlabel('Sigma X'); axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Distribution de l\\'incertitude sur X'); axes[0].legend()\n",
    "\n",
    "axes[1].hist(y_sigma[:, 1], bins=50, alpha=0.7, color='coral', edgecolor='white')\n",
    "axes[1].axvline(y_sigma[:, 1].mean(), color='red', linestyle='--', label=f'Moyenne={y_sigma[:, 1].mean():.4f}')\n",
    "axes[1].set_xlabel('Sigma Y'); axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Distribution de l\\'incertitude sur Y'); axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Heatmap incertitude par position ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "nbins = 20\n",
    "x_edges = np.linspace(0, 1, nbins + 1)\n",
    "y_edges = np.linspace(0, 1, nbins + 1)\n",
    "\n",
    "for ax_idx, (title, values) in enumerate([\n",
    "    ('Erreur euclidienne moyenne', eucl_errors),\n",
    "    ('Sigma moyen predit', (y_sigma[:, 0] + y_sigma[:, 1]) / 2)\n",
    "]):\n",
    "    val_map = np.full((nbins, nbins), np.nan)\n",
    "    count_map = np.zeros((nbins, nbins))\n",
    "    for i in range(len(y_test)):\n",
    "        xi = np.clip(np.searchsorted(x_edges, y_test[i, 0]) - 1, 0, nbins - 1)\n",
    "        yi = np.clip(np.searchsorted(y_edges, y_test[i, 1]) - 1, 0, nbins - 1)\n",
    "        if np.isnan(val_map[yi, xi]):\n",
    "            val_map[yi, xi] = 0\n",
    "        val_map[yi, xi] += values[i]\n",
    "        count_map[yi, xi] += 1\n",
    "    mean_map = np.where(count_map > 0, val_map / count_map, np.nan)\n",
    "    im = axes[ax_idx].imshow(mean_map, origin='lower', aspect='equal', cmap='RdYlGn_r', extent=[0, 1, 0, 1])\n",
    "    axes[ax_idx].set_xlabel('X'); axes[ax_idx].set_ylabel('Y')\n",
    "    axes[ax_idx].set_title(title)\n",
    "    plt.colorbar(im, ax=axes[ax_idx])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "corr, pval = spearmanr(sigma_mean, eucl_errors)\n",
    "print(f'Corrélation de Spearman (sigma vs erreur) : {corr:.3f} (p={pval:.2e})')\n",
    "print(f'  -> {\"Bonne\" if corr > 0.3 else \"Faible\"} calibration : le modèle {\"sait\" if corr > 0.3 else \"ne sait pas bien\"} quand il se trompe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Sauvegarde des prédictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../outputs/preds_transformer_02i.npy', y_pred)\n",
    "np.save('../outputs/sigma_transformer_02i.npy', y_sigma)\n",
    "np.save('../outputs/d_pred_transformer_02i.npy', d_pred_ensemble)\n",
    "np.save('../outputs/y_test_transformer_02i.npy', y_test)\n",
    "np.save('../outputs/d_test_transformer_02i.npy', d_test_targets)\n",
    "np.save('../outputs/zone_pred_transformer_02i.npy', zone_pred)\n",
    "np.save('../outputs/zone_test_transformer_02i.npy', zone_test_targets)\n",
    "np.save('../outputs/probs_transformer_02i.npy', probs_ensemble)\n",
    "\n",
    "print(f'Prédictions sauvegardées :')\n",
    "print(f'  preds_transformer_02i.npy : {y_pred.shape}')\n",
    "print(f'  sigma_transformer_02i.npy : {y_sigma.shape}')\n",
    "print(f'  d_pred_transformer_02i.npy : {d_pred_ensemble.shape}')\n",
    "print(f'  zone_pred_transformer_02i.npy : {zone_pred.shape}')\n",
    "print(f'  probs_transformer_02i.npy : {probs_ensemble.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Interprétation\n",
    "\n",
    "### Ce notebook combine toutes les améliorations\n",
    "\n",
    "**1. Spike dropout (15%) + Bruit gaussien (std=0.5)**\n",
    "- Régularisation par data augmentation : le modèle voit des variantes bruitées de chaque séquence\n",
    "- Force le modèle à utiliser le population coding (redondance entre neurones) plutôt que de dépendre d'un seul spike\n",
    "- L'effet se mesure par la variance inter-folds : plus stable = meilleure généralisation\n",
    "\n",
    "**2. Feasibility loss (λ=10.0)**\n",
    "- Incorpore la connaissance du domaine (géométrie du U-maze) directement dans la loss\n",
    "- Pénalise quadratiquement les prédictions hors couloir\n",
    "- L'effet se mesure par le % de prédictions hors labyrinthe et la distance moyenne au squelette\n",
    "\n",
    "**3. Classification hiérarchique 3 zones**\n",
    "- Exploite la structure du labyrinthe : gauche / haut / droite\n",
    "- Mélange d'experts : chaque zone a son propre head de régression spécialisé\n",
    "- L'incertitude aux transitions est naturellement plus élevée (mélange multimodal)\n",
    "- L'effet se mesure par la matrice de confusion et l'erreur par zone\n",
    "\n",
    "**4. 5-fold cross-validation (30 epochs)**\n",
    "- 5 modèles indépendants → ensemble robuste\n",
    "- Incertitude épistémique (variance entre folds) + aléatoire (sigma prédits)\n",
    "- Early stopping avec patience=7 pour éviter l'overfitting\n",
    "\n",
    "### Métriques clés à comparer avec les notebooks précédents\n",
    "- **Erreur euclidienne** : plus basse = meilleure précision\n",
    "- **R²** : plus proche de 1 = meilleur\n",
    "- **% hors labyrinthe** : plus bas = feasibility loss fonctionne\n",
    "- **Accuracy zone** : classification correcte\n",
    "- **Calibration** : sigma corrélé avec erreur = le modèle sait quand il se trompe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}