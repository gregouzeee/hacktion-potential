{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Notebook 2e : Transformer Multi-tâches + Spike Dropout + Gaussian Noise\n\n## Modifications par rapport au notebook 02\n\n1. **Multi-tâches (position + vitesse)** : le modèle prédit simultanément la position (x, y) et la vitesse de l'animal. La vitesse est une target auxiliaire qui force le modèle à capturer des features plus riches du signal neuronal. Inspiré de [Frey et al. 2021 (eLife)](https://elifesciences.org/articles/66551) qui montrent que le multi-task (position + direction + vitesse) améliore le décodage.\n\n2. **Spike dropout (15%)** : pendant le training, on masque aléatoirement 15% des spikes de chaque séquence. Force le modèle à exploiter des patterns redondants dans la population neuronale.\n\n3. **Gaussian noise sur les waveforms** : pendant le training, on ajoute du bruit N(0, σ²) sur les waveforms bruts. Force le modèle à être robuste au bruit d'enregistrement et empêche la mémorisation de formes de spike exactes.\n\n**Architecture** : identique au notebook 02 (CNN par shank + Transformer + Gaussian NLL), avec une **tête supplémentaire** pour la prédiction de vitesse (MSE loss).\n\n**Loss combinée** : `L = L_position (Gaussian NLL) + λ * L_speed (MSE)` avec λ = 0.5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports et configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Reproductibilité\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Device\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device('mps')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "print(f'Device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# --- Chargement des données ---\n",
    "# Les données doivent être dans data/ (via: python download_data.py)\n",
    "LOCAL_DIR = os.path.join(os.path.abspath('..'), 'data')\n",
    "\n",
    "PARQUET_NAME = \"M1199_PAG_stride4_win108_test.parquet\"\n",
    "JSON_NAME = \"M1199_PAG.json\"\n",
    "\n",
    "PARQUET_FILE = os.path.join(LOCAL_DIR, PARQUET_NAME)\n",
    "JSON_FILE = os.path.join(LOCAL_DIR, JSON_NAME)\n",
    "\n",
    "if not os.path.exists(PARQUET_FILE):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Données introuvables dans {LOCAL_DIR}/\\n\"\n",
    "        f\"Lancez d'abord: python download_data.py\"\n",
    "    )\n",
    "\n",
    "print(f\"Chargement depuis {LOCAL_DIR}/\")\n",
    "df = pd.read_parquet(PARQUET_FILE)\n",
    "with open(JSON_FILE, \"r\") as f:\n",
    "    params = json.load(f)\n",
    "\n",
    "print(f\"Shape: {df.shape}\")\n",
    "\n",
    "nGroups = params['nGroups']\n",
    "nChannelsPerGroup = [params[f'group{g}']['nChannels'] for g in range(nGroups)]\n",
    "print(f\"nGroups={nGroups}, nChannelsPerGroup={nChannelsPerGroup}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement et filtrage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Filtrage speedMask (on ne garde que les exemples en mouvement)\nspeed_masks = np.array([x[0] for x in df['speedMask']])\ndf_moving = df[speed_masks].reset_index(drop=True)\nprint(f'Exemples en mouvement : {len(df_moving)}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing : reconstruction de la séquence chronologique\n",
    "\n",
    "Pour chaque fenêtre, on reconstruit la séquence ordonnée de spikes :\n",
    "1. On reshape les waveforms de chaque shank : `groupX` → `(n_spikes, nCh, 32)`\n",
    "2. On parcourt le tableau `groups` dans l'ordre chronologique\n",
    "3. Pour chaque timestep, on récupère le waveform correspondant via `indicesX`\n",
    "4. On ignore les indices à 0 (padding)\n",
    "\n",
    "Le résultat : une liste de tuples `(waveform, shank_id)` ordonnée dans le temps.\n",
    "\n",
    "Comme les shanks ont des nombres de canaux différents (6 ou 4), on utilise un **encodeur par shank** qui projette chaque waveform vers un espace commun de dimension D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premier exemple : 78 spikes réels dans la séquence\n",
      "Shanks utilisés : {0, 1, 2, 3}\n",
      "Premier spike : shank=1, shape=(4, 32)\n"
     ]
    }
   ],
   "source": [
    "def reconstruct_sequence(row, nGroups, nChannelsPerGroup, max_seq_len=128):\n",
    "    \"\"\"\n",
    "    Reconstruit la séquence chronologique de spikes.\n",
    "    \n",
    "    Retourne:\n",
    "        waveforms_per_group: dict {g: np.array (n_spikes_g, nCh, 32)}\n",
    "        sequence_groups: np.array (seq_len,) - quel shank à chaque timestep\n",
    "        sequence_indices: np.array (seq_len,) - quel index de spike dans le shank\n",
    "        seq_len: longueur effective de la séquence\n",
    "    \"\"\"\n",
    "    groups = row['groups']\n",
    "    length = min(len(groups), max_seq_len)\n",
    "    \n",
    "    # Reshape des waveforms par shank\n",
    "    waveforms = {}\n",
    "    for g in range(nGroups):\n",
    "        nCh = nChannelsPerGroup[g]\n",
    "        raw = row[f'group{g}']\n",
    "        waveforms[g] = raw.reshape(-1, nCh, 32)\n",
    "    \n",
    "    # Reconstruction de la séquence\n",
    "    seq_waveforms = []  # liste de (nCh, 32) - taille variable par spike\n",
    "    seq_shank_ids = []\n",
    "    \n",
    "    for t in range(length):\n",
    "        g = int(groups[t])\n",
    "        idx = int(row[f'indices{g}'][t])\n",
    "        if idx > 0 and idx <= waveforms[g].shape[0]:\n",
    "            seq_waveforms.append((waveforms[g][idx - 1], g))  # idx est 1-based\n",
    "            seq_shank_ids.append(g)\n",
    "    \n",
    "    return seq_waveforms, seq_shank_ids\n",
    "\n",
    "# Test rapide\n",
    "wf, sids = reconstruct_sequence(df_moving.iloc[0], nGroups, nChannelsPerGroup)\n",
    "print(f'Premier exemple : {len(wf)} spikes réels dans la séquence')\n",
    "print(f'Shanks utilisés : {set(sids)}')\n",
    "print(f'Premier spike : shank={wf[0][1]}, shape={wf[0][0].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset PyTorch\n",
    "\n",
    "Le Dataset gère :\n",
    "- La reconstruction de séquence à la volée (pas besoin de stocker tout en mémoire)\n",
    "- Le padding des waveforms à la taille max de canaux (6) pour pouvoir empiler dans un tenseur\n",
    "- Le retour du masque de padding pour l'attention\n",
    "\n",
    "On utilise un `collate_fn` custom pour gérer les séquences de longueur variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "MAX_SEQ_LEN = 128  # On tronque les séquences trop longues (max observé = 190)\nMAX_CHANNELS = max(nChannelsPerGroup)  # 6\n\nclass SpikeSequenceDataset(Dataset):\n    def __init__(self, dataframe, nGroups, nChannelsPerGroup, max_seq_len=MAX_SEQ_LEN):\n        self.df = dataframe\n        self.nGroups = nGroups\n        self.nChannelsPerGroup = nChannelsPerGroup\n        self.max_seq_len = max_seq_len\n        \n        # Pré-extraire les targets : position (x, y) + vitesse\n        self.targets = np.array([[x[0], x[1]] for x in dataframe['pos']], dtype=np.float32)\n        self.speeds = np.array([x[3] for x in dataframe['pos']], dtype=np.float32)\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        seq, shank_ids = reconstruct_sequence(row, self.nGroups, self.nChannelsPerGroup, self.max_seq_len)\n        \n        seq_len = len(seq)\n        if seq_len == 0:\n            # Cas dégénéré : aucun spike valide\n            seq_len = 1\n            waveforms = np.zeros((1, MAX_CHANNELS, 32), dtype=np.float32)\n            shank_ids_arr = np.array([0], dtype=np.int64)\n        else:\n            # Padder les waveforms à MAX_CHANNELS canaux\n            waveforms = np.zeros((seq_len, MAX_CHANNELS, 32), dtype=np.float32)\n            shank_ids_arr = np.array(shank_ids, dtype=np.int64)\n            for t, (wf, g) in enumerate(seq):\n                nCh = wf.shape[0]\n                waveforms[t, :nCh, :] = wf\n        \n        target = self.targets[idx]\n        speed = self.speeds[idx]\n        return {\n            'waveforms': torch.from_numpy(waveforms),      # (seq_len, MAX_CH, 32)\n            'shank_ids': torch.from_numpy(shank_ids_arr),   # (seq_len,)\n            'seq_len': seq_len,\n            'target': torch.from_numpy(target),             # (2,)\n            'speed': torch.tensor(speed, dtype=torch.float32)  # scalar\n        }\n\n\ndef collate_fn(batch):\n    \"\"\"Collate avec padding dynamique à la longueur max du batch.\"\"\"\n    max_len = max(item['seq_len'] for item in batch)\n    batch_size = len(batch)\n    \n    waveforms = torch.zeros(batch_size, max_len, MAX_CHANNELS, 32)\n    shank_ids = torch.zeros(batch_size, max_len, dtype=torch.long)\n    mask = torch.ones(batch_size, max_len, dtype=torch.bool)  # True = padding (à ignorer)\n    targets = torch.stack([item['target'] for item in batch])\n    speeds = torch.stack([item['speed'] for item in batch])\n    \n    for i, item in enumerate(batch):\n        sl = item['seq_len']\n        waveforms[i, :sl] = item['waveforms']\n        shank_ids[i, :sl] = item['shank_ids']\n        mask[i, :sl] = False  # False = pas de padding\n    \n    return {\n        'waveforms': waveforms,\n        'shank_ids': shank_ids,\n        'mask': mask,\n        'targets': targets,\n        'speeds': speeds\n    }\n\n# Vérifier les statistiques de vitesse\nspeeds_all = np.array([x[3] for x in df_moving['pos']])\nprint(f'Dataset et collate_fn définis (avec vitesse).')\nprint(f'Statistiques vitesse : min={speeds_all.min():.4f}, max={speeds_all.max():.4f}, mean={speeds_all.mean():.4f}, std={speeds_all.std():.4f}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Architecture du modèle\n\nIdentique au notebook 02, avec trois additions :\n\n1. **Spike dropout** dans le `forward()` : masquage aléatoire de 15% des spikes actifs pendant le training.\n\n2. **Gaussian noise** sur les waveforms : ajout de bruit N(0, σ²) sur les waveforms avant l'encodage CNN, pendant le training uniquement.\n\n3. **Tête de prédiction de vitesse** (`speed_head`) : en plus des têtes mu et log_sigma pour la position, une tête MLP supplémentaire prédit la vitesse scalaire de l'animal. Cette tête partage le même backbone Transformer (shared representation learning)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class SpikeEncoder(nn.Module):\n    \"\"\"Encode un waveform (MAX_CH, 32) en un vecteur de dimension embed_dim.\n    Utilise un CNN 1D sur la dimension temporelle (32 points).\"\"\"\n    \n    def __init__(self, n_channels, embed_dim):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv1d(n_channels, 32, kernel_size=5, padding=2),\n            nn.ReLU(),\n            nn.Conv1d(32, embed_dim, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool1d(1)  # (batch, embed_dim, 1)\n        )\n    \n    def forward(self, x):\n        # x: (batch * seq_len, n_channels, 32)\n        return self.conv(x).squeeze(-1)  # (batch * seq_len, embed_dim)\n\n\nclass PositionalEncoding(nn.Module):\n    \"\"\"Sinusoidal positional encoding.\"\"\"\n    \n    def __init__(self, embed_dim, max_len=256):\n        super().__init__()\n        pe = torch.zeros(max_len, embed_dim)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe.unsqueeze(0))  # (1, max_len, embed_dim)\n    \n    def forward(self, x):\n        # x: (batch, seq_len, embed_dim)\n        return x + self.pe[:, :x.size(1)]\n\n\nclass SpikeTransformer(nn.Module):\n    def __init__(self, nGroups, nChannelsPerGroup, embed_dim=64, nhead=4, \n                 num_layers=2, dropout=0.2, spike_dropout=0.15, noise_std=0.5,\n                 max_channels=MAX_CHANNELS):\n        super().__init__()\n        self.nGroups = nGroups\n        self.embed_dim = embed_dim\n        self.max_channels = max_channels\n        self.spike_dropout = spike_dropout\n        self.noise_std = noise_std\n        \n        # Un encodeur par shank\n        self.spike_encoders = nn.ModuleList([\n            SpikeEncoder(max_channels, embed_dim) for _ in range(nGroups)\n        ])\n        \n        # Embedding de shank\n        self.shank_embedding = nn.Embedding(nGroups, embed_dim)\n        \n        # Positional encoding\n        self.pos_encoding = PositionalEncoding(embed_dim)\n        \n        # Transformer encoder\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=embed_dim, nhead=nhead, dim_feedforward=embed_dim * 4,\n            dropout=dropout, batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(\n            encoder_layer, num_layers=num_layers, enable_nested_tensor=False\n        )\n        \n        # Readout position : mu (2) et log_sigma (2)\n        self.mu_head = nn.Sequential(\n            nn.Linear(embed_dim, embed_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(embed_dim, 2)  # (mu_x, mu_y)\n        )\n        self.log_sigma_head = nn.Sequential(\n            nn.Linear(embed_dim, embed_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(embed_dim, 2)  # (log_sigma_x, log_sigma_y)\n        )\n        \n        # Readout vitesse (tâche auxiliaire)\n        self.speed_head = nn.Sequential(\n            nn.Linear(embed_dim, embed_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(embed_dim, 1)  # vitesse scalaire\n        )\n    \n    def _apply_spike_dropout(self, mask):\n        \"\"\"Masque aléatoirement des spikes actifs pendant le training.\"\"\"\n        if not self.training or self.spike_dropout <= 0:\n            return mask\n        \n        drop_mask = torch.rand_like(mask.float()) < self.spike_dropout\n        active = ~mask\n        new_drops = drop_mask & active\n        \n        # S'assurer qu'il reste au moins 1 spike par séquence\n        remaining = active & ~new_drops\n        n_remaining = remaining.sum(dim=1)\n        all_dropped = n_remaining == 0\n        if all_dropped.any():\n            new_drops[all_dropped] = False\n        \n        return mask | new_drops\n    \n    def _apply_waveform_noise(self, waveforms):\n        \"\"\"Ajoute du bruit gaussien sur les waveforms pendant le training.\"\"\"\n        if not self.training or self.noise_std <= 0:\n            return waveforms\n        noise = torch.randn_like(waveforms) * self.noise_std\n        return waveforms + noise\n    \n    def forward(self, waveforms, shank_ids, mask):\n        \"\"\"\n        Args:\n            waveforms: (batch, seq_len, max_ch, 32)\n            shank_ids: (batch, seq_len)\n            mask: (batch, seq_len) - True = padding\n        Returns:\n            mu: (batch, 2) - position prédite\n            sigma: (batch, 2) - incertitude (écart-type)\n            speed_pred: (batch, 1) - vitesse prédite\n        \"\"\"\n        batch_size, seq_len = waveforms.shape[:2]\n        \n        # --- Data augmentation (training only) ---\n        mask = self._apply_spike_dropout(mask)\n        waveforms = self._apply_waveform_noise(waveforms)\n        \n        # --- Encode chaque spike avec l'encodeur de son shank ---\n        embeddings = torch.zeros(batch_size, seq_len, self.embed_dim, device=waveforms.device)\n        \n        for g in range(self.nGroups):\n            group_mask = (shank_ids == g) & (~mask)\n            if group_mask.any():\n                group_wf = waveforms[group_mask]\n                group_emb = self.spike_encoders[g](group_wf)\n                embeddings[group_mask] = group_emb\n        \n        # --- Ajouter le shank embedding ---\n        shank_emb = self.shank_embedding(shank_ids)\n        embeddings = embeddings + shank_emb\n        \n        # --- Positional encoding ---\n        embeddings = self.pos_encoding(embeddings)\n        \n        # --- Transformer ---\n        encoded = self.transformer(embeddings, src_key_padding_mask=mask)\n        \n        # --- Masked average pooling ---\n        active_mask = (~mask).unsqueeze(-1).float()\n        pooled = (encoded * active_mask).sum(dim=1) / (active_mask.sum(dim=1) + 1e-8)\n        \n        # --- Sorties : mu, sigma (position) et speed ---\n        mu = self.mu_head(pooled)\n        log_sigma = self.log_sigma_head(pooled)\n        sigma = torch.exp(log_sigma)  # sigma > 0\n        speed_pred = self.speed_head(pooled)  # (batch, 1)\n        \n        return mu, sigma, speed_pred\n\n\n# Test rapide\nSPIKE_DROPOUT = 0.15\nNOISE_STD = 0.5\nmodel = SpikeTransformer(nGroups, nChannelsPerGroup, embed_dim=64, nhead=4, num_layers=2,\n                         spike_dropout=SPIKE_DROPOUT, noise_std=NOISE_STD)\nn_params = sum(p.numel() for p in model.parameters())\nprint(f'Modèle créé : {n_params:,} paramètres')\nprint(f'Data augmentation : spike dropout={SPIKE_DROPOUT:.0%}, gaussian noise std={NOISE_STD}')\nprint(f'Têtes de sortie : position (mu + sigma) + vitesse')\nprint(model)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Split et DataLoaders\n\nOn sépare les données en **90% train / 10% test** (split temporel : les 10% les plus récents servent de test).\n\nSur les 90% de train, on effectue une **validation croisée à 5 folds** (KFold, random seed 41) pour :\n- Estimer la performance de manière robuste\n- Entraîner 5 modèles qui seront ensuite utilisés en ensemble sur le test set"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.model_selection import KFold\n\n# Split temporel 90/10\nsplit_idx = int(len(df_moving) * 0.9)\ndf_train_full = df_moving.iloc[:split_idx].reset_index(drop=True)\ndf_test = df_moving.iloc[split_idx:].reset_index(drop=True)\n\nprint(f'Train (full) : {len(df_train_full)} exemples')\nprint(f'Test         : {len(df_test)} exemples')\n\n# KFold sur les 90% de train\nN_FOLDS = 5\nkf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=41)\n\nBATCH_SIZE = 64\n\n# Préparer le test loader (commun à tous les folds)\ntest_dataset = SpikeSequenceDataset(df_test, nGroups, nChannelsPerGroup)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n                         collate_fn=collate_fn, num_workers=0)\n\n# Afficher la répartition des folds\nfor fold, (train_idx, val_idx) in enumerate(kf.split(df_train_full)):\n    print(f'  Fold {fold+1}: train={len(train_idx)}, val={len(val_idx)}')\n\nprint(f'\\nTest: {len(test_dataset)} exemples, {len(test_loader)} batches')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Entraînement (5-Fold Cross-Validation)\n\nPour chaque fold :\n- On entraîne un modèle sur 4/5 des données train, on valide sur le 1/5 restant\n- **Loss combinée** : Gaussian NLL (position) + λ × MSE (vitesse)\n- **Optimiseur** : AdamW avec weight decay\n- **Scheduler** : OneCycleLR (warmup puis decay)\n- **Early stopping** basé sur la loss combinée\n\nÀ la fin, on a 5 modèles qu'on peut ensembler pour le test final."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Hyperparamètres\nEMBED_DIM = 64\nNHEAD = 4\nNUM_LAYERS = 2\nDROPOUT = 0.2\nSPIKE_DROPOUT = 0.15  # data augmentation : masquage aléatoire de spikes\nNOISE_STD = 0.5       # data augmentation : bruit gaussien sur waveforms\nSPEED_WEIGHT = 0.5    # poids de la loss vitesse dans la loss totale\nLR = 1e-3\nWEIGHT_DECAY = 1e-4\nEPOCHS = 30\nPATIENCE = 7  # early stopping\n\nprint(f'Hyperparamètres : embed_dim={EMBED_DIM}, nhead={NHEAD}, layers={NUM_LAYERS}, dropout={DROPOUT}')\nprint(f'Data augmentation : spike dropout={SPIKE_DROPOUT:.0%}, gaussian noise std={NOISE_STD}')\nprint(f'Multi-tâches : speed_weight={SPEED_WEIGHT} (L = L_pos + {SPEED_WEIGHT} * L_speed)')\nprint(f'Entraînement : {EPOCHS} epochs max, patience={PATIENCE}, LR={LR}')\nprint(f'Loss : GaussianNLLLoss (position) + MSE (vitesse)')\nprint(f'Device : {DEVICE}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_epoch(model, loader, optimizer, scheduler, pos_criterion, speed_criterion, speed_weight, device):\n    model.train()\n    total_loss = 0\n    total_pos_loss = 0\n    total_speed_loss = 0\n    n_batches = 0\n    for batch in loader:\n        wf = batch['waveforms'].to(device)\n        sid = batch['shank_ids'].to(device)\n        mask = batch['mask'].to(device)\n        targets = batch['targets'].to(device)\n        speeds = batch['speeds'].to(device)\n        \n        optimizer.zero_grad()\n        mu, sigma, speed_pred = model(wf, sid, mask)\n        \n        # Loss position (Gaussian NLL)\n        pos_loss = pos_criterion(mu, targets, sigma ** 2)\n        # Loss vitesse (MSE)\n        speed_loss = speed_criterion(speed_pred.squeeze(-1), speeds)\n        # Loss combinée\n        loss = pos_loss + speed_weight * speed_loss\n        \n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n        \n        total_loss += loss.item()\n        total_pos_loss += pos_loss.item()\n        total_speed_loss += speed_loss.item()\n        n_batches += 1\n    \n    return total_loss / n_batches, total_pos_loss / n_batches, total_speed_loss / n_batches\n\n\n@torch.no_grad()\ndef eval_epoch(model, loader, pos_criterion, speed_criterion, speed_weight, device):\n    model.eval()\n    total_loss = 0\n    total_pos_loss = 0\n    total_speed_loss = 0\n    n_batches = 0\n    all_mu = []\n    all_sigma = []\n    all_targets = []\n    all_speed_pred = []\n    all_speed_true = []\n    for batch in loader:\n        wf = batch['waveforms'].to(device)\n        sid = batch['shank_ids'].to(device)\n        mask = batch['mask'].to(device)\n        targets = batch['targets'].to(device)\n        speeds = batch['speeds'].to(device)\n        \n        mu, sigma, speed_pred = model(wf, sid, mask)\n        \n        pos_loss = pos_criterion(mu, targets, sigma ** 2)\n        speed_loss = speed_criterion(speed_pred.squeeze(-1), speeds)\n        loss = pos_loss + speed_weight * speed_loss\n        \n        total_loss += loss.item()\n        total_pos_loss += pos_loss.item()\n        total_speed_loss += speed_loss.item()\n        n_batches += 1\n        all_mu.append(mu.cpu().numpy())\n        all_sigma.append(sigma.cpu().numpy())\n        all_targets.append(targets.cpu().numpy())\n        all_speed_pred.append(speed_pred.squeeze(-1).cpu().numpy())\n        all_speed_true.append(speeds.cpu().numpy())\n    \n    return (total_loss / n_batches,\n            total_pos_loss / n_batches,\n            total_speed_loss / n_batches,\n            np.concatenate(all_mu), \n            np.concatenate(all_sigma), \n            np.concatenate(all_targets),\n            np.concatenate(all_speed_pred),\n            np.concatenate(all_speed_true))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Boucle d'entraînement avec KFold\nfold_results = []\nall_train_losses = {}\nall_val_losses = {}\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(df_train_full)):\n    print(f'\\n{\"=\"*60}')\n    print(f'FOLD {fold+1}/{N_FOLDS}')\n    print(f'{\"=\"*60}')\n    \n    # Créer les datasets pour ce fold\n    df_fold_train = df_train_full.iloc[train_idx].reset_index(drop=True)\n    df_fold_val = df_train_full.iloc[val_idx].reset_index(drop=True)\n    \n    fold_train_dataset = SpikeSequenceDataset(df_fold_train, nGroups, nChannelsPerGroup)\n    fold_val_dataset = SpikeSequenceDataset(df_fold_val, nGroups, nChannelsPerGroup)\n    \n    fold_train_loader = DataLoader(fold_train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n                                   collate_fn=collate_fn, num_workers=0)\n    fold_val_loader = DataLoader(fold_val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n                                 collate_fn=collate_fn, num_workers=0)\n    \n    print(f'  Train: {len(fold_train_dataset)}, Val: {len(fold_val_dataset)}')\n    \n    # Nouveau modèle pour chaque fold\n    model = SpikeTransformer(\n        nGroups, nChannelsPerGroup,\n        embed_dim=EMBED_DIM, nhead=NHEAD, num_layers=NUM_LAYERS,\n        dropout=DROPOUT, spike_dropout=SPIKE_DROPOUT, noise_std=NOISE_STD\n    ).to(DEVICE)\n    \n    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n    scheduler = optim.lr_scheduler.OneCycleLR(\n        optimizer, max_lr=LR, epochs=EPOCHS, steps_per_epoch=len(fold_train_loader)\n    )\n    pos_criterion = nn.GaussianNLLLoss()\n    speed_criterion = nn.MSELoss()\n    \n    best_val_loss = float('inf')\n    patience_counter = 0\n    train_losses = []\n    val_losses = []\n    model_path = f'../outputs/best_transformer_02e_fold{fold+1}.pt'\n    \n    for epoch in range(EPOCHS):\n        train_loss, train_pos, train_speed = train_epoch(\n            model, fold_train_loader, optimizer, scheduler,\n            pos_criterion, speed_criterion, SPEED_WEIGHT, DEVICE\n        )\n        val_loss, val_pos, val_speed, _, _, _, _, _ = eval_epoch(\n            model, fold_val_loader, pos_criterion, speed_criterion, SPEED_WEIGHT, DEVICE\n        )\n        \n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        \n        if epoch % 5 == 0 or epoch == EPOCHS - 1:\n            print(f'  Epoch {epoch+1:02d}/{EPOCHS} | Train: {train_loss:.5f} (pos={train_pos:.5f}, spd={train_speed:.5f}) | Val: {val_loss:.5f} (pos={val_pos:.5f}, spd={val_speed:.5f})')\n        \n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            patience_counter = 0\n            torch.save(model.state_dict(), model_path)\n        else:\n            patience_counter += 1\n            if patience_counter >= PATIENCE:\n                print(f'  Early stopping a epoch {epoch+1}')\n                break\n    \n    all_train_losses[fold] = train_losses\n    all_val_losses[fold] = val_losses\n    \n    # Évaluer sur la validation de ce fold\n    model.load_state_dict(torch.load(model_path, map_location=DEVICE, weights_only=True))\n    val_loss, val_pos_loss, val_speed_loss, val_mu, val_sigma, val_targets, val_spd_pred, val_spd_true = eval_epoch(\n        model, fold_val_loader, pos_criterion, speed_criterion, SPEED_WEIGHT, DEVICE\n    )\n    val_eucl = np.sqrt((val_targets[:, 0] - val_mu[:, 0])**2 + (val_targets[:, 1] - val_mu[:, 1])**2)\n    val_speed_r2 = r2_score(val_spd_true, val_spd_pred)\n    \n    fold_results.append({\n        'fold': fold + 1,\n        'best_val_loss': best_val_loss,\n        'val_eucl_mean': val_eucl.mean(),\n        'val_r2_x': r2_score(val_targets[:, 0], val_mu[:, 0]),\n        'val_r2_y': r2_score(val_targets[:, 1], val_mu[:, 1]),\n        'val_speed_r2': val_speed_r2,\n        'epochs': len(train_losses),\n    })\n    print(f'  Best val loss: {best_val_loss:.5f} | Eucl: {val_eucl.mean():.4f} | R2: X={fold_results[-1][\"val_r2_x\"]:.4f}, Y={fold_results[-1][\"val_r2_y\"]:.4f} | Speed R2={val_speed_r2:.4f}')\n\n# Résumé des folds\nprint(f'\\n{\"=\"*60}')\nprint(f'RESUME CROSS-VALIDATION ({N_FOLDS} folds)')\nprint(f'{\"=\"*60}')\nfor r in fold_results:\n    print(f'  Fold {r[\"fold\"]}: NLL={r[\"best_val_loss\"]:.5f} | Eucl={r[\"val_eucl_mean\"]:.4f} | R2_X={r[\"val_r2_x\"]:.4f} | R2_Y={r[\"val_r2_y\"]:.4f} | Speed_R2={r[\"val_speed_r2\"]:.4f} | Epochs={r[\"epochs\"]}')\n\nmean_eucl = np.mean([r['val_eucl_mean'] for r in fold_results])\nstd_eucl = np.std([r['val_eucl_mean'] for r in fold_results])\nmean_r2_x = np.mean([r['val_r2_x'] for r in fold_results])\nmean_r2_y = np.mean([r['val_r2_y'] for r in fold_results])\nmean_speed_r2 = np.mean([r['val_speed_r2'] for r in fold_results])\nprint(f'\\n  Moyenne : Eucl={mean_eucl:.4f} (+/- {std_eucl:.4f}) | R2_X={mean_r2_x:.4f} | R2_Y={mean_r2_y:.4f} | Speed_R2={mean_speed_r2:.4f}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Courbes d'entraînement par fold\nfig, axes = plt.subplots(1, 2, figsize=(16, 5))\n\ncolors = plt.cm.tab10(np.linspace(0, 1, N_FOLDS))\n\nfor fold in range(N_FOLDS):\n    axes[0].plot(all_train_losses[fold], color=colors[fold], linewidth=1.5, label=f'Fold {fold+1}')\n    axes[1].plot(all_val_losses[fold], color=colors[fold], linewidth=1.5, label=f'Fold {fold+1}')\n\naxes[0].set_xlabel('Epoch'); axes[0].set_ylabel('Gaussian NLL Loss')\naxes[0].set_title('Train Loss par fold'); axes[0].legend(); axes[0].grid(True, alpha=0.3)\n\naxes[1].set_xlabel('Epoch'); axes[1].set_ylabel('Gaussian NLL Loss')\naxes[1].set_title('Validation Loss par fold'); axes[1].legend(); axes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Évaluation finale sur le test set\n\nOn charge les 5 modèles entraînés et on fait la moyenne de leurs prédictions (ensemble). Pour les sigmas, on combine les variances : `sigma_ensemble² = mean(sigma_i²) + mean((mu_i - mu_mean)²)` (loi de la variance totale)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Évaluation ensemble des N folds sur le test set\npos_criterion = nn.GaussianNLLLoss()\nspeed_criterion = nn.MSELoss()\nall_fold_mu = []\nall_fold_sigma = []\nall_fold_speed = []\n\nfor fold in range(N_FOLDS):\n    model_path = f'../outputs/best_transformer_02e_fold{fold+1}.pt'\n    model = SpikeTransformer(\n        nGroups, nChannelsPerGroup,\n        embed_dim=EMBED_DIM, nhead=NHEAD, num_layers=NUM_LAYERS,\n        dropout=DROPOUT, spike_dropout=SPIKE_DROPOUT, noise_std=NOISE_STD\n    ).to(DEVICE)\n    model.load_state_dict(torch.load(model_path, map_location=DEVICE, weights_only=True))\n    \n    _, _, _, fold_mu, fold_sigma, y_test, fold_spd_pred, y_speed_test = eval_epoch(\n        model, test_loader, pos_criterion, speed_criterion, SPEED_WEIGHT, DEVICE\n    )\n    all_fold_mu.append(fold_mu)\n    all_fold_sigma.append(fold_sigma)\n    all_fold_speed.append(fold_spd_pred)\n    \n    fold_eucl = np.sqrt((y_test[:, 0] - fold_mu[:, 0])**2 + (y_test[:, 1] - fold_mu[:, 1])**2)\n    fold_spd_r2 = r2_score(y_speed_test, fold_spd_pred)\n    print(f'Fold {fold+1} sur test: Eucl={fold_eucl.mean():.4f}, Speed R2={fold_spd_r2:.4f}')\n\n# Ensemble : moyenne des mu et speed\nall_fold_mu = np.stack(all_fold_mu)        # (N_FOLDS, n_test, 2)\nall_fold_sigma = np.stack(all_fold_sigma)  # (N_FOLDS, n_test, 2)\nall_fold_speed = np.stack(all_fold_speed)  # (N_FOLDS, n_test)\n\ny_pred = all_fold_mu.mean(axis=0)          # (n_test, 2)\ny_speed_pred = all_fold_speed.mean(axis=0) # (n_test,)\n\n# Sigma ensemble (loi de la variance totale)\nmean_var = (all_fold_sigma ** 2).mean(axis=0)  # aleatoric\nvar_mu = all_fold_mu.var(axis=0)                # epistemic\ny_sigma = np.sqrt(mean_var + var_mu)\n\n# Métriques position\nmse_x = mean_squared_error(y_test[:, 0], y_pred[:, 0])\nmse_y = mean_squared_error(y_test[:, 1], y_pred[:, 1])\nmae_x = mean_absolute_error(y_test[:, 0], y_pred[:, 0])\nmae_y = mean_absolute_error(y_test[:, 1], y_pred[:, 1])\nr2_x = r2_score(y_test[:, 0], y_pred[:, 0])\nr2_y = r2_score(y_test[:, 1], y_pred[:, 1])\neucl_errors = np.sqrt((y_test[:, 0] - y_pred[:, 0])**2 + (y_test[:, 1] - y_pred[:, 1])**2)\n\n# Métriques vitesse\nspeed_r2 = r2_score(y_speed_test, y_speed_pred)\nspeed_mse = mean_squared_error(y_speed_test, y_speed_pred)\nspeed_mae = mean_absolute_error(y_speed_test, y_speed_pred)\n\nprint(f'\\n=== Transformer 02e : Multi-tâches + Spike Dropout {SPIKE_DROPOUT:.0%} + Noise std={NOISE_STD} — Ensemble ({N_FOLDS} folds) ===')\nprint(f'\\n--- Position ---')\nprint(f'  MSE  : X={mse_x:.5f}, Y={mse_y:.5f}')\nprint(f'  MAE  : X={mae_x:.4f}, Y={mae_y:.4f}')\nprint(f'  R²   : X={r2_x:.4f}, Y={r2_y:.4f}')\nprint(f'  Eucl : mean={eucl_errors.mean():.4f}, median={np.median(eucl_errors):.4f}, p90={np.percentile(eucl_errors, 90):.4f}')\nprint(f'\\n--- Vitesse ---')\nprint(f'  R²   : {speed_r2:.4f}')\nprint(f'  MSE  : {speed_mse:.6f}')\nprint(f'  MAE  : {speed_mae:.4f}')\nprint(f'\\n--- Incertitude ---')\nprint(f'  Sigma moyen : X={y_sigma[:, 0].mean():.4f}, Y={y_sigma[:, 1].mean():.4f}')\nprint(f'  Sigma median: X={np.median(y_sigma[:, 0]):.4f}, Y={np.median(y_sigma[:, 1]):.4f}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Scatter pred vs true (position + vitesse) ---\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\naxes[0].scatter(y_test[:, 0], y_pred[:, 0], s=1, alpha=0.3)\naxes[0].plot([0, 1], [0, 1], 'r--', linewidth=2)\naxes[0].set_xlabel('True X'); axes[0].set_ylabel('Predicted X')\naxes[0].set_title(f'02e Multi-task - Position X (R²={r2_x:.3f})')\naxes[0].set_aspect('equal')\n\naxes[1].scatter(y_test[:, 1], y_pred[:, 1], s=1, alpha=0.3)\naxes[1].plot([0, 1], [0, 1], 'r--', linewidth=2)\naxes[1].set_xlabel('True Y'); axes[1].set_ylabel('Predicted Y')\naxes[1].set_title(f'02e Multi-task - Position Y (R²={r2_y:.3f})')\naxes[1].set_aspect('equal')\n\naxes[2].scatter(y_speed_test, y_speed_pred, s=1, alpha=0.3, color='green')\ns_min, s_max = y_speed_test.min(), y_speed_test.max()\naxes[2].plot([s_min, s_max], [s_min, s_max], 'r--', linewidth=2)\naxes[2].set_xlabel('True Speed'); axes[2].set_ylabel('Predicted Speed')\naxes[2].set_title(f'02e Multi-task - Vitesse (R²={speed_r2:.3f})')\naxes[2].set_aspect('equal')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Trajectoire prédite vs vraie avec incertitude + vitesse ---\nsegment = slice(0, 500)\nseg_idx = np.arange(500)\n\nfig, axes = plt.subplots(3, 2, figsize=(16, 16))\n\n# 1. Trajectoire 2D\naxes[0, 0].plot(y_test[segment, 0], y_test[segment, 1], 'b-', alpha=0.5, label='Vraie trajectoire', linewidth=1)\naxes[0, 0].plot(y_pred[segment, 0], y_pred[segment, 1], 'r-', alpha=0.5, label='Prediction (mu)', linewidth=1)\naxes[0, 0].set_xlabel('X'); axes[0, 0].set_ylabel('Y')\naxes[0, 0].set_title('Trajectoire (500 premiers points test)')\naxes[0, 0].legend()\naxes[0, 0].set_aspect('equal')\n\n# 2. Position X avec bande d'incertitude (mu +/- 2*sigma)\naxes[0, 1].plot(seg_idx, y_test[segment, 0], 'b-', label='Vrai X', linewidth=1.5)\naxes[0, 1].plot(seg_idx, y_pred[segment, 0], 'r-', alpha=0.7, label='Prediction (mu)', linewidth=1)\naxes[0, 1].fill_between(seg_idx, \n                         y_pred[segment, 0] - 2 * y_sigma[segment, 0],\n                         y_pred[segment, 0] + 2 * y_sigma[segment, 0],\n                         alpha=0.2, color='red', label='Incertitude (2 sigma)')\naxes[0, 1].set_xlabel('Index'); axes[0, 1].set_ylabel('Position X')\naxes[0, 1].set_title('Position X avec incertitude')\naxes[0, 1].legend()\n\n# 3. Position Y avec bande d'incertitude\naxes[1, 0].plot(seg_idx, y_test[segment, 1], 'b-', label='Vrai Y', linewidth=1.5)\naxes[1, 0].plot(seg_idx, y_pred[segment, 1], 'r-', alpha=0.7, label='Prediction (mu)', linewidth=1)\naxes[1, 0].fill_between(seg_idx,\n                         y_pred[segment, 1] - 2 * y_sigma[segment, 1],\n                         y_pred[segment, 1] + 2 * y_sigma[segment, 1],\n                         alpha=0.2, color='red', label='Incertitude (2 sigma)')\naxes[1, 0].set_xlabel('Index'); axes[1, 0].set_ylabel('Position Y')\naxes[1, 0].set_title('Position Y avec incertitude')\naxes[1, 0].legend()\n\n# 4. Erreur euclidienne vs sigma moyen\nsigma_mean = (y_sigma[:, 0] + y_sigma[:, 1]) / 2\naxes[1, 1].scatter(sigma_mean, eucl_errors, s=1, alpha=0.3)\naxes[1, 1].set_xlabel('Sigma moyen predit'); axes[1, 1].set_ylabel('Erreur euclidienne reelle')\naxes[1, 1].set_title('Calibration : incertitude vs erreur')\nsigma_range = np.linspace(0, sigma_mean.max(), 100)\naxes[1, 1].plot(sigma_range, 2 * sigma_range, 'r--', label='y = 2*sigma', linewidth=1.5)\naxes[1, 1].legend()\n\n# 5. Vitesse prédite vs vraie (temporel)\naxes[2, 0].plot(seg_idx, y_speed_test[segment], 'b-', label='Vraie vitesse', linewidth=1.5)\naxes[2, 0].plot(seg_idx, y_speed_pred[segment], 'r-', alpha=0.7, label='Prediction', linewidth=1)\naxes[2, 0].set_xlabel('Index'); axes[2, 0].set_ylabel('Vitesse')\naxes[2, 0].set_title('Vitesse (500 premiers points test)')\naxes[2, 0].legend()\n\n# 6. Distribution des erreurs de vitesse\nspeed_errors = np.abs(y_speed_test - y_speed_pred)\naxes[2, 1].hist(speed_errors, bins=50, alpha=0.7, color='green', edgecolor='white')\naxes[2, 1].axvline(speed_errors.mean(), color='red', linestyle='--', label=f'MAE={speed_errors.mean():.4f}')\naxes[2, 1].set_xlabel('Erreur absolue vitesse'); axes[2, 1].set_ylabel('Count')\naxes[2, 1].set_title('Distribution des erreurs de vitesse')\naxes[2, 1].legend()\n\nplt.tight_layout()\nplt.show()\n\n# Calibration : quel % des vraies positions tombe dans l'intervalle predit ?\nin_1sigma = np.mean(eucl_errors < sigma_mean)\nin_2sigma = np.mean(eucl_errors < 2 * sigma_mean)\nin_3sigma = np.mean(eucl_errors < 3 * sigma_mean)\nprint(f'Calibration de l\\'incertitude :')\nprint(f'  Erreur < 1*sigma : {in_1sigma:.1%} (attendu ~39% pour gaussienne 2D)')\nprint(f'  Erreur < 2*sigma : {in_2sigma:.1%} (attendu ~86%)')\nprint(f'  Erreur < 3*sigma : {in_3sigma:.1%} (attendu ~99%)')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Heatmap des erreurs spatiales ---\nfig, ax = plt.subplots(figsize=(8, 7))\n\nnbins = 20\nerror_map = np.full((nbins, nbins), np.nan)\ncount_map = np.zeros((nbins, nbins))\nx_edges = np.linspace(0, 1, nbins + 1)\ny_edges = np.linspace(0, 1, nbins + 1)\n\nfor i in range(len(y_test)):\n    xi = np.clip(np.searchsorted(x_edges, y_test[i, 0]) - 1, 0, nbins - 1)\n    yi = np.clip(np.searchsorted(y_edges, y_test[i, 1]) - 1, 0, nbins - 1)\n    if np.isnan(error_map[yi, xi]):\n        error_map[yi, xi] = 0\n    error_map[yi, xi] += eucl_errors[i]\n    count_map[yi, xi] += 1\n\nmean_error_map = np.where(count_map > 0, error_map / count_map, np.nan)\n\nim = ax.imshow(mean_error_map, origin='lower', aspect='equal', cmap='RdYlGn_r', extent=[0, 1, 0, 1])\nax.set_xlabel('X'); ax.set_ylabel('Y')\nax.set_title('02e Multi-task - Erreur moyenne par position')\nplt.colorbar(im, ax=ax, label='Erreur euclidienne moyenne')\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Sauvegarde des prédictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "np.save('../outputs/preds_transformer_02e.npy', y_pred)\nnp.save('../outputs/sigma_transformer_02e.npy', y_sigma)\nnp.save('../outputs/y_test_transformer_02e.npy', y_test)\nnp.save('../outputs/speed_pred_transformer_02e.npy', y_speed_pred)\nnp.save('../outputs/speed_test_transformer_02e.npy', y_speed_test)\nprint(f'Predictions ensemble ({N_FOLDS} folds) sauvegardees.')\nprint(f'  preds_transformer_02e.npy : mu ensemble ({y_pred.shape})')\nprint(f'  sigma_transformer_02e.npy : sigma ensemble ({y_sigma.shape})')\nprint(f'  y_test_transformer_02e.npy : targets ({y_test.shape})')\nprint(f'  speed_pred_transformer_02e.npy : speed predictions ({y_speed_pred.shape})')\nprint(f'  speed_test_transformer_02e.npy : speed targets ({y_speed_test.shape})')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 11. Analyse de l'incertitude\n\nAvec la sortie probabiliste (Gaussian NLL), le modèle prédit non seulement une position (mu) mais aussi son incertitude (sigma). Cela permet de :\n- **Identifier les zones difficiles** : là où sigma est élevé, le modèle \"sait qu'il ne sait pas\"\n- **Pondérer les prédictions** : en aval, on peut donner moins de poids aux prédictions incertaines\n- **Détecter les anomalies** : des sigma anormalement élevés peuvent signaler des données hors distribution\n\nLe multi-task learning devrait améliorer la calibration : en apprenant aussi la vitesse, le modèle développe des représentations internes plus riches qui permettent une meilleure estimation de l'incertitude."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Heatmap de l'incertitude moyenne par position ---\nfig, axes = plt.subplots(1, 2, figsize=(16, 7))\n\nnbins = 20\nx_edges = np.linspace(0, 1, nbins + 1)\ny_edges = np.linspace(0, 1, nbins + 1)\n\nfor ax_idx, (title, values) in enumerate([\n    ('Erreur euclidienne moyenne', eucl_errors),\n    ('Sigma moyen predit', (y_sigma[:, 0] + y_sigma[:, 1]) / 2)\n]):\n    val_map = np.full((nbins, nbins), np.nan)\n    count_map = np.zeros((nbins, nbins))\n    \n    for i in range(len(y_test)):\n        xi = np.clip(np.searchsorted(x_edges, y_test[i, 0]) - 1, 0, nbins - 1)\n        yi = np.clip(np.searchsorted(y_edges, y_test[i, 1]) - 1, 0, nbins - 1)\n        if np.isnan(val_map[yi, xi]):\n            val_map[yi, xi] = 0\n        val_map[yi, xi] += values[i]\n        count_map[yi, xi] += 1\n    \n    mean_map = np.where(count_map > 0, val_map / count_map, np.nan)\n    \n    im = axes[ax_idx].imshow(mean_map, origin='lower', aspect='equal', \n                              cmap='RdYlGn_r', extent=[0, 1, 0, 1])\n    axes[ax_idx].set_xlabel('X'); axes[ax_idx].set_ylabel('Y')\n    axes[ax_idx].set_title(f'02e Multi-task - {title}')\n    plt.colorbar(im, ax=axes[ax_idx])\n\nplt.tight_layout()\nplt.show()\n\n# Correlation entre sigma predit et erreur reelle\nfrom scipy.stats import spearmanr\ncorr, pval = spearmanr(sigma_mean, eucl_errors)\nprint(f'Correlation de Spearman (sigma vs erreur) : {corr:.3f} (p={pval:.2e})')\nprint(f'  -> {\"Bonne\" if corr > 0.3 else \"Faible\"} calibration : le modele {\"sait\" if corr > 0.3 else \"ne sait pas bien\"} quand il se trompe')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Distribution des sigma predits ---\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\naxes[0].hist(y_sigma[:, 0], bins=50, alpha=0.7, color='steelblue', edgecolor='white')\naxes[0].axvline(y_sigma[:, 0].mean(), color='red', linestyle='--', label=f'Moyenne={y_sigma[:, 0].mean():.4f}')\naxes[0].set_xlabel('Sigma X'); axes[0].set_ylabel('Count')\naxes[0].set_title('Distribution de l\\'incertitude sur X')\naxes[0].legend()\n\naxes[1].hist(y_sigma[:, 1], bins=50, alpha=0.7, color='coral', edgecolor='white')\naxes[1].axvline(y_sigma[:, 1].mean(), color='red', linestyle='--', label=f'Moyenne={y_sigma[:, 1].mean():.4f}')\naxes[1].set_xlabel('Sigma Y'); axes[1].set_ylabel('Count')\naxes[1].set_title('Distribution de l\\'incertitude sur Y')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 12. Interpretation\n\n### Modifications apportées (02e vs 02)\n\n**1. Multi-tâches (position + vitesse)** : le modèle prédit simultanément la position (x, y) via Gaussian NLL et la vitesse via MSE. La loss combinée est `L = L_pos + 0.5 * L_speed`. Inspiré de [Frey et al. 2021](https://elifesciences.org/articles/66551) qui montrent que le multi-task (position + direction + vitesse) améliore le décodage spatial hippocampique.\n\n**Pourquoi le multi-task aide :**\n- La vitesse est une variable cinématique liée à la position (c'est sa dérivée temporelle)\n- Les place cells hippocampiques encodent conjointement position et vitesse\n- Forcer le backbone Transformer à extraire des features utiles pour les deux tâches agit comme un **régulariseur inductif** : le modèle doit apprendre des représentations plus riches et moins spécifiques à une seule tâche\n\n**2. Spike dropout (15%)** : masquage aléatoire de spikes actifs pendant le training.\n\n**3. Gaussian noise (std=0.5)** : bruit additif N(0, 0.5²) sur les waveforms bruts pendant le training.\n\n### Résumé des modifications cumulées\n| Composant | 02 (baseline) | 02e (ce notebook) |\n|-----------|--------------|-------------------|\n| Spike dropout | Non | Oui (15%) |\n| Gaussian noise | Non | Oui (std=0.5) |\n| Multi-tâches | Non | Oui (position + vitesse) |\n| Loss | Gaussian NLL | Gaussian NLL + 0.5 * MSE(speed) |\n| Nb paramètres | ~200k | ~204k (+speed_head) |"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}