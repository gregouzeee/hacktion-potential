{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2 : Transformer sur séquence de spikes\n",
    "\n",
    "## Approche\n",
    "\n",
    "On traite chaque fenêtre de 108ms comme une **séquence chronologique de spikes**. Chaque spike est encodé par un petit CNN 1D (spécifique à son shank), puis enrichi avec un embedding de shank et un positional encoding. La séquence est traitée par un Transformer encoder.\n",
    "\n",
    "**Pourquoi un Transformer ?** L'information spatiale dans l'hippocampe est codée par des *ensembles de neurones* (population coding). Le mécanisme de self-attention permet à chaque spike de \"voir\" tous les autres et de capturer les co-activations entre shanks. C'est impossible avec un simple average pooling.\n",
    "\n",
    "**Framework** : PyTorch, accéléré sur MPS (Apple Silicon)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports et configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Reproductibilité\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Device\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device('mps')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "print(f'Device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# --- Chargement des données ---\n",
    "# Les données doivent être dans data/ (via: python download_data.py)\n",
    "LOCAL_DIR = os.path.join(os.path.abspath('..'), 'data')\n",
    "\n",
    "PARQUET_NAME = \"M1199_PAG_stride4_win108_test.parquet\"\n",
    "JSON_NAME = \"M1199_PAG.json\"\n",
    "\n",
    "PARQUET_FILE = os.path.join(LOCAL_DIR, PARQUET_NAME)\n",
    "JSON_FILE = os.path.join(LOCAL_DIR, JSON_NAME)\n",
    "\n",
    "if not os.path.exists(PARQUET_FILE):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Données introuvables dans {LOCAL_DIR}/\\n\"\n",
    "        f\"Lancez d'abord: python download_data.py\"\n",
    "    )\n",
    "\n",
    "print(f\"Chargement depuis {LOCAL_DIR}/\")\n",
    "df = pd.read_parquet(PARQUET_FILE)\n",
    "with open(JSON_FILE, \"r\") as f:\n",
    "    params = json.load(f)\n",
    "\n",
    "print(f\"Shape: {df.shape}\")\n",
    "\n",
    "nGroups = params['nGroups']\n",
    "nChannelsPerGroup = [params[f'group{g}']['nChannels'] for g in range(nGroups)]\n",
    "print(f\"nGroups={nGroups}, nChannelsPerGroup={nChannelsPerGroup}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement et filtrage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Filtrage speedMask (on ne garde que les exemples en mouvement)\nspeed_masks = np.array([x[0] for x in df['speedMask']])\ndf_moving = df[speed_masks].reset_index(drop=True)\nprint(f'Exemples en mouvement : {len(df_moving)}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing : reconstruction de la séquence chronologique\n",
    "\n",
    "Pour chaque fenêtre, on reconstruit la séquence ordonnée de spikes :\n",
    "1. On reshape les waveforms de chaque shank : `groupX` → `(n_spikes, nCh, 32)`\n",
    "2. On parcourt le tableau `groups` dans l'ordre chronologique\n",
    "3. Pour chaque timestep, on récupère le waveform correspondant via `indicesX`\n",
    "4. On ignore les indices à 0 (padding)\n",
    "\n",
    "Le résultat : une liste de tuples `(waveform, shank_id)` ordonnée dans le temps.\n",
    "\n",
    "Comme les shanks ont des nombres de canaux différents (6 ou 4), on utilise un **encodeur par shank** qui projette chaque waveform vers un espace commun de dimension D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premier exemple : 78 spikes réels dans la séquence\n",
      "Shanks utilisés : {0, 1, 2, 3}\n",
      "Premier spike : shank=1, shape=(4, 32)\n"
     ]
    }
   ],
   "source": [
    "def reconstruct_sequence(row, nGroups, nChannelsPerGroup, max_seq_len=128):\n",
    "    \"\"\"\n",
    "    Reconstruit la séquence chronologique de spikes.\n",
    "    \n",
    "    Retourne:\n",
    "        waveforms_per_group: dict {g: np.array (n_spikes_g, nCh, 32)}\n",
    "        sequence_groups: np.array (seq_len,) - quel shank à chaque timestep\n",
    "        sequence_indices: np.array (seq_len,) - quel index de spike dans le shank\n",
    "        seq_len: longueur effective de la séquence\n",
    "    \"\"\"\n",
    "    groups = row['groups']\n",
    "    length = min(len(groups), max_seq_len)\n",
    "    \n",
    "    # Reshape des waveforms par shank\n",
    "    waveforms = {}\n",
    "    for g in range(nGroups):\n",
    "        nCh = nChannelsPerGroup[g]\n",
    "        raw = row[f'group{g}']\n",
    "        waveforms[g] = raw.reshape(-1, nCh, 32)\n",
    "    \n",
    "    # Reconstruction de la séquence\n",
    "    seq_waveforms = []  # liste de (nCh, 32) - taille variable par spike\n",
    "    seq_shank_ids = []\n",
    "    \n",
    "    for t in range(length):\n",
    "        g = int(groups[t])\n",
    "        idx = int(row[f'indices{g}'][t])\n",
    "        if idx > 0 and idx <= waveforms[g].shape[0]:\n",
    "            seq_waveforms.append((waveforms[g][idx - 1], g))  # idx est 1-based\n",
    "            seq_shank_ids.append(g)\n",
    "    \n",
    "    return seq_waveforms, seq_shank_ids\n",
    "\n",
    "# Test rapide\n",
    "wf, sids = reconstruct_sequence(df_moving.iloc[0], nGroups, nChannelsPerGroup)\n",
    "print(f'Premier exemple : {len(wf)} spikes réels dans la séquence')\n",
    "print(f'Shanks utilisés : {set(sids)}')\n",
    "print(f'Premier spike : shank={wf[0][1]}, shape={wf[0][0].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset PyTorch\n",
    "\n",
    "Le Dataset gère :\n",
    "- La reconstruction de séquence à la volée (pas besoin de stocker tout en mémoire)\n",
    "- Le padding des waveforms à la taille max de canaux (6) pour pouvoir empiler dans un tenseur\n",
    "- Le retour du masque de padding pour l'attention\n",
    "\n",
    "On utilise un `collate_fn` custom pour gérer les séquences de longueur variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset et collate_fn définis.\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQ_LEN = 128  # On tronque les séquences trop longues (max observé = 190)\n",
    "MAX_CHANNELS = max(nChannelsPerGroup)  # 6\n",
    "\n",
    "class SpikeSequenceDataset(Dataset):\n",
    "    def __init__(self, dataframe, nGroups, nChannelsPerGroup, max_seq_len=MAX_SEQ_LEN):\n",
    "        self.df = dataframe\n",
    "        self.nGroups = nGroups\n",
    "        self.nChannelsPerGroup = nChannelsPerGroup\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # Pré-extraire les targets\n",
    "        self.targets = np.array([[x[0], x[1]] for x in dataframe['pos']], dtype=np.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        seq, shank_ids = reconstruct_sequence(row, self.nGroups, self.nChannelsPerGroup, self.max_seq_len)\n",
    "        \n",
    "        seq_len = len(seq)\n",
    "        if seq_len == 0:\n",
    "            # Cas dégénéré : aucun spike valide\n",
    "            seq_len = 1\n",
    "            waveforms = np.zeros((1, MAX_CHANNELS, 32), dtype=np.float32)\n",
    "            shank_ids_arr = np.array([0], dtype=np.int64)\n",
    "        else:\n",
    "            # Padder les waveforms à MAX_CHANNELS canaux\n",
    "            waveforms = np.zeros((seq_len, MAX_CHANNELS, 32), dtype=np.float32)\n",
    "            shank_ids_arr = np.array(shank_ids, dtype=np.int64)\n",
    "            for t, (wf, g) in enumerate(seq):\n",
    "                nCh = wf.shape[0]\n",
    "                waveforms[t, :nCh, :] = wf\n",
    "        \n",
    "        target = self.targets[idx]\n",
    "        return {\n",
    "            'waveforms': torch.from_numpy(waveforms),      # (seq_len, MAX_CH, 32)\n",
    "            'shank_ids': torch.from_numpy(shank_ids_arr),   # (seq_len,)\n",
    "            'seq_len': seq_len,\n",
    "            'target': torch.from_numpy(target)              # (2,)\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Collate avec padding dynamique à la longueur max du batch.\"\"\"\n",
    "    max_len = max(item['seq_len'] for item in batch)\n",
    "    batch_size = len(batch)\n",
    "    \n",
    "    waveforms = torch.zeros(batch_size, max_len, MAX_CHANNELS, 32)\n",
    "    shank_ids = torch.zeros(batch_size, max_len, dtype=torch.long)\n",
    "    mask = torch.ones(batch_size, max_len, dtype=torch.bool)  # True = padding (à ignorer)\n",
    "    targets = torch.stack([item['target'] for item in batch])\n",
    "    \n",
    "    for i, item in enumerate(batch):\n",
    "        sl = item['seq_len']\n",
    "        waveforms[i, :sl] = item['waveforms']\n",
    "        shank_ids[i, :sl] = item['shank_ids']\n",
    "        mask[i, :sl] = False  # False = pas de padding\n",
    "    \n",
    "    return {\n",
    "        'waveforms': waveforms,\n",
    "        'shank_ids': shank_ids,\n",
    "        'mask': mask,\n",
    "        'targets': targets\n",
    "    }\n",
    "\n",
    "print('Dataset et collate_fn définis.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Architecture du modèle\n\nLe modèle a 4 composants :\n\n1. **Spike Encoder** : un petit CNN 1D par shank qui projette chaque waveform `(nCh, 32)` vers un embedding de dimension D. Chaque shank a son propre encodeur car ils ont des géométries différentes.\n\n2. **Shank Embedding** : un embedding appris (comme un \"token type\" en BERT) qui permet au modèle de savoir de quel shank provient chaque spike.\n\n3. **Positional Encoding** : encoding sinusoïdal classique basé sur la position dans la séquence. Informe le modèle de l'ordre temporel des spikes.\n\n4. **Transformer Encoder** : 2 couches de self-attention (4 heads, dim=64). Chaque spike peut \"voir\" tous les autres pour capturer les co-activations.\n\n5. **Readout probabiliste** : average pooling masqué (on ignore le padding) → deux têtes séparées :\n   - **mu_head** → (mu_x, mu_y) : la position prédite\n   - **log_sigma_head** → (log_sigma_x, log_sigma_y) : l'incertitude (passée par exp() pour garantir sigma > 0)\n   \n   La loss est la **Gaussian NLL** : `0.5 * [log(sigma²) + (y - mu)² / sigma²]`, ce qui permet au modèle d'exprimer son incertitude et de produire des probabilités en sortie."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class SpikeEncoder(nn.Module):\n    \"\"\"Encode un waveform (MAX_CH, 32) en un vecteur de dimension embed_dim.\n    Utilise un CNN 1D sur la dimension temporelle (32 points).\"\"\"\n    \n    def __init__(self, n_channels, embed_dim):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv1d(n_channels, 32, kernel_size=5, padding=2),\n            nn.ReLU(),\n            nn.Conv1d(32, embed_dim, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool1d(1)  # (batch, embed_dim, 1)\n        )\n    \n    def forward(self, x):\n        # x: (batch * seq_len, n_channels, 32)\n        return self.conv(x).squeeze(-1)  # (batch * seq_len, embed_dim)\n\n\nclass PositionalEncoding(nn.Module):\n    \"\"\"Sinusoidal positional encoding.\"\"\"\n    \n    def __init__(self, embed_dim, max_len=256):\n        super().__init__()\n        pe = torch.zeros(max_len, embed_dim)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe.unsqueeze(0))  # (1, max_len, embed_dim)\n    \n    def forward(self, x):\n        # x: (batch, seq_len, embed_dim)\n        return x + self.pe[:, :x.size(1)]\n\n\nclass SpikeTransformer(nn.Module):\n    def __init__(self, nGroups, nChannelsPerGroup, embed_dim=64, nhead=4, \n                 num_layers=2, dropout=0.2, max_channels=MAX_CHANNELS):\n        super().__init__()\n        self.nGroups = nGroups\n        self.embed_dim = embed_dim\n        self.max_channels = max_channels\n        \n        # Un encodeur par shank\n        self.spike_encoders = nn.ModuleList([\n            SpikeEncoder(max_channels, embed_dim) for _ in range(nGroups)\n        ])\n        \n        # Embedding de shank\n        self.shank_embedding = nn.Embedding(nGroups, embed_dim)\n        \n        # Positional encoding\n        self.pos_encoding = PositionalEncoding(embed_dim)\n        \n        # Transformer encoder\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=embed_dim, nhead=nhead, dim_feedforward=embed_dim * 4,\n            dropout=dropout, batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(\n            encoder_layer, num_layers=num_layers, enable_nested_tensor=False\n        )\n        \n        # Readout : produit mu (2) et log_sigma (2)\n        self.mu_head = nn.Sequential(\n            nn.Linear(embed_dim, embed_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(embed_dim, 2)  # (mu_x, mu_y)\n        )\n        self.log_sigma_head = nn.Sequential(\n            nn.Linear(embed_dim, embed_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(embed_dim, 2)  # (log_sigma_x, log_sigma_y)\n        )\n    \n    def forward(self, waveforms, shank_ids, mask):\n        \"\"\"\n        Args:\n            waveforms: (batch, seq_len, max_ch, 32)\n            shank_ids: (batch, seq_len)\n            mask: (batch, seq_len) - True = padding\n        Returns:\n            mu: (batch, 2) - position prédite\n            sigma: (batch, 2) - incertitude (écart-type)\n        \"\"\"\n        batch_size, seq_len = waveforms.shape[:2]\n        \n        # --- Encode chaque spike avec l'encodeur de son shank ---\n        embeddings = torch.zeros(batch_size, seq_len, self.embed_dim, device=waveforms.device)\n        \n        for g in range(self.nGroups):\n            group_mask = (shank_ids == g) & (~mask)\n            if group_mask.any():\n                group_wf = waveforms[group_mask]\n                group_emb = self.spike_encoders[g](group_wf)\n                embeddings[group_mask] = group_emb\n        \n        # --- Ajouter le shank embedding ---\n        shank_emb = self.shank_embedding(shank_ids)\n        embeddings = embeddings + shank_emb\n        \n        # --- Positional encoding ---\n        embeddings = self.pos_encoding(embeddings)\n        \n        # --- Transformer ---\n        encoded = self.transformer(embeddings, src_key_padding_mask=mask)\n        \n        # --- Masked average pooling ---\n        active_mask = (~mask).unsqueeze(-1).float()\n        pooled = (encoded * active_mask).sum(dim=1) / (active_mask.sum(dim=1) + 1e-8)\n        \n        # --- Sorties : mu et sigma ---\n        mu = self.mu_head(pooled)\n        log_sigma = self.log_sigma_head(pooled)\n        sigma = torch.exp(log_sigma)  # sigma > 0\n        \n        return mu, sigma\n\n\n# Test rapide\nmodel = SpikeTransformer(nGroups, nChannelsPerGroup, embed_dim=64, nhead=4, num_layers=2)\nn_params = sum(p.numel() for p in model.parameters())\nprint(f'Modèle créé : {n_params:,} paramètres')\nprint(model)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Split et DataLoaders\n\nOn sépare les données en **90% train / 10% test** (split temporel : les 10% les plus récents servent de test).\n\nSur les 90% de train, on effectue une **validation croisée à 5 folds** (KFold, random seed 41) pour :\n- Estimer la performance de manière robuste\n- Entraîner 5 modèles qui seront ensuite utilisés en ensemble sur le test set"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.model_selection import KFold\n\n# Split temporel 90/10\nsplit_idx = int(len(df_moving) * 0.9)\ndf_train_full = df_moving.iloc[:split_idx].reset_index(drop=True)\ndf_test = df_moving.iloc[split_idx:].reset_index(drop=True)\n\nprint(f'Train (full) : {len(df_train_full)} exemples')\nprint(f'Test         : {len(df_test)} exemples')\n\n# KFold sur les 90% de train\nN_FOLDS = 5\nkf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=41)\n\nBATCH_SIZE = 64\n\n# Préparer le test loader (commun à tous les folds)\ntest_dataset = SpikeSequenceDataset(df_test, nGroups, nChannelsPerGroup)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n                         collate_fn=collate_fn, num_workers=0)\n\n# Afficher la répartition des folds\nfor fold, (train_idx, val_idx) in enumerate(kf.split(df_train_full)):\n    print(f'  Fold {fold+1}: train={len(train_idx)}, val={len(val_idx)}')\n\nprint(f'\\nTest: {len(test_dataset)} exemples, {len(test_loader)} batches')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Entraînement (5-Fold Cross-Validation)\n\nPour chaque fold :\n- On entraîne un modèle sur 4/5 des données train, on valide sur le 1/5 restant\n- **Loss** : Gaussian NLL (negative log-likelihood)\n- **Optimiseur** : AdamW avec weight decay\n- **Scheduler** : OneCycleLR (warmup puis decay)\n- **Early stopping** : on garde le meilleur modèle de chaque fold\n\nÀ la fin, on a 5 modèles qu'on peut ensembler pour le test final."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Hyperparamètres\nEMBED_DIM = 64\nNHEAD = 4\nNUM_LAYERS = 2\nDROPOUT = 0.2\nLR = 1e-3\nWEIGHT_DECAY = 1e-4\nEPOCHS = 30\nPATIENCE = 7  # early stopping\n\nprint(f'Hyperparamètres : embed_dim={EMBED_DIM}, nhead={NHEAD}, layers={NUM_LAYERS}, dropout={DROPOUT}')\nprint(f'Entraînement : {EPOCHS} epochs max, patience={PATIENCE}, LR={LR}')\nprint(f'Loss : GaussianNLLLoss')\nprint(f'Device : {DEVICE}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_epoch(model, loader, optimizer, scheduler, criterion, device):\n    model.train()\n    total_loss = 0\n    n_batches = 0\n    for batch in loader:\n        wf = batch['waveforms'].to(device)\n        sid = batch['shank_ids'].to(device)\n        mask = batch['mask'].to(device)\n        targets = batch['targets'].to(device)\n        \n        optimizer.zero_grad()\n        mu, sigma = model(wf, sid, mask)\n        # GaussianNLLLoss attend (mu, target, variance)\n        loss = criterion(mu, targets, sigma ** 2)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n        \n        total_loss += loss.item()\n        n_batches += 1\n    \n    return total_loss / n_batches\n\n\n@torch.no_grad()\ndef eval_epoch(model, loader, criterion, device):\n    model.eval()\n    total_loss = 0\n    n_batches = 0\n    all_mu = []\n    all_sigma = []\n    all_targets = []\n    for batch in loader:\n        wf = batch['waveforms'].to(device)\n        sid = batch['shank_ids'].to(device)\n        mask = batch['mask'].to(device)\n        targets = batch['targets'].to(device)\n        \n        mu, sigma = model(wf, sid, mask)\n        loss = criterion(mu, targets, sigma ** 2)\n        \n        total_loss += loss.item()\n        n_batches += 1\n        all_mu.append(mu.cpu().numpy())\n        all_sigma.append(sigma.cpu().numpy())\n        all_targets.append(targets.cpu().numpy())\n    \n    return (total_loss / n_batches, \n            np.concatenate(all_mu), \n            np.concatenate(all_sigma), \n            np.concatenate(all_targets))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Boucle d'entraînement avec KFold\nfold_results = []\nall_train_losses = {}\nall_val_losses = {}\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(df_train_full)):\n    print(f'\\n{\"=\"*60}')\n    print(f'FOLD {fold+1}/{N_FOLDS}')\n    print(f'{\"=\"*60}')\n    \n    # Créer les datasets pour ce fold\n    df_fold_train = df_train_full.iloc[train_idx].reset_index(drop=True)\n    df_fold_val = df_train_full.iloc[val_idx].reset_index(drop=True)\n    \n    fold_train_dataset = SpikeSequenceDataset(df_fold_train, nGroups, nChannelsPerGroup)\n    fold_val_dataset = SpikeSequenceDataset(df_fold_val, nGroups, nChannelsPerGroup)\n    \n    fold_train_loader = DataLoader(fold_train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n                                   collate_fn=collate_fn, num_workers=0)\n    fold_val_loader = DataLoader(fold_val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n                                 collate_fn=collate_fn, num_workers=0)\n    \n    print(f'  Train: {len(fold_train_dataset)}, Val: {len(fold_val_dataset)}')\n    \n    # Nouveau modèle pour chaque fold\n    model = SpikeTransformer(\n        nGroups, nChannelsPerGroup,\n        embed_dim=EMBED_DIM, nhead=NHEAD, num_layers=NUM_LAYERS, dropout=DROPOUT\n    ).to(DEVICE)\n    \n    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n    scheduler = optim.lr_scheduler.OneCycleLR(\n        optimizer, max_lr=LR, epochs=EPOCHS, steps_per_epoch=len(fold_train_loader)\n    )\n    criterion = nn.GaussianNLLLoss()\n    \n    best_val_loss = float('inf')\n    patience_counter = 0\n    train_losses = []\n    val_losses = []\n    model_path = f'../outputs/best_transformer_fold{fold+1}.pt'\n    \n    for epoch in range(EPOCHS):\n        train_loss = train_epoch(model, fold_train_loader, optimizer, scheduler, criterion, DEVICE)\n        val_loss, _, _, _ = eval_epoch(model, fold_val_loader, criterion, DEVICE)\n        \n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        \n        if epoch % 5 == 0 or epoch == EPOCHS - 1:\n            print(f'  Epoch {epoch+1:02d}/{EPOCHS} | Train: {train_loss:.5f} | Val: {val_loss:.5f}')\n        \n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            patience_counter = 0\n            torch.save(model.state_dict(), model_path)\n        else:\n            patience_counter += 1\n            if patience_counter >= PATIENCE:\n                print(f'  Early stopping a epoch {epoch+1}')\n                break\n    \n    all_train_losses[fold] = train_losses\n    all_val_losses[fold] = val_losses\n    \n    # Évaluer sur la validation de ce fold\n    model.load_state_dict(torch.load(model_path, map_location=DEVICE, weights_only=True))\n    val_loss, val_mu, val_sigma, val_targets = eval_epoch(model, fold_val_loader, criterion, DEVICE)\n    val_eucl = np.sqrt((val_targets[:, 0] - val_mu[:, 0])**2 + (val_targets[:, 1] - val_mu[:, 1])**2)\n    \n    fold_results.append({\n        'fold': fold + 1,\n        'best_val_loss': best_val_loss,\n        'val_eucl_mean': val_eucl.mean(),\n        'val_r2_x': r2_score(val_targets[:, 0], val_mu[:, 0]),\n        'val_r2_y': r2_score(val_targets[:, 1], val_mu[:, 1]),\n        'epochs': len(train_losses),\n    })\n    print(f'  Best val loss: {best_val_loss:.5f} | Eucl: {val_eucl.mean():.4f} | R2: X={fold_results[-1][\"val_r2_x\"]:.4f}, Y={fold_results[-1][\"val_r2_y\"]:.4f}')\n\n# Résumé des folds\nprint(f'\\n{\"=\"*60}')\nprint(f'RESUME CROSS-VALIDATION ({N_FOLDS} folds)')\nprint(f'{\"=\"*60}')\nfor r in fold_results:\n    print(f'  Fold {r[\"fold\"]}: NLL={r[\"best_val_loss\"]:.5f} | Eucl={r[\"val_eucl_mean\"]:.4f} | R2_X={r[\"val_r2_x\"]:.4f} | R2_Y={r[\"val_r2_y\"]:.4f} | Epochs={r[\"epochs\"]}')\n\nmean_eucl = np.mean([r['val_eucl_mean'] for r in fold_results])\nstd_eucl = np.std([r['val_eucl_mean'] for r in fold_results])\nmean_r2_x = np.mean([r['val_r2_x'] for r in fold_results])\nmean_r2_y = np.mean([r['val_r2_y'] for r in fold_results])\nprint(f'\\n  Moyenne : Eucl={mean_eucl:.4f} (+/- {std_eucl:.4f}) | R2_X={mean_r2_x:.4f} | R2_Y={mean_r2_y:.4f}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Courbes d'entraînement par fold\nfig, axes = plt.subplots(1, 2, figsize=(16, 5))\n\ncolors = plt.cm.tab10(np.linspace(0, 1, N_FOLDS))\n\nfor fold in range(N_FOLDS):\n    axes[0].plot(all_train_losses[fold], color=colors[fold], linewidth=1.5, label=f'Fold {fold+1}')\n    axes[1].plot(all_val_losses[fold], color=colors[fold], linewidth=1.5, label=f'Fold {fold+1}')\n\naxes[0].set_xlabel('Epoch'); axes[0].set_ylabel('Gaussian NLL Loss')\naxes[0].set_title('Train Loss par fold'); axes[0].legend(); axes[0].grid(True, alpha=0.3)\n\naxes[1].set_xlabel('Epoch'); axes[1].set_ylabel('Gaussian NLL Loss')\naxes[1].set_title('Validation Loss par fold'); axes[1].legend(); axes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Évaluation finale sur le test set\n\nOn charge les 5 modèles entraînés et on fait la moyenne de leurs prédictions (ensemble). Pour les sigmas, on combine les variances : `sigma_ensemble² = mean(sigma_i²) + mean((mu_i - mu_mean)²)` (loi de la variance totale)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Évaluation ensemble des 5 folds sur le test set\ncriterion = nn.GaussianNLLLoss()\nall_fold_mu = []\nall_fold_sigma = []\n\nfor fold in range(N_FOLDS):\n    model_path = f'../outputs/best_transformer_fold{fold+1}.pt'\n    model = SpikeTransformer(\n        nGroups, nChannelsPerGroup,\n        embed_dim=EMBED_DIM, nhead=NHEAD, num_layers=NUM_LAYERS, dropout=DROPOUT\n    ).to(DEVICE)\n    model.load_state_dict(torch.load(model_path, map_location=DEVICE, weights_only=True))\n    \n    _, fold_mu, fold_sigma, y_test = eval_epoch(model, test_loader, criterion, DEVICE)\n    all_fold_mu.append(fold_mu)\n    all_fold_sigma.append(fold_sigma)\n    \n    fold_eucl = np.sqrt((y_test[:, 0] - fold_mu[:, 0])**2 + (y_test[:, 1] - fold_mu[:, 1])**2)\n    print(f'Fold {fold+1} sur test: Eucl={fold_eucl.mean():.4f}')\n\n# Ensemble : moyenne des mu\nall_fold_mu = np.stack(all_fold_mu)      # (N_FOLDS, n_test, 2)\nall_fold_sigma = np.stack(all_fold_sigma)  # (N_FOLDS, n_test, 2)\n\ny_pred = all_fold_mu.mean(axis=0)  # (n_test, 2)\n\n# Sigma ensemble (loi de la variance totale)\n# Var_totale = E[sigma²] + Var[mu]\nmean_var = (all_fold_sigma ** 2).mean(axis=0)  # aleatoric\nvar_mu = all_fold_mu.var(axis=0)                # epistemic\ny_sigma = np.sqrt(mean_var + var_mu)\n\n# Métriques\nmse_x = mean_squared_error(y_test[:, 0], y_pred[:, 0])\nmse_y = mean_squared_error(y_test[:, 1], y_pred[:, 1])\nmae_x = mean_absolute_error(y_test[:, 0], y_pred[:, 0])\nmae_y = mean_absolute_error(y_test[:, 1], y_pred[:, 1])\nr2_x = r2_score(y_test[:, 0], y_pred[:, 0])\nr2_y = r2_score(y_test[:, 1], y_pred[:, 1])\neucl_errors = np.sqrt((y_test[:, 0] - y_pred[:, 0])**2 + (y_test[:, 1] - y_pred[:, 1])**2)\n\nprint(f'\\n=== Transformer Ensemble ({N_FOLDS} folds, Gaussian NLL) ===')\nprint(f'  MSE  : X={mse_x:.5f}, Y={mse_y:.5f}')\nprint(f'  MAE  : X={mae_x:.4f}, Y={mae_y:.4f}')\nprint(f'  R²   : X={r2_x:.4f}, Y={r2_y:.4f}')\nprint(f'  Eucl : mean={eucl_errors.mean():.4f}, median={np.median(eucl_errors):.4f}, p90={np.percentile(eucl_errors, 90):.4f}')\nprint(f'\\n  Sigma moyen : X={y_sigma[:, 0].mean():.4f}, Y={y_sigma[:, 1].mean():.4f}')\nprint(f'  Sigma median: X={np.median(y_sigma[:, 0]):.4f}, Y={np.median(y_sigma[:, 1]):.4f}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Scatter pred vs true ---\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\naxes[0].scatter(y_test[:, 0], y_pred[:, 0], s=1, alpha=0.3)\naxes[0].plot([0, 1], [0, 1], 'r--', linewidth=2)\naxes[0].set_xlabel('True X'); axes[0].set_ylabel('Predicted X')\naxes[0].set_title(f'Transformer - Position X (R²={r2_x:.3f})')\naxes[0].set_aspect('equal')\n\naxes[1].scatter(y_test[:, 1], y_pred[:, 1], s=1, alpha=0.3)\naxes[1].plot([0, 1], [0, 1], 'r--', linewidth=2)\naxes[1].set_xlabel('True Y'); axes[1].set_ylabel('Predicted Y')\naxes[1].set_title(f'Transformer - Position Y (R²={r2_y:.3f})')\naxes[1].set_aspect('equal')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Points prédits vs vrais avec incertitude ---\nsegment = slice(0, 500)\nseg_idx = np.arange(500)\n\nfig, axes = plt.subplots(2, 2, figsize=(16, 12))\n\n# 1. Points 2D colores par ordre chronologique\ncolors = np.arange(500)\naxes[0, 0].scatter(y_test[segment, 0], y_test[segment, 1], c=colors, cmap='winter', s=8, alpha=0.6, label='Vraie position')\nsc = axes[0, 0].scatter(y_pred[segment, 0], y_pred[segment, 1], c=colors, cmap='autumn', s=8, alpha=0.6, marker='x', label='Prediction (mu)')\naxes[0, 0].set_xlabel('X'); axes[0, 0].set_ylabel('Y')\naxes[0, 0].set_title('Positions (500 premiers points test, couleur = ordre chronologique)')\naxes[0, 0].legend()\naxes[0, 0].set_aspect('equal')\ncbar = plt.colorbar(sc, ax=axes[0, 0])\ncbar.set_label('Index temporel')\n\n# 2. Position X avec bande d'incertitude (mu +/- 2*sigma)\naxes[0, 1].plot(seg_idx, y_test[segment, 0], 'b-', label='Vrai X', linewidth=1.5)\naxes[0, 1].plot(seg_idx, y_pred[segment, 0], 'r-', alpha=0.7, label='Prediction (mu)', linewidth=1)\naxes[0, 1].fill_between(seg_idx, \n                         y_pred[segment, 0] - 2 * y_sigma[segment, 0],\n                         y_pred[segment, 0] + 2 * y_sigma[segment, 0],\n                         alpha=0.2, color='red', label='Incertitude (2 sigma)')\naxes[0, 1].set_xlabel('Index'); axes[0, 1].set_ylabel('Position X')\naxes[0, 1].set_title('Position X avec incertitude')\naxes[0, 1].legend()\n\n# 3. Position Y avec bande d'incertitude\naxes[1, 0].plot(seg_idx, y_test[segment, 1], 'b-', label='Vrai Y', linewidth=1.5)\naxes[1, 0].plot(seg_idx, y_pred[segment, 1], 'r-', alpha=0.7, label='Prediction (mu)', linewidth=1)\naxes[1, 0].fill_between(seg_idx,\n                         y_pred[segment, 1] - 2 * y_sigma[segment, 1],\n                         y_pred[segment, 1] + 2 * y_sigma[segment, 1],\n                         alpha=0.2, color='red', label='Incertitude (2 sigma)')\naxes[1, 0].set_xlabel('Index'); axes[1, 0].set_ylabel('Position Y')\naxes[1, 0].set_title('Position Y avec incertitude')\naxes[1, 0].legend()\n\n# 4. Erreur euclidienne vs sigma moyen\nsigma_mean = (y_sigma[:, 0] + y_sigma[:, 1]) / 2\naxes[1, 1].scatter(sigma_mean, eucl_errors, s=1, alpha=0.3)\naxes[1, 1].set_xlabel('Sigma moyen predit'); axes[1, 1].set_ylabel('Erreur euclidienne reelle')\naxes[1, 1].set_title('Calibration : incertitude vs erreur')\n# Ligne de reference y=2*x (si bien calibre, ~95% des points sous cette ligne)\nsigma_range = np.linspace(0, sigma_mean.max(), 100)\naxes[1, 1].plot(sigma_range, 2 * sigma_range, 'r--', label='y = 2*sigma', linewidth=1.5)\naxes[1, 1].legend()\n\nplt.tight_layout()\nplt.show()\n\n# Calibration : quel % des vraies positions tombe dans l'intervalle predit ?\nin_1sigma = np.mean(eucl_errors < sigma_mean)\nin_2sigma = np.mean(eucl_errors < 2 * sigma_mean)\nin_3sigma = np.mean(eucl_errors < 3 * sigma_mean)\nprint(f'Calibration de l\\'incertitude :')\nprint(f'  Erreur < 1*sigma : {in_1sigma:.1%} (attendu ~39% pour gaussienne 2D)')\nprint(f'  Erreur < 2*sigma : {in_2sigma:.1%} (attendu ~86%)')\nprint(f'  Erreur < 3*sigma : {in_3sigma:.1%} (attendu ~99%)')"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Heatmap des erreurs spatiales ---\n",
    "fig, ax = plt.subplots(figsize=(8, 7))\n",
    "\n",
    "nbins = 20\n",
    "error_map = np.full((nbins, nbins), np.nan)\n",
    "count_map = np.zeros((nbins, nbins))\n",
    "x_edges = np.linspace(0, 1, nbins + 1)\n",
    "y_edges = np.linspace(0, 1, nbins + 1)\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    xi = np.clip(np.searchsorted(x_edges, y_test[i, 0]) - 1, 0, nbins - 1)\n",
    "    yi = np.clip(np.searchsorted(y_edges, y_test[i, 1]) - 1, 0, nbins - 1)\n",
    "    if np.isnan(error_map[yi, xi]):\n",
    "        error_map[yi, xi] = 0\n",
    "    error_map[yi, xi] += eucl_errors[i]\n",
    "    count_map[yi, xi] += 1\n",
    "\n",
    "mean_error_map = np.where(count_map > 0, error_map / count_map, np.nan)\n",
    "\n",
    "im = ax.imshow(mean_error_map, origin='lower', aspect='equal', cmap='RdYlGn_r', extent=[0, 1, 0, 1])\n",
    "ax.set_xlabel('X'); ax.set_ylabel('Y')\n",
    "ax.set_title('Transformer - Erreur moyenne par position')\n",
    "plt.colorbar(im, ax=ax, label='Erreur euclidienne moyenne')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Sauvegarde des prédictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "np.save('../outputs/preds_transformer.npy', y_pred)\nnp.save('../outputs/sigma_transformer.npy', y_sigma)\nnp.save('../outputs/y_test_transformer.npy', y_test)\nprint(f'Predictions ensemble ({N_FOLDS} folds) sauvegardees.')\nprint(f'  preds_transformer.npy : mu ensemble ({y_pred.shape})')\nprint(f'  sigma_transformer.npy : sigma ensemble ({y_sigma.shape})')\nprint(f'  y_test_transformer.npy : targets ({y_test.shape})')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 11. Analyse de l'incertitude\n\nAvec la sortie probabiliste (Gaussian NLL), le modèle prédit non seulement une position (mu) mais aussi son incertitude (sigma). Cela permet de :\n- **Identifier les zones difficiles** : là où sigma est élevé, le modèle \"sait qu'il ne sait pas\"\n- **Pondérer les prédictions** : en aval, on peut donner moins de poids aux prédictions incertaines\n- **Détecter les anomalies** : des sigma anormalement élevés peuvent signaler des données hors distribution\n\nNote : le filtre de Kalman et le GRU multi-fenêtres ne sont pas applicables ici car l'objectif est de prédire la position à partir d'une **seule fenêtre isolée**, sans contexte temporel."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Heatmap de l'incertitude moyenne par position ---\nfig, axes = plt.subplots(1, 2, figsize=(16, 7))\n\nnbins = 20\nx_edges = np.linspace(0, 1, nbins + 1)\ny_edges = np.linspace(0, 1, nbins + 1)\n\nfor ax_idx, (title, values) in enumerate([\n    ('Erreur euclidienne moyenne', eucl_errors),\n    ('Sigma moyen predit', (y_sigma[:, 0] + y_sigma[:, 1]) / 2)\n]):\n    val_map = np.full((nbins, nbins), np.nan)\n    count_map = np.zeros((nbins, nbins))\n    \n    for i in range(len(y_test)):\n        xi = np.clip(np.searchsorted(x_edges, y_test[i, 0]) - 1, 0, nbins - 1)\n        yi = np.clip(np.searchsorted(y_edges, y_test[i, 1]) - 1, 0, nbins - 1)\n        if np.isnan(val_map[yi, xi]):\n            val_map[yi, xi] = 0\n        val_map[yi, xi] += values[i]\n        count_map[yi, xi] += 1\n    \n    mean_map = np.where(count_map > 0, val_map / count_map, np.nan)\n    \n    im = axes[ax_idx].imshow(mean_map, origin='lower', aspect='equal', \n                              cmap='RdYlGn_r', extent=[0, 1, 0, 1])\n    axes[ax_idx].set_xlabel('X'); axes[ax_idx].set_ylabel('Y')\n    axes[ax_idx].set_title(title)\n    plt.colorbar(im, ax=axes[ax_idx])\n\nplt.tight_layout()\nplt.show()\n\n# Correlation entre sigma predit et erreur reelle\nfrom scipy.stats import spearmanr\ncorr, pval = spearmanr(sigma_mean, eucl_errors)\nprint(f'Correlation de Spearman (sigma vs erreur) : {corr:.3f} (p={pval:.2e})')\nprint(f'  -> {\"Bonne\" if corr > 0.3 else \"Faible\"} calibration : le modele {\"sait\" if corr > 0.3 else \"ne sait pas bien\"} quand il se trompe')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Distribution des sigma predits ---\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\naxes[0].hist(y_sigma[:, 0], bins=50, alpha=0.7, color='steelblue', edgecolor='white')\naxes[0].axvline(y_sigma[:, 0].mean(), color='red', linestyle='--', label=f'Moyenne={y_sigma[:, 0].mean():.4f}')\naxes[0].set_xlabel('Sigma X'); axes[0].set_ylabel('Count')\naxes[0].set_title('Distribution de l\\'incertitude sur X')\naxes[0].legend()\n\naxes[1].hist(y_sigma[:, 1], bins=50, alpha=0.7, color='coral', edgecolor='white')\naxes[1].axvline(y_sigma[:, 1].mean(), color='red', linestyle='--', label=f'Moyenne={y_sigma[:, 1].mean():.4f}')\naxes[1].set_xlabel('Sigma Y'); axes[1].set_ylabel('Count')\naxes[1].set_title('Distribution de l\\'incertitude sur Y')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 12. Interpretation\n\n### Sortie probabiliste (Gaussian NLL)\n\nLe modèle prédit une **distribution gaussienne** par coordonnée : `P(position | spikes) = N(mu, sigma²)`. Cela apporte deux avantages par rapport à la MSE :\n- **Incertitude calibrée** : quand le modèle n'est pas sûr (peu de spikes, zone peu visitée), il élargit sigma au lieu de prédire une position arbitraire\n- **Loss adaptative** : les exemples faciles (petit sigma) contribuent peu à la loss, tandis que les exemples difficiles (grand sigma) sont naturellement down-pondérés. Cela évite que les outliers dominent l'entraînement.\n\n### Calibration\n\nPour vérifier si les sigmas prédits sont fiables, on mesure la proportion de vraies positions qui tombent dans les intervalles de confiance :\n- **1 sigma** : ~39% attendu pour une gaussienne 2D isotrope\n- **2 sigma** : ~86% attendu\n- **3 sigma** : ~99% attendu\n\nSi le modèle est **sous-confiant** (sigma trop large), ces proportions seront plus élevées que les valeurs théoriques. S'il est **sur-confiant** (sigma trop petit), elles seront plus faibles.\n\n### Ce que le Transformer apporte\n\nContrairement à l'approche feature engineering, le Transformer travaille directement sur les **waveforms bruts** et peut capturer :\n- Les **formes de spike** spécifiques à différents neurones (via le CNN encoder)\n- Les **co-activations** entre spikes de différents shanks (via le self-attention)\n- L'**ordre temporel** des spikes dans la séquence (via le positional encoding)\n\n### Limites\n- 22k exemples c'est peu pour un Transformer — risque d'overfitting malgré le dropout et le weight decay\n- La troncature à 128 spikes perd de l'information pour les fenêtres très actives\n- Le padding à 6 canaux pour les shanks à 4 canaux introduit des zéros non informatifs\n- L'hypothèse de gaussienne isotrope (sigma_x et sigma_y indépendants) ignore les corrélations spatiales entre X et Y\n\n### Améliorations possibles\n- Augmenter le nombre d'epochs et la capacité du modèle (embed_dim=128, num_layers=4)\n- Data augmentation (bruit sur les waveforms, masquage aléatoire de spikes)\n- Gaussienne 2D complète (avec matrice de covariance) au lieu de sigma_x, sigma_y indépendants\n- Multi-task learning : prédire (x, y, heading, speed) simultanément"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}