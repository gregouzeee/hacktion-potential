{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 02h : Transformer Hiérarchique (Classification bras + Régression conditionnelle)\n",
    "\n",
    "## Modifications par rapport au notebook 02g\n",
    "\n",
    "1. **Classification explicite gauche/droite** : un head de classification binaire prédit P(bras droit) à partir de la distance curviligne `d` (seuil à d=0.5). Le couloir haut est attribué au bras le plus proche.\n",
    "\n",
    "2. **Régression conditionnelle** : deux heads de régression spécialisés (un par bras) prédisent (x, y) + incertitude. La prédiction finale est un **mélange pondéré** par la probabilité de classification.\n",
    "\n",
    "3. **Loss hiérarchique** : chaque head de régression n'est entraîné que sur les exemples de son bras (routage par les labels ground truth).\n",
    "\n",
    "4. **Spike dropout (15%) + Gaussian noise (std=0.5)** : data augmentation identique au notebook 02d.\n",
    "\n",
    "**Géométrie du U** (coordonnées normalisées [0, 1]) :\n",
    "- Bras gauche : x ∈ [0, 0.3], y ∈ [0, 1]\n",
    "- Bras droit : x ∈ [0.7, 1.0], y ∈ [0, 1]\n",
    "- Couloir haut : x ∈ [0, 1], y ∈ [0.7, 1.0]\n",
    "- Largeur du couloir : 0.3\n",
    "\n",
    "**Squelette central** : 3 segments formant un U :\n",
    "1. (0.15, 0) → (0.15, 0.85) — bras gauche\n",
    "2. (0.15, 0.85) → (0.85, 0.85) — couloir haut\n",
    "3. (0.85, 0.85) → (0.85, 0) — bras droit\n",
    "\n",
    "**Classification bras** : d < 0.5 → bras gauche, d ≥ 0.5 → bras droit\n",
    "\n",
    "**Loss combinée** : `L = L_cls (BCE) + L_pos_left (Gaussian NLL, masqué) + L_pos_right (Gaussian NLL, masqué) + λ_d * L_curvilinear (MSE) + λ_feas * L_feasibility`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports et configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Reproductibilité\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Device\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device('mps')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "print(f'Device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Chargement des données ---\n",
    "LOCAL_DIR = os.path.join(os.path.abspath('..'), 'data')\n",
    "\n",
    "PARQUET_NAME = \"M1199_PAG_stride4_win108_test.parquet\"\n",
    "JSON_NAME = \"M1199_PAG.json\"\n",
    "\n",
    "PARQUET_FILE = os.path.join(LOCAL_DIR, PARQUET_NAME)\n",
    "JSON_FILE = os.path.join(LOCAL_DIR, JSON_NAME)\n",
    "\n",
    "if not os.path.exists(PARQUET_FILE):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Données introuvables dans {LOCAL_DIR}/\\n\"\n",
    "        f\"Lancez d'abord: python download_data.py\"\n",
    "    )\n",
    "\n",
    "print(f\"Chargement depuis {LOCAL_DIR}/\")\n",
    "df = pd.read_parquet(PARQUET_FILE)\n",
    "with open(JSON_FILE, \"r\") as f:\n",
    "    params = json.load(f)\n",
    "\n",
    "print(f\"Shape: {df.shape}\")\n",
    "\n",
    "nGroups = params['nGroups']\n",
    "nChannelsPerGroup = [params[f'group{g}']['nChannels'] for g in range(nGroups)]\n",
    "print(f\"nGroups={nGroups}, nChannelsPerGroup={nChannelsPerGroup}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement et filtrage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrage speedMask (on ne garde que les exemples en mouvement)\n",
    "speed_masks = np.array([x[0] for x in df['speedMask']])\n",
    "df_moving = df[speed_masks].reset_index(drop=True)\n",
    "print(f'Exemples en mouvement : {len(df_moving)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Géométrie du U-maze, distance curviligne et labels de bras\n",
    "\n",
    "Le squelette du U est défini par 3 segments :\n",
    "1. Bras gauche : (0.15, 0) → (0.15, 0.85)\n",
    "2. Couloir haut : (0.15, 0.85) → (0.85, 0.85)\n",
    "3. Bras droit : (0.85, 0.85) → (0.85, 0)\n",
    "\n",
    "Pour chaque position (x, y), on projette sur le segment le plus proche et on calcule la distance curviligne cumulative `d` ∈ [0, 1].\n",
    "\n",
    "**Classification bras** : d < 0.5 → gauche (label 0), d ≥ 0.5 → droite (label 1).\n",
    "Le seuil d=0.5 correspond au milieu du couloir haut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Définition du squelette du U ---\n",
    "SKELETON_SEGMENTS = np.array([\n",
    "    [0.15, 0.0, 0.15, 0.85],   # Segment 1 : bras gauche (bas → haut)\n",
    "    [0.15, 0.85, 0.85, 0.85],  # Segment 2 : couloir haut (gauche → droite)\n",
    "    [0.85, 0.85, 0.85, 0.0],   # Segment 3 : bras droit (haut → bas)\n",
    "])\n",
    "\n",
    "CORRIDOR_HALF_WIDTH = 0.15\n",
    "\n",
    "# Longueurs cumulatives des segments\n",
    "SEGMENT_LENGTHS = np.array([\n",
    "    np.sqrt((s[2]-s[0])**2 + (s[3]-s[1])**2) for s in SKELETON_SEGMENTS\n",
    "])\n",
    "TOTAL_LENGTH = SEGMENT_LENGTHS.sum()  # 0.85 + 0.70 + 0.85 = 2.40\n",
    "CUMULATIVE_LENGTHS = np.concatenate([[0], np.cumsum(SEGMENT_LENGTHS)])\n",
    "\n",
    "print(f'Longueurs des segments : {SEGMENT_LENGTHS}')\n",
    "print(f'Longueur totale du U : {TOTAL_LENGTH:.2f}')\n",
    "print(f'Longueurs cumulatives : {CUMULATIVE_LENGTHS}')\n",
    "\n",
    "\n",
    "def project_point_on_segment(px, py, x1, y1, x2, y2):\n",
    "    \"\"\"Projette un point (px, py) sur le segment [(x1,y1), (x2,y2)].\n",
    "    \n",
    "    Retourne:\n",
    "        t: paramètre de projection clampé à [0, 1]\n",
    "        dist: distance du point à la projection\n",
    "        proj_x, proj_y: coordonnées de la projection\n",
    "    \"\"\"\n",
    "    dx, dy = x2 - x1, y2 - y1\n",
    "    seg_len_sq = dx**2 + dy**2\n",
    "    if seg_len_sq < 1e-12:\n",
    "        return 0.0, np.sqrt((px - x1)**2 + (py - y1)**2), x1, y1\n",
    "    t = ((px - x1) * dx + (py - y1) * dy) / seg_len_sq\n",
    "    t = np.clip(t, 0.0, 1.0)\n",
    "    proj_x = x1 + t * dx\n",
    "    proj_y = y1 + t * dy\n",
    "    dist = np.sqrt((px - proj_x)**2 + (py - proj_y)**2)\n",
    "    return t, dist, proj_x, proj_y\n",
    "\n",
    "\n",
    "def compute_curvilinear_distance(x, y):\n",
    "    \"\"\"Calcule la distance curviligne normalisée d ∈ [0, 1] le long du U.\"\"\"\n",
    "    best_dist = np.inf\n",
    "    best_d = 0.0\n",
    "    \n",
    "    for i, (x1, y1, x2, y2) in enumerate(SKELETON_SEGMENTS):\n",
    "        t, dist, _, _ = project_point_on_segment(x, y, x1, y1, x2, y2)\n",
    "        if dist < best_dist:\n",
    "            best_dist = dist\n",
    "            best_d = (CUMULATIVE_LENGTHS[i] + t * SEGMENT_LENGTHS[i]) / TOTAL_LENGTH\n",
    "    \n",
    "    return best_d\n",
    "\n",
    "\n",
    "def compute_distance_to_skeleton(x, y):\n",
    "    \"\"\"Distance minimale du point (x, y) au squelette du U.\"\"\"\n",
    "    best_dist = np.inf\n",
    "    for x1, y1, x2, y2 in SKELETON_SEGMENTS:\n",
    "        _, dist, _, _ = project_point_on_segment(x, y, x1, y1, x2, y2)\n",
    "        best_dist = min(best_dist, dist)\n",
    "    return best_dist\n",
    "\n",
    "\n",
    "# --- Calcul de d pour tous les exemples ---\n",
    "positions = np.array([[x[0], x[1]] for x in df_moving['pos']], dtype=np.float32)\n",
    "curvilinear_d = np.array([\n",
    "    compute_curvilinear_distance(x, y) for x, y in positions\n",
    "], dtype=np.float32)\n",
    "\n",
    "# --- Labels de bras : 0 = gauche (d < 0.5), 1 = droite (d >= 0.5) ---\n",
    "ARM_THRESHOLD = 0.5\n",
    "arm_labels = (curvilinear_d >= ARM_THRESHOLD).astype(np.float32)\n",
    "\n",
    "print(f'd curviligne : min={curvilinear_d.min():.4f}, max={curvilinear_d.max():.4f}, mean={curvilinear_d.mean():.4f}')\n",
    "print(f'\\nClassification bras (seuil d={ARM_THRESHOLD}) :')\n",
    "print(f'  Bras gauche (d < {ARM_THRESHOLD}) : {(arm_labels == 0).sum()} ({(arm_labels == 0).mean():.1%})')\n",
    "print(f'  Bras droit  (d >= {ARM_THRESHOLD}) : {(arm_labels == 1).sum()} ({(arm_labels == 1).mean():.1%})')\n",
    "\n",
    "# Vérification : distance au squelette\n",
    "dist_to_skel = np.array([compute_distance_to_skeleton(x, y) for x, y in positions])\n",
    "print(f'\\nDistance au squelette : mean={dist_to_skel.mean():.4f}, max={dist_to_skel.max():.4f}')\n",
    "print(f'  % dans le couloir (dist < {CORRIDOR_HALF_WIDTH}) : {(dist_to_skel < CORRIDOR_HALF_WIDTH).mean():.1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualisation : d + classification bras ---\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 6))\n",
    "\n",
    "# 1. Squelette du U sur les positions\n",
    "axes[0].scatter(positions[:, 0], positions[:, 1], c='lightgray', s=1, alpha=0.3)\n",
    "for x1, y1, x2, y2 in SKELETON_SEGMENTS:\n",
    "    axes[0].plot([x1, x2], [y1, y2], 'r-', linewidth=3)\n",
    "axes[0].set_xlabel('X'); axes[0].set_ylabel('Y')\n",
    "axes[0].set_title('Squelette du U sur les positions')\n",
    "axes[0].set_aspect('equal')\n",
    "\n",
    "# 2. Positions colorées par d\n",
    "sc = axes[1].scatter(positions[:, 0], positions[:, 1], c=curvilinear_d, s=1, alpha=0.5, cmap='viridis')\n",
    "plt.colorbar(sc, ax=axes[1], label='d (distance curviligne normalisée)')\n",
    "axes[1].set_xlabel('X'); axes[1].set_ylabel('Y')\n",
    "axes[1].set_title('Distance curviligne d le long du U')\n",
    "axes[1].set_aspect('equal')\n",
    "\n",
    "# 3. Classification bras gauche (bleu) vs droit (rouge)\n",
    "left_mask = arm_labels == 0\n",
    "right_mask = arm_labels == 1\n",
    "axes[2].scatter(positions[left_mask, 0], positions[left_mask, 1], c='blue', s=1, alpha=0.3, label='Gauche (d<0.5)')\n",
    "axes[2].scatter(positions[right_mask, 0], positions[right_mask, 1], c='red', s=1, alpha=0.3, label='Droite (d≥0.5)')\n",
    "axes[2].axvline(x=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[2].set_xlabel('X'); axes[2].set_ylabel('Y')\n",
    "axes[2].set_title(f'Classification bras (seuil d={ARM_THRESHOLD})')\n",
    "axes[2].legend(markerscale=10)\n",
    "axes[2].set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Histogramme de d\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.hist(curvilinear_d, bins=100, alpha=0.7, edgecolor='black', linewidth=0.3)\n",
    "ax.axvline(x=ARM_THRESHOLD, color='red', linestyle='--', linewidth=2, label=f'Seuil d={ARM_THRESHOLD}')\n",
    "ax.set_xlabel('d (distance curviligne)'); ax.set_ylabel('Nombre d\\'exemples')\n",
    "ax.set_title('Distribution de la distance curviligne')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocessing : reconstruction de la séquence chronologique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_sequence(row, nGroups, nChannelsPerGroup, max_seq_len=128):\n",
    "    \"\"\"\n",
    "    Reconstruit la séquence chronologique de spikes.\n",
    "    \"\"\"\n",
    "    groups = row['groups']\n",
    "    length = min(len(groups), max_seq_len)\n",
    "    \n",
    "    waveforms = {}\n",
    "    for g in range(nGroups):\n",
    "        nCh = nChannelsPerGroup[g]\n",
    "        raw = row[f'group{g}']\n",
    "        waveforms[g] = raw.reshape(-1, nCh, 32)\n",
    "    \n",
    "    seq_waveforms = []\n",
    "    seq_shank_ids = []\n",
    "    \n",
    "    for t in range(length):\n",
    "        g = int(groups[t])\n",
    "        idx = int(row[f'indices{g}'][t])\n",
    "        if idx > 0 and idx <= waveforms[g].shape[0]:\n",
    "            seq_waveforms.append((waveforms[g][idx - 1], g))\n",
    "            seq_shank_ids.append(g)\n",
    "    \n",
    "    return seq_waveforms, seq_shank_ids\n",
    "\n",
    "# Test rapide\n",
    "wf, sids = reconstruct_sequence(df_moving.iloc[0], nGroups, nChannelsPerGroup)\n",
    "print(f'Premier exemple : {len(wf)} spikes réels dans la séquence')\n",
    "print(f'Shanks utilisés : {set(sids)}')\n",
    "print(f'Premier spike : shank={wf[0][1]}, shape={wf[0][0].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset PyTorch (avec arm_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 128\n",
    "MAX_CHANNELS = max(nChannelsPerGroup)  # 6\n",
    "\n",
    "class SpikeSequenceDataset(Dataset):\n",
    "    def __init__(self, dataframe, nGroups, nChannelsPerGroup, curvilinear_d, arm_labels, max_seq_len=MAX_SEQ_LEN):\n",
    "        self.df = dataframe\n",
    "        self.nGroups = nGroups\n",
    "        self.nChannelsPerGroup = nChannelsPerGroup\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # Pré-extraire les targets\n",
    "        self.targets = np.array([[x[0], x[1]] for x in dataframe['pos']], dtype=np.float32)\n",
    "        self.curvilinear_d = curvilinear_d.astype(np.float32)\n",
    "        self.arm_labels = arm_labels.astype(np.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        seq, shank_ids = reconstruct_sequence(row, self.nGroups, self.nChannelsPerGroup, self.max_seq_len)\n",
    "        \n",
    "        seq_len = len(seq)\n",
    "        if seq_len == 0:\n",
    "            seq_len = 1\n",
    "            waveforms = np.zeros((1, MAX_CHANNELS, 32), dtype=np.float32)\n",
    "            shank_ids_arr = np.array([0], dtype=np.int64)\n",
    "        else:\n",
    "            waveforms = np.zeros((seq_len, MAX_CHANNELS, 32), dtype=np.float32)\n",
    "            shank_ids_arr = np.array(shank_ids, dtype=np.int64)\n",
    "            for t, (wf, g) in enumerate(seq):\n",
    "                nCh = wf.shape[0]\n",
    "                waveforms[t, :nCh, :] = wf\n",
    "        \n",
    "        target = self.targets[idx]\n",
    "        d = self.curvilinear_d[idx]\n",
    "        arm = self.arm_labels[idx]\n",
    "        return {\n",
    "            'waveforms': torch.from_numpy(waveforms),\n",
    "            'shank_ids': torch.from_numpy(shank_ids_arr),\n",
    "            'seq_len': seq_len,\n",
    "            'target': torch.from_numpy(target),\n",
    "            'd': torch.tensor(d, dtype=torch.float32),\n",
    "            'arm': torch.tensor(arm, dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Collate avec padding dynamique.\"\"\"\n",
    "    max_len = max(item['seq_len'] for item in batch)\n",
    "    batch_size = len(batch)\n",
    "    \n",
    "    waveforms = torch.zeros(batch_size, max_len, MAX_CHANNELS, 32)\n",
    "    shank_ids = torch.zeros(batch_size, max_len, dtype=torch.long)\n",
    "    mask = torch.ones(batch_size, max_len, dtype=torch.bool)\n",
    "    targets = torch.stack([item['target'] for item in batch])\n",
    "    d_targets = torch.stack([item['d'] for item in batch])\n",
    "    arm_targets = torch.stack([item['arm'] for item in batch])\n",
    "    \n",
    "    for i, item in enumerate(batch):\n",
    "        sl = item['seq_len']\n",
    "        waveforms[i, :sl] = item['waveforms']\n",
    "        shank_ids[i, :sl] = item['shank_ids']\n",
    "        mask[i, :sl] = False\n",
    "    \n",
    "    return {\n",
    "        'waveforms': waveforms,\n",
    "        'shank_ids': shank_ids,\n",
    "        'mask': mask,\n",
    "        'targets': targets,\n",
    "        'd_targets': d_targets,\n",
    "        'arm_targets': arm_targets\n",
    "    }\n",
    "\n",
    "print('Dataset et collate_fn définis (avec arm_label + d curviligne).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Loss de faisabilité (pénalisation hors labyrinthe)\n",
    "\n",
    "Identique au notebook 02g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_epoch(model, loader, optimizer, scheduler, criterion_bce, criterion_nll, criterion_d, feas_loss, device):\n    model.train()\n    total_loss = 0\n    total_cls_loss = 0\n    total_pos_loss = 0\n    total_d_loss = 0\n    total_feas_loss = 0\n    total_cls_correct = 0\n    total_samples = 0\n    n_batches = 0\n    \n    for batch in loader:\n        wf = batch['waveforms'].to(device)\n        sid = batch['shank_ids'].to(device)\n        mask = batch['mask'].to(device)\n        targets = batch['targets'].to(device)\n        d_targets = batch['d_targets'].to(device)\n        arm_targets = batch['arm_targets'].to(device)\n        \n        optimizer.zero_grad()\n        cls_logit, mu_left, sigma_left, mu_right, sigma_right, d_pred = model(wf, sid, mask)\n        \n        # --- Loss classification (BCE) ---\n        loss_cls = criterion_bce(cls_logit.squeeze(-1), arm_targets)\n        \n        # --- Loss position gauche et droite (Gaussian NLL, masquées) ---\n        left_mask = (arm_targets == 0)\n        right_mask = (arm_targets == 1)\n        \n        loss_pos = torch.tensor(0.0, device=device)\n        if left_mask.any():\n            loss_left = criterion_nll(\n                mu_left[left_mask], targets[left_mask], sigma_left[left_mask] ** 2\n            )\n            loss_pos = loss_pos + loss_left\n        \n        if right_mask.any():\n            loss_right = criterion_nll(\n                mu_right[right_mask], targets[right_mask], sigma_right[right_mask] ** 2\n            )\n            loss_pos = loss_pos + loss_right\n        \n        # --- Loss curviligne (MSE) ---\n        loss_d = criterion_d(d_pred.squeeze(-1), d_targets)\n        \n        # --- Loss faisabilité (sur la prédiction combinée) ---\n        p_right = torch.sigmoid(cls_logit)  # (batch, 1) avec gradient\n        p_left = 1.0 - p_right\n        mu_combined = p_right * mu_right + p_left * mu_left\n        loss_feas = feas_loss(mu_combined)\n        \n        # --- Loss totale ---\n        loss = loss_cls + loss_pos + LAMBDA_D * loss_d + LAMBDA_FEAS * loss_feas\n        \n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n        \n        total_loss += loss.item()\n        total_cls_loss += loss_cls.item()\n        total_pos_loss += loss_pos.item()\n        total_d_loss += loss_d.item()\n        total_feas_loss += loss_feas.item()\n        \n        # Accuracy classification\n        with torch.no_grad():\n            preds_cls = (torch.sigmoid(cls_logit.squeeze(-1)) >= 0.5).float()\n            total_cls_correct += (preds_cls == arm_targets).sum().item()\n            total_samples += len(arm_targets)\n        \n        n_batches += 1\n    \n    cls_acc = total_cls_correct / total_samples\n    return (total_loss / n_batches, total_cls_loss / n_batches, total_pos_loss / n_batches,\n            total_d_loss / n_batches, total_feas_loss / n_batches, cls_acc)\n\n\n@torch.no_grad()\ndef eval_epoch(model, loader, criterion_bce, criterion_nll, criterion_d, feas_loss, device):\n    model.eval()\n    total_loss = 0\n    total_cls_loss = 0\n    total_pos_loss = 0\n    total_d_loss = 0\n    total_feas_loss = 0\n    total_cls_correct = 0\n    total_samples = 0\n    n_batches = 0\n    all_mu = []\n    all_sigma = []\n    all_p_right = []\n    all_d_pred = []\n    all_targets = []\n    all_d_targets = []\n    all_arm_targets = []\n    \n    for batch in loader:\n        wf = batch['waveforms'].to(device)\n        sid = batch['shank_ids'].to(device)\n        mask = batch['mask'].to(device)\n        targets = batch['targets'].to(device)\n        d_targets = batch['d_targets'].to(device)\n        arm_targets = batch['arm_targets'].to(device)\n        \n        # Prédiction combinée\n        mu, sigma, p_right, d_pred = model.predict(wf, sid, mask)\n        \n        # Recalculer les sorties brutes pour la loss\n        cls_logit, mu_left, sigma_left, mu_right, sigma_right, _ = model(wf, sid, mask)\n        \n        # Loss classification\n        loss_cls = criterion_bce(cls_logit.squeeze(-1), arm_targets)\n        \n        # Loss position (masquée par bras)\n        left_mask = (arm_targets == 0)\n        right_mask = (arm_targets == 1)\n        \n        loss_pos = torch.tensor(0.0, device=device)\n        if left_mask.any():\n            loss_pos = loss_pos + criterion_nll(\n                mu_left[left_mask], targets[left_mask], sigma_left[left_mask] ** 2\n            )\n        if right_mask.any():\n            loss_pos = loss_pos + criterion_nll(\n                mu_right[right_mask], targets[right_mask], sigma_right[right_mask] ** 2\n            )\n        \n        loss_d = criterion_d(d_pred.squeeze(-1), d_targets)\n        loss_feas = feas_loss(mu)\n        loss = loss_cls + loss_pos + LAMBDA_D * loss_d + LAMBDA_FEAS * loss_feas\n        \n        total_loss += loss.item()\n        total_cls_loss += loss_cls.item()\n        total_pos_loss += loss_pos.item()\n        total_d_loss += loss_d.item()\n        total_feas_loss += loss_feas.item()\n        \n        preds_cls = (p_right.squeeze(-1) >= 0.5).float()\n        total_cls_correct += (preds_cls == arm_targets).sum().item()\n        total_samples += len(arm_targets)\n        \n        n_batches += 1\n        all_mu.append(mu.cpu().numpy())\n        all_sigma.append(sigma.cpu().numpy())\n        all_p_right.append(p_right.cpu().numpy())\n        all_d_pred.append(d_pred.cpu().numpy())\n        all_targets.append(targets.cpu().numpy())\n        all_d_targets.append(d_targets.cpu().numpy())\n        all_arm_targets.append(arm_targets.cpu().numpy())\n    \n    cls_acc = total_cls_correct / total_samples\n    return (total_loss / n_batches, total_cls_loss / n_batches, total_pos_loss / n_batches,\n            total_d_loss / n_batches, total_feas_loss / n_batches, cls_acc,\n            np.concatenate(all_mu), np.concatenate(all_sigma),\n            np.concatenate(all_p_right), np.concatenate(all_d_pred),\n            np.concatenate(all_targets), np.concatenate(all_d_targets),\n            np.concatenate(all_arm_targets))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Architecture du modèle hiérarchique\n",
    "\n",
    "**Backbone partagé** : identique à 02g (encodeurs CNN par shank → Transformer → masked avg pooling).\n",
    "\n",
    "**Heads spécialisés** :\n",
    "- `cls_head` : classification binaire (gauche/droite) → P(droite)\n",
    "- `mu_head_left` + `log_sigma_head_left` : régression position + incertitude pour le bras gauche\n",
    "- `mu_head_right` + `log_sigma_head_right` : idem pour le bras droit\n",
    "- `d_head` : prédiction de d (tâche auxiliaire conservée)\n",
    "\n",
    "**Combinaison à l'inférence** :\n",
    "```\n",
    "p = P(droite)\n",
    "mu = p * mu_right + (1-p) * mu_left\n",
    "sigma² = p * (sigma_right² + mu_right²) + (1-p) * (sigma_left² + mu_left²) - mu²\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpikeEncoder(nn.Module):\n",
    "    \"\"\"Encode un waveform (MAX_CH, 32) en un vecteur de dimension embed_dim.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_channels, embed_dim):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(n_channels, 32, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, embed_dim, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x).squeeze(-1)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Sinusoidal positional encoding.\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, max_len=256):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "class SpikeTransformerHierarchical(nn.Module):\n",
    "    \"\"\"Transformer hiérarchique : classification bras + régression conditionnelle.\n",
    "    \n",
    "    Le backbone Transformer est partagé. Deux heads de régression spécialisés\n",
    "    (gauche et droite) sont combinés via la probabilité de classification.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, nGroups, nChannelsPerGroup, embed_dim=64, nhead=4, \n",
    "                 num_layers=2, dropout=0.2, spike_dropout=0.15, noise_std=0.5,\n",
    "                 max_channels=MAX_CHANNELS):\n",
    "        super().__init__()\n",
    "        self.nGroups = nGroups\n",
    "        self.embed_dim = embed_dim\n",
    "        self.max_channels = max_channels\n",
    "        self.spike_dropout = spike_dropout\n",
    "        self.noise_std = noise_std\n",
    "        \n",
    "        # Un encodeur par shank\n",
    "        self.spike_encoders = nn.ModuleList([\n",
    "            SpikeEncoder(max_channels, embed_dim) for _ in range(nGroups)\n",
    "        ])\n",
    "        \n",
    "        # Embedding de shank\n",
    "        self.shank_embedding = nn.Embedding(nGroups, embed_dim)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoding = PositionalEncoding(embed_dim)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, nhead=nhead, dim_feedforward=embed_dim * 4,\n",
    "            dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer, num_layers=num_layers, enable_nested_tensor=False\n",
    "        )\n",
    "        \n",
    "        # --- Tête de classification : P(bras droit) ---\n",
    "        self.cls_head = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim, 1)\n",
    "        )\n",
    "        \n",
    "        # --- Tête bras gauche : position + incertitude ---\n",
    "        self.mu_head_left = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim, 2)\n",
    "        )\n",
    "        self.log_sigma_head_left = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim, 2)\n",
    "        )\n",
    "        \n",
    "        # --- Tête bras droit : position + incertitude ---\n",
    "        self.mu_head_right = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim, 2)\n",
    "        )\n",
    "        self.log_sigma_head_right = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim, 2)\n",
    "        )\n",
    "        \n",
    "        # --- Tête distance curviligne ---\n",
    "        self.d_head = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def _apply_spike_dropout(self, mask):\n",
    "        if not self.training or self.spike_dropout <= 0:\n",
    "            return mask\n",
    "        drop_mask = torch.rand_like(mask.float()) < self.spike_dropout\n",
    "        active = ~mask\n",
    "        new_drops = drop_mask & active\n",
    "        remaining = active & ~new_drops\n",
    "        n_remaining = remaining.sum(dim=1)\n",
    "        all_dropped = n_remaining == 0\n",
    "        if all_dropped.any():\n",
    "            new_drops[all_dropped] = False\n",
    "        return mask | new_drops\n",
    "    \n",
    "    def _apply_waveform_noise(self, waveforms):\n",
    "        if not self.training or self.noise_std <= 0:\n",
    "            return waveforms\n",
    "        noise = torch.randn_like(waveforms) * self.noise_std\n",
    "        return waveforms + noise\n",
    "    \n",
    "    def forward(self, waveforms, shank_ids, mask):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            cls_logit: (batch, 1) - logit de classification P(droite)\n",
    "            mu_left: (batch, 2) - position prédite bras gauche\n",
    "            sigma_left: (batch, 2) - incertitude bras gauche\n",
    "            mu_right: (batch, 2) - position prédite bras droit\n",
    "            sigma_right: (batch, 2) - incertitude bras droit\n",
    "            d_pred: (batch, 1) - distance curviligne prédite\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = waveforms.shape[:2]\n",
    "        \n",
    "        # Data augmentation\n",
    "        mask = self._apply_spike_dropout(mask)\n",
    "        waveforms = self._apply_waveform_noise(waveforms)\n",
    "        \n",
    "        # Encode chaque spike\n",
    "        embeddings = torch.zeros(batch_size, seq_len, self.embed_dim, device=waveforms.device)\n",
    "        for g in range(self.nGroups):\n",
    "            group_mask = (shank_ids == g) & (~mask)\n",
    "            if group_mask.any():\n",
    "                group_wf = waveforms[group_mask]\n",
    "                group_emb = self.spike_encoders[g](group_wf)\n",
    "                embeddings[group_mask] = group_emb\n",
    "        \n",
    "        # Shank embedding + Positional encoding\n",
    "        shank_emb = self.shank_embedding(shank_ids)\n",
    "        embeddings = embeddings + shank_emb\n",
    "        embeddings = self.pos_encoding(embeddings)\n",
    "        \n",
    "        # Transformer\n",
    "        encoded = self.transformer(embeddings, src_key_padding_mask=mask)\n",
    "        \n",
    "        # Masked average pooling\n",
    "        active_mask = (~mask).unsqueeze(-1).float()\n",
    "        pooled = (encoded * active_mask).sum(dim=1) / (active_mask.sum(dim=1) + 1e-8)\n",
    "        \n",
    "        # --- Sorties ---\n",
    "        # Classification\n",
    "        cls_logit = self.cls_head(pooled)  # (batch, 1) - logit brut\n",
    "        \n",
    "        # Bras gauche\n",
    "        mu_left = self.mu_head_left(pooled)\n",
    "        sigma_left = torch.exp(self.log_sigma_head_left(pooled))\n",
    "        \n",
    "        # Bras droit\n",
    "        mu_right = self.mu_head_right(pooled)\n",
    "        sigma_right = torch.exp(self.log_sigma_head_right(pooled))\n",
    "        \n",
    "        # Distance curviligne\n",
    "        d_pred = self.d_head(pooled)\n",
    "        \n",
    "        return cls_logit, mu_left, sigma_left, mu_right, sigma_right, d_pred\n",
    "    \n",
    "    def predict(self, waveforms, shank_ids, mask):\n",
    "        \"\"\"Prédiction combinée via mélange pondéré par P(droite).\n",
    "        \n",
    "        Returns:\n",
    "            mu: (batch, 2) - position combinée\n",
    "            sigma: (batch, 2) - incertitude combinée\n",
    "            p_right: (batch, 1) - probabilité bras droit\n",
    "            d_pred: (batch, 1) - distance curviligne\n",
    "        \"\"\"\n",
    "        cls_logit, mu_left, sigma_left, mu_right, sigma_right, d_pred = self.forward(\n",
    "            waveforms, shank_ids, mask\n",
    "        )\n",
    "        \n",
    "        p_right = torch.sigmoid(cls_logit)  # (batch, 1)\n",
    "        p_left = 1.0 - p_right\n",
    "        \n",
    "        # Mélange pondéré des moyennes\n",
    "        mu = p_right * mu_right + p_left * mu_left\n",
    "        \n",
    "        # Variance totale (loi de la variance totale pour mélange)\n",
    "        # Var = E[Var] + Var[E] = p*sigma² + p*(mu - mu_combined)²\n",
    "        var_combined = (\n",
    "            p_right * (sigma_right ** 2 + mu_right ** 2)\n",
    "            + p_left * (sigma_left ** 2 + mu_left ** 2)\n",
    "            - mu ** 2\n",
    "        )\n",
    "        sigma = torch.sqrt(var_combined.clamp(min=1e-8))\n",
    "        \n",
    "        return mu, sigma, p_right, d_pred\n",
    "\n",
    "\n",
    "# Test rapide\n",
    "SPIKE_DROPOUT = 0.15\n",
    "NOISE_STD = 0.5\n",
    "model = SpikeTransformerHierarchical(nGroups, nChannelsPerGroup, embed_dim=64, nhead=4, num_layers=2,\n",
    "                                      spike_dropout=SPIKE_DROPOUT, noise_std=NOISE_STD)\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Modèle créé : {n_params:,} paramètres')\n",
    "print(f'Têtes : classification (gauche/droite) + 2 régressions conditionnelles + d curviligne')\n",
    "print(f'\\nComparaison avec 02g :')\n",
    "n_params_02g = n_params - sum(\n",
    "    p.numel() for name, p in model.named_parameters() \n",
    "    if 'left' in name or 'right' in name or 'cls_head' in name\n",
    ") + sum(\n",
    "    p.numel() for name, p in model.named_parameters() \n",
    "    if 'mu_head_left' in name  # proxy : une seule tête mu + sigma dans 02g\n",
    ") + sum(\n",
    "    p.numel() for name, p in model.named_parameters() \n",
    "    if 'log_sigma_head_left' in name\n",
    ")\n",
    "print(f'  02g : ~{n_params_02g:,} paramètres (1 tête position)')\n",
    "print(f'  02h : {n_params:,} paramètres (+{n_params - n_params_02g:,} pour les heads supplémentaires)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Split et DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Split temporel 90/10\n",
    "split_idx = int(len(df_moving) * 0.9)\n",
    "df_train_full = df_moving.iloc[:split_idx].reset_index(drop=True)\n",
    "df_test = df_moving.iloc[split_idx:].reset_index(drop=True)\n",
    "d_train_full = curvilinear_d[:split_idx]\n",
    "d_test = curvilinear_d[split_idx:]\n",
    "arm_train_full = arm_labels[:split_idx]\n",
    "arm_test = arm_labels[split_idx:]\n",
    "\n",
    "print(f'Train (full) : {len(df_train_full)} exemples')\n",
    "print(f'Test         : {len(df_test)} exemples')\n",
    "print(f'\\nDistribution bras (train) : gauche={int((arm_train_full == 0).sum())}, droite={int((arm_train_full == 1).sum())}')\n",
    "print(f'Distribution bras (test)  : gauche={int((arm_test == 0).sum())}, droite={int((arm_test == 1).sum())}')\n",
    "\n",
    "# KFold\n",
    "N_FOLDS = 5\n",
    "kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=41)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Test loader\n",
    "test_dataset = SpikeSequenceDataset(df_test, nGroups, nChannelsPerGroup, d_test, arm_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                         collate_fn=collate_fn, num_workers=0)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(df_train_full)):\n",
    "    print(f'  Fold {fold+1}: train={len(train_idx)}, val={len(val_idx)}')\n",
    "\n",
    "print(f'\\nTest: {len(test_dataset)} exemples, {len(test_loader)} batches')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Entraînement (5-Fold Cross-Validation)\n",
    "\n",
    "**Loss combinée** :\n",
    "```\n",
    "L = L_cls (BCE sur classification bras)\n",
    "  + L_pos_left (Gaussian NLL, masqué sur bras gauche)\n",
    "  + L_pos_right (Gaussian NLL, masqué sur bras droit)\n",
    "  + λ_d * L_curvilinear (MSE sur d)\n",
    "  + λ_feas * L_feasibility (pénalité hors labyrinthe)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparamètres\n",
    "EMBED_DIM = 64\n",
    "NHEAD = 4\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.2\n",
    "SPIKE_DROPOUT = 0.15\n",
    "NOISE_STD = 0.5\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "EPOCHS = 30\n",
    "PATIENCE = 7\n",
    "\n",
    "# Poids des loss auxiliaires\n",
    "LAMBDA_D = 1.0       # poids de la loss curviligne\n",
    "LAMBDA_FEAS = 10.0   # poids de la pénalité hors labyrinthe\n",
    "\n",
    "print(f'Hyperparamètres : embed_dim={EMBED_DIM}, nhead={NHEAD}, layers={NUM_LAYERS}, dropout={DROPOUT}')\n",
    "print(f'Data augmentation : spike dropout={SPIKE_DROPOUT:.0%}, gaussian noise std={NOISE_STD}')\n",
    "print(f'Loss : BCE(cls) + GaussianNLL(left, masqué) + GaussianNLL(right, masqué) + {LAMBDA_D}*MSE(d) + {LAMBDA_FEAS}*Feasibility')\n",
    "print(f'Entraînement : {EPOCHS} epochs max, patience={PATIENCE}, LR={LR}')\n",
    "print(f'Device : {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, scheduler, criterion_bce, criterion_nll, criterion_d, feas_loss, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_cls_loss = 0\n",
    "    total_pos_loss = 0\n",
    "    total_d_loss = 0\n",
    "    total_feas_loss = 0\n",
    "    total_cls_correct = 0\n",
    "    total_samples = 0\n",
    "    n_batches = 0\n",
    "    \n",
    "    for batch in loader:\n",
    "        wf = batch['waveforms'].to(device)\n",
    "        sid = batch['shank_ids'].to(device)\n",
    "        mask = batch['mask'].to(device)\n",
    "        targets = batch['targets'].to(device)\n",
    "        d_targets = batch['d_targets'].to(device)\n",
    "        arm_targets = batch['arm_targets'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        cls_logit, mu_left, sigma_left, mu_right, sigma_right, d_pred = model(wf, sid, mask)\n",
    "        \n",
    "        # --- Loss classification (BCE) ---\n",
    "        loss_cls = criterion_bce(cls_logit.squeeze(-1), arm_targets)\n",
    "        \n",
    "        # --- Loss position gauche (Gaussian NLL, masqué) ---\n",
    "        left_mask = (arm_targets == 0)\n",
    "        right_mask = (arm_targets == 1)\n",
    "        \n",
    "        loss_pos = torch.tensor(0.0, device=device)\n",
    "        if left_mask.any():\n",
    "            loss_left = criterion_nll(\n",
    "                mu_left[left_mask], targets[left_mask], sigma_left[left_mask] ** 2\n",
    "            )\n",
    "            loss_pos = loss_pos + loss_left\n",
    "        \n",
    "        if right_mask.any():\n",
    "            loss_right = criterion_nll(\n",
    "                mu_right[right_mask], targets[right_mask], sigma_right[right_mask] ** 2\n",
    "            )\n",
    "            loss_pos = loss_pos + loss_right\n",
    "        \n",
    "        # --- Loss curviligne (MSE) ---\n",
    "        loss_d = criterion_d(d_pred.squeeze(-1), d_targets)\n",
    "        \n",
    "        # --- Loss faisabilité (sur la prédiction combinée) ---\n",
    "        with torch.no_grad():\n",
    "            p_right = torch.sigmoid(cls_logit)\n",
    "        p_right_detached = torch.sigmoid(cls_logit)  # garder le gradient pour mu\n",
    "        p_left = 1.0 - p_right_detached\n",
    "        mu_combined = p_right_detached * mu_right + p_left * mu_left\n",
    "        loss_feas = feas_loss(mu_combined)\n",
    "        \n",
    "        # --- Loss totale ---\n",
    "        loss = loss_cls + loss_pos + LAMBDA_D * loss_d + LAMBDA_FEAS * loss_feas\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_cls_loss += loss_cls.item()\n",
    "        total_pos_loss += loss_pos.item()\n",
    "        total_d_loss += loss_d.item()\n",
    "        total_feas_loss += loss_feas.item()\n",
    "        \n",
    "        # Accuracy classification\n",
    "        with torch.no_grad():\n",
    "            preds_cls = (torch.sigmoid(cls_logit.squeeze(-1)) >= 0.5).float()\n",
    "            total_cls_correct += (preds_cls == arm_targets).sum().item()\n",
    "            total_samples += len(arm_targets)\n",
    "        \n",
    "        n_batches += 1\n",
    "    \n",
    "    cls_acc = total_cls_correct / total_samples\n",
    "    return (total_loss / n_batches, total_cls_loss / n_batches, total_pos_loss / n_batches,\n",
    "            total_d_loss / n_batches, total_feas_loss / n_batches, cls_acc)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(model, loader, criterion_bce, criterion_nll, criterion_d, feas_loss, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_cls_loss = 0\n",
    "    total_pos_loss = 0\n",
    "    total_d_loss = 0\n",
    "    total_feas_loss = 0\n",
    "    total_cls_correct = 0\n",
    "    total_samples = 0\n",
    "    n_batches = 0\n",
    "    all_mu = []\n",
    "    all_sigma = []\n",
    "    all_p_right = []\n",
    "    all_d_pred = []\n",
    "    all_targets = []\n",
    "    all_d_targets = []\n",
    "    all_arm_targets = []\n",
    "    \n",
    "    for batch in loader:\n",
    "        wf = batch['waveforms'].to(device)\n",
    "        sid = batch['shank_ids'].to(device)\n",
    "        mask = batch['mask'].to(device)\n",
    "        targets = batch['targets'].to(device)\n",
    "        d_targets = batch['d_targets'].to(device)\n",
    "        arm_targets = batch['arm_targets'].to(device)\n",
    "        \n",
    "        # Prédiction combinée\n",
    "        mu, sigma, p_right, d_pred = model.predict(wf, sid, mask)\n",
    "        \n",
    "        # Recalculer les sorties brutes pour la loss\n",
    "        cls_logit, mu_left, sigma_left, mu_right, sigma_right, _ = model(wf, sid, mask)\n",
    "        \n",
    "        # Loss classification\n",
    "        loss_cls = criterion_bce(cls_logit.squeeze(-1), arm_targets)\n",
    "        \n",
    "        # Loss position (masquée par bras)\n",
    "        left_mask = (arm_targets == 0)\n",
    "        right_mask = (arm_targets == 1)\n",
    "        \n",
    "        loss_pos = torch.tensor(0.0, device=device)\n",
    "        if left_mask.any():\n",
    "            loss_pos = loss_pos + criterion_nll(\n",
    "                mu_left[left_mask], targets[left_mask], sigma_left[left_mask] ** 2\n",
    "            )\n",
    "        if right_mask.any():\n",
    "            loss_pos = loss_pos + criterion_nll(\n",
    "                mu_right[right_mask], targets[right_mask], sigma_right[right_mask] ** 2\n",
    "            )\n",
    "        \n",
    "        loss_d = criterion_d(d_pred.squeeze(-1), d_targets)\n",
    "        loss_feas = feas_loss(mu)\n",
    "        loss = loss_cls + loss_pos + LAMBDA_D * loss_d + LAMBDA_FEAS * loss_feas\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_cls_loss += loss_cls.item()\n",
    "        total_pos_loss += loss_pos.item()\n",
    "        total_d_loss += loss_d.item()\n",
    "        total_feas_loss += loss_feas.item()\n",
    "        \n",
    "        preds_cls = (p_right.squeeze(-1) >= 0.5).float()\n",
    "        total_cls_correct += (preds_cls == arm_targets).sum().item()\n",
    "        total_samples += len(arm_targets)\n",
    "        \n",
    "        n_batches += 1\n",
    "        all_mu.append(mu.cpu().numpy())\n",
    "        all_sigma.append(sigma.cpu().numpy())\n",
    "        all_p_right.append(p_right.cpu().numpy())\n",
    "        all_d_pred.append(d_pred.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "        all_d_targets.append(d_targets.cpu().numpy())\n",
    "        all_arm_targets.append(arm_targets.cpu().numpy())\n",
    "    \n",
    "    cls_acc = total_cls_correct / total_samples\n",
    "    return (total_loss / n_batches, total_cls_loss / n_batches, total_pos_loss / n_batches,\n",
    "            total_d_loss / n_batches, total_feas_loss / n_batches, cls_acc,\n",
    "            np.concatenate(all_mu), np.concatenate(all_sigma),\n",
    "            np.concatenate(all_p_right), np.concatenate(all_d_pred),\n",
    "            np.concatenate(all_targets), np.concatenate(all_d_targets),\n",
    "            np.concatenate(all_arm_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boucle d'entraînement avec KFold\n",
    "fold_results = []\n",
    "all_train_losses = {}\n",
    "all_val_losses = {}\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(df_train_full)):\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'FOLD {fold+1}/{N_FOLDS}')\n",
    "    print(f'{\"=\"*60}')\n",
    "    \n",
    "    # Datasets pour ce fold\n",
    "    df_fold_train = df_train_full.iloc[train_idx].reset_index(drop=True)\n",
    "    df_fold_val = df_train_full.iloc[val_idx].reset_index(drop=True)\n",
    "    d_fold_train = d_train_full[train_idx]\n",
    "    d_fold_val = d_train_full[val_idx]\n",
    "    arm_fold_train = arm_train_full[train_idx]\n",
    "    arm_fold_val = arm_train_full[val_idx]\n",
    "    \n",
    "    fold_train_dataset = SpikeSequenceDataset(df_fold_train, nGroups, nChannelsPerGroup, d_fold_train, arm_fold_train)\n",
    "    fold_val_dataset = SpikeSequenceDataset(df_fold_val, nGroups, nChannelsPerGroup, d_fold_val, arm_fold_val)\n",
    "    \n",
    "    fold_train_loader = DataLoader(fold_train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                                   collate_fn=collate_fn, num_workers=0)\n",
    "    fold_val_loader = DataLoader(fold_val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                                 collate_fn=collate_fn, num_workers=0)\n",
    "    \n",
    "    print(f'  Train: {len(fold_train_dataset)}, Val: {len(fold_val_dataset)}')\n",
    "    \n",
    "    # Nouveau modèle\n",
    "    model = SpikeTransformerHierarchical(\n",
    "        nGroups, nChannelsPerGroup,\n",
    "        embed_dim=EMBED_DIM, nhead=NHEAD, num_layers=NUM_LAYERS,\n",
    "        dropout=DROPOUT, spike_dropout=SPIKE_DROPOUT, noise_std=NOISE_STD\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=LR, epochs=EPOCHS, steps_per_epoch=len(fold_train_loader)\n",
    "    )\n",
    "    criterion_bce = nn.BCEWithLogitsLoss()\n",
    "    criterion_nll = nn.GaussianNLLLoss()\n",
    "    criterion_d = nn.MSELoss()\n",
    "    feas_loss_fn = FeasibilityLoss(SKELETON_SEGMENTS, CORRIDOR_HALF_WIDTH).to(DEVICE)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    model_path = f'../outputs/best_transformer_02h_fold{fold+1}.pt'\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        t_loss, t_cls, t_pos, t_d, t_feas, t_acc = train_epoch(\n",
    "            model, fold_train_loader, optimizer, scheduler,\n",
    "            criterion_bce, criterion_nll, criterion_d, feas_loss_fn, DEVICE\n",
    "        )\n",
    "        v_loss, v_cls, v_pos, v_d, v_feas, v_acc, _, _, _, _, _, _, _ = eval_epoch(\n",
    "            model, fold_val_loader, criterion_bce, criterion_nll, criterion_d, feas_loss_fn, DEVICE\n",
    "        )\n",
    "        \n",
    "        train_losses.append(t_loss)\n",
    "        val_losses.append(v_loss)\n",
    "        \n",
    "        if epoch % 5 == 0 or epoch == EPOCHS - 1:\n",
    "            print(f'  Epoch {epoch+1:02d}/{EPOCHS} | Train: {t_loss:.4f} (cls={t_cls:.4f}, pos={t_pos:.4f}, d={t_d:.5f}, feas={t_feas:.6f}, acc={t_acc:.1%}) | Val: {v_loss:.4f} (acc={v_acc:.1%})')\n",
    "        \n",
    "        if v_loss < best_val_loss:\n",
    "            best_val_loss = v_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f'  Early stopping a epoch {epoch+1}')\n",
    "                break\n",
    "    \n",
    "    all_train_losses[fold] = train_losses\n",
    "    all_val_losses[fold] = val_losses\n",
    "    \n",
    "    # Évaluer sur la validation de ce fold\n",
    "    model.load_state_dict(torch.load(model_path, map_location=DEVICE, weights_only=True))\n",
    "    (_, _, _, _, _, val_acc, val_mu, val_sigma, val_p_right, val_d_pred, \n",
    "     val_targets, val_d_targets, val_arm_targets) = eval_epoch(\n",
    "        model, fold_val_loader, criterion_bce, criterion_nll, criterion_d, feas_loss_fn, DEVICE\n",
    "    )\n",
    "    val_eucl = np.sqrt((val_targets[:, 0] - val_mu[:, 0])**2 + (val_targets[:, 1] - val_mu[:, 1])**2)\n",
    "    val_d_mae = np.abs(val_d_targets - val_d_pred.squeeze()).mean()\n",
    "    \n",
    "    # % de prédictions hors labyrinthe\n",
    "    val_dist_to_skel = np.array([\n",
    "        compute_distance_to_skeleton(val_mu[i, 0], val_mu[i, 1]) for i in range(len(val_mu))\n",
    "    ])\n",
    "    pct_outside = (val_dist_to_skel > CORRIDOR_HALF_WIDTH).mean()\n",
    "    \n",
    "    fold_results.append({\n",
    "        'fold': fold + 1,\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'val_eucl_mean': val_eucl.mean(),\n",
    "        'val_r2_x': r2_score(val_targets[:, 0], val_mu[:, 0]),\n",
    "        'val_r2_y': r2_score(val_targets[:, 1], val_mu[:, 1]),\n",
    "        'val_d_mae': val_d_mae,\n",
    "        'val_cls_acc': val_acc,\n",
    "        'val_pct_outside': pct_outside,\n",
    "        'epochs': len(train_losses),\n",
    "    })\n",
    "    print(f'  Best val loss: {best_val_loss:.5f} | Eucl: {val_eucl.mean():.4f} | R2: X={fold_results[-1][\"val_r2_x\"]:.4f}, Y={fold_results[-1][\"val_r2_y\"]:.4f}')\n",
    "    print(f'  d MAE: {val_d_mae:.4f} | Cls acc: {val_acc:.1%} | Hors labyrinthe: {pct_outside:.1%}')\n",
    "\n",
    "# Résumé\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print(f'RESUME CROSS-VALIDATION ({N_FOLDS} folds)')\n",
    "print(f'{\"=\"*60}')\n",
    "for r in fold_results:\n",
    "    print(f'  Fold {r[\"fold\"]}: Loss={r[\"best_val_loss\"]:.5f} | Eucl={r[\"val_eucl_mean\"]:.4f} | R2_X={r[\"val_r2_x\"]:.4f} | R2_Y={r[\"val_r2_y\"]:.4f} | d_MAE={r[\"val_d_mae\"]:.4f} | cls_acc={r[\"val_cls_acc\"]:.1%} | hors={r[\"val_pct_outside\"]:.1%}')\n",
    "\n",
    "mean_eucl = np.mean([r['val_eucl_mean'] for r in fold_results])\n",
    "std_eucl = np.std([r['val_eucl_mean'] for r in fold_results])\n",
    "mean_r2_x = np.mean([r['val_r2_x'] for r in fold_results])\n",
    "mean_r2_y = np.mean([r['val_r2_y'] for r in fold_results])\n",
    "mean_d_mae = np.mean([r['val_d_mae'] for r in fold_results])\n",
    "mean_cls_acc = np.mean([r['val_cls_acc'] for r in fold_results])\n",
    "mean_outside = np.mean([r['val_pct_outside'] for r in fold_results])\n",
    "print(f'\\n  Moyenne : Eucl={mean_eucl:.4f} (+/- {std_eucl:.4f}) | R2_X={mean_r2_x:.4f} | R2_Y={mean_r2_y:.4f} | d_MAE={mean_d_mae:.4f} | cls_acc={mean_cls_acc:.1%} | hors={mean_outside:.1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courbes d'entraînement par fold\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, N_FOLDS))\n",
    "\n",
    "for fold in range(N_FOLDS):\n",
    "    axes[0].plot(all_train_losses[fold], color=colors[fold], linewidth=1.5, label=f'Fold {fold+1}')\n",
    "    axes[1].plot(all_val_losses[fold], color=colors[fold], linewidth=1.5, label=f'Fold {fold+1}')\n",
    "\n",
    "axes[0].set_xlabel('Epoch'); axes[0].set_ylabel('Loss totale')\n",
    "axes[0].set_title('Train Loss par fold'); axes[0].legend(); axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].set_xlabel('Epoch'); axes[1].set_ylabel('Loss totale')\n",
    "axes[1].set_title('Validation Loss par fold'); axes[1].legend(); axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Évaluation finale sur le test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluation ensemble des 5 folds sur le test set\n",
    "criterion_bce = nn.BCEWithLogitsLoss()\n",
    "criterion_nll = nn.GaussianNLLLoss()\n",
    "criterion_d = nn.MSELoss()\n",
    "feas_loss_fn = FeasibilityLoss(SKELETON_SEGMENTS, CORRIDOR_HALF_WIDTH).to(DEVICE)\n",
    "\n",
    "all_fold_mu = []\n",
    "all_fold_sigma = []\n",
    "all_fold_p_right = []\n",
    "all_fold_d = []\n",
    "\n",
    "for fold in range(N_FOLDS):\n",
    "    model_path = f'../outputs/best_transformer_02h_fold{fold+1}.pt'\n",
    "    model = SpikeTransformerHierarchical(\n",
    "        nGroups, nChannelsPerGroup,\n",
    "        embed_dim=EMBED_DIM, nhead=NHEAD, num_layers=NUM_LAYERS,\n",
    "        dropout=DROPOUT, spike_dropout=SPIKE_DROPOUT, noise_std=NOISE_STD\n",
    "    ).to(DEVICE)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=DEVICE, weights_only=True))\n",
    "    \n",
    "    (_, _, _, _, _, fold_acc, fold_mu, fold_sigma, fold_p_right, fold_d,\n",
    "     y_test, d_test_targets, arm_test_targets) = eval_epoch(\n",
    "        model, test_loader, criterion_bce, criterion_nll, criterion_d, feas_loss_fn, DEVICE\n",
    "    )\n",
    "    all_fold_mu.append(fold_mu)\n",
    "    all_fold_sigma.append(fold_sigma)\n",
    "    all_fold_p_right.append(fold_p_right)\n",
    "    all_fold_d.append(fold_d)\n",
    "    \n",
    "    fold_eucl = np.sqrt((y_test[:, 0] - fold_mu[:, 0])**2 + (y_test[:, 1] - fold_mu[:, 1])**2)\n",
    "    print(f'Fold {fold+1} sur test: Eucl={fold_eucl.mean():.4f}, cls_acc={fold_acc:.1%}')\n",
    "\n",
    "# Ensemble\n",
    "all_fold_mu = np.stack(all_fold_mu)\n",
    "all_fold_sigma = np.stack(all_fold_sigma)\n",
    "all_fold_p_right = np.stack(all_fold_p_right)\n",
    "all_fold_d = np.stack(all_fold_d)\n",
    "\n",
    "y_pred = all_fold_mu.mean(axis=0)\n",
    "d_pred_ensemble = all_fold_d.mean(axis=0).squeeze()\n",
    "p_right_ensemble = all_fold_p_right.mean(axis=0).squeeze()\n",
    "\n",
    "# Sigma ensemble (loi de la variance totale)\n",
    "mean_var = (all_fold_sigma ** 2).mean(axis=0)\n",
    "var_mu = all_fold_mu.var(axis=0)\n",
    "y_sigma = np.sqrt(mean_var + var_mu)\n",
    "\n",
    "# --- Métriques position ---\n",
    "mse_x = mean_squared_error(y_test[:, 0], y_pred[:, 0])\n",
    "mse_y = mean_squared_error(y_test[:, 1], y_pred[:, 1])\n",
    "mae_x = mean_absolute_error(y_test[:, 0], y_pred[:, 0])\n",
    "mae_y = mean_absolute_error(y_test[:, 1], y_pred[:, 1])\n",
    "r2_x = r2_score(y_test[:, 0], y_pred[:, 0])\n",
    "r2_y = r2_score(y_test[:, 1], y_pred[:, 1])\n",
    "eucl_errors = np.sqrt((y_test[:, 0] - y_pred[:, 0])**2 + (y_test[:, 1] - y_pred[:, 1])**2)\n",
    "\n",
    "# --- Métriques d ---\n",
    "d_mae = np.abs(d_test_targets - d_pred_ensemble).mean()\n",
    "d_r2 = r2_score(d_test_targets, d_pred_ensemble)\n",
    "\n",
    "# --- Métriques classification ---\n",
    "arm_pred = (p_right_ensemble >= 0.5).astype(float)\n",
    "cls_accuracy = (arm_pred == arm_test_targets).mean()\n",
    "\n",
    "# --- % hors labyrinthe ---\n",
    "test_dist_to_skel = np.array([\n",
    "    compute_distance_to_skeleton(y_pred[i, 0], y_pred[i, 1]) for i in range(len(y_pred))\n",
    "])\n",
    "pct_outside = (test_dist_to_skel > CORRIDOR_HALF_WIDTH).mean()\n",
    "\n",
    "# --- Taux de confusion de bras ---\n",
    "# Confusion = prédiction dans le mauvais bras (arm_pred != arm_true)\n",
    "arm_confusion_mask = arm_pred != arm_test_targets\n",
    "arm_confusion_rate = arm_confusion_mask.mean()\n",
    "eucl_confusion = eucl_errors[arm_confusion_mask].mean() if arm_confusion_mask.any() else 0.0\n",
    "eucl_correct = eucl_errors[~arm_confusion_mask].mean()\n",
    "\n",
    "# --- Erreur par zone ---\n",
    "left_zone = d_test_targets < 0.354\n",
    "corridor_zone = (d_test_targets >= 0.354) & (d_test_targets <= 0.646)\n",
    "right_zone = d_test_targets > 0.646\n",
    "\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print(f'Transformer 02h : Hiérarchique — Ensemble ({N_FOLDS} folds)')\n",
    "print(f'{\"=\"*60}')\n",
    "print(f'  MSE  : X={mse_x:.5f}, Y={mse_y:.5f}')\n",
    "print(f'  MAE  : X={mae_x:.4f}, Y={mae_y:.4f}')\n",
    "print(f'  R²   : X={r2_x:.4f}, Y={r2_y:.4f}')\n",
    "print(f'  Eucl : mean={eucl_errors.mean():.4f}, median={np.median(eucl_errors):.4f}, p90={np.percentile(eucl_errors, 90):.4f}')\n",
    "print(f'\\n  d curviligne : MAE={d_mae:.4f}, R²={d_r2:.4f}')\n",
    "print(f'  Classification bras : accuracy={cls_accuracy:.1%}')\n",
    "print(f'  Hors labyrinthe : {pct_outside:.1%}')\n",
    "print(f'\\n  Confusion de bras : {arm_confusion_rate:.1%}')\n",
    "print(f'    Eucl (bras correct)   : {eucl_correct:.4f}')\n",
    "print(f'    Eucl (bras confondu)  : {eucl_confusion:.4f}')\n",
    "print(f'\\n  Erreur par zone :')\n",
    "print(f'    Bras gauche (d<0.354) : Eucl={eucl_errors[left_zone].mean():.4f} ({left_zone.sum()} points)')\n",
    "print(f'    Couloir haut          : Eucl={eucl_errors[corridor_zone].mean():.4f} ({corridor_zone.sum()} points)')\n",
    "print(f'    Bras droit (d>0.646)  : Eucl={eucl_errors[right_zone].mean():.4f} ({right_zone.sum()} points)')\n",
    "print(f'\\n  Sigma moyen : X={y_sigma[:, 0].mean():.4f}, Y={y_sigma[:, 1].mean():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Scatter pred vs true ---\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "axes[0].scatter(y_test[:, 0], y_pred[:, 0], s=1, alpha=0.3)\n",
    "axes[0].plot([0, 1], [0, 1], 'r--', linewidth=2)\n",
    "axes[0].set_xlabel('True X'); axes[0].set_ylabel('Predicted X')\n",
    "axes[0].set_title(f'02h - Position X (R²={r2_x:.3f})')\n",
    "axes[0].set_aspect('equal')\n",
    "\n",
    "axes[1].scatter(y_test[:, 1], y_pred[:, 1], s=1, alpha=0.3)\n",
    "axes[1].plot([0, 1], [0, 1], 'r--', linewidth=2)\n",
    "axes[1].set_xlabel('True Y'); axes[1].set_ylabel('Predicted Y')\n",
    "axes[1].set_title(f'02h - Position Y (R²={r2_y:.3f})')\n",
    "axes[1].set_aspect('equal')\n",
    "\n",
    "axes[2].scatter(d_test_targets, d_pred_ensemble, s=1, alpha=0.3)\n",
    "axes[2].plot([0, 1], [0, 1], 'r--', linewidth=2)\n",
    "axes[2].set_xlabel('True d'); axes[2].set_ylabel('Predicted d')\n",
    "axes[2].set_title(f'02h - Distance curviligne (R²={d_r2:.3f})')\n",
    "axes[2].set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Trajectoire + classification + confusion ---\n",
    "segment = slice(0, 500)\n",
    "seg_idx = np.arange(500)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Trajectoire 2D avec squelette\n",
    "axes[0, 0].plot(y_test[segment, 0], y_test[segment, 1], 'b-', alpha=0.5, label='Vraie trajectoire', linewidth=1)\n",
    "axes[0, 0].plot(y_pred[segment, 0], y_pred[segment, 1], 'r-', alpha=0.5, label='Prediction (mu)', linewidth=1)\n",
    "for x1, y1, x2, y2 in SKELETON_SEGMENTS:\n",
    "    axes[0, 0].plot([x1, x2], [y1, y2], 'k--', linewidth=1, alpha=0.3)\n",
    "axes[0, 0].set_xlabel('X'); axes[0, 0].set_ylabel('Y')\n",
    "axes[0, 0].set_title('Trajectoire (500 premiers points test)')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].set_aspect('equal')\n",
    "\n",
    "# 2. P(droite) sur le temps\n",
    "axes[0, 1].plot(seg_idx, arm_test_targets[segment], 'b-', label='Vrai bras (0=G, 1=D)', linewidth=1.5, alpha=0.5)\n",
    "axes[0, 1].plot(seg_idx, p_right_ensemble[segment], 'r-', alpha=0.7, label='P(droite)', linewidth=1)\n",
    "axes[0, 1].axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[0, 1].set_xlabel('Index'); axes[0, 1].set_ylabel('P(droite)')\n",
    "axes[0, 1].set_title('Classification bras : P(droite) vs vérité')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# 3. Distance curviligne\n",
    "axes[1, 0].plot(seg_idx, d_test_targets[segment], 'b-', label='Vrai d', linewidth=1.5)\n",
    "axes[1, 0].plot(seg_idx, d_pred_ensemble[segment], 'r-', alpha=0.7, label='Prediction d', linewidth=1)\n",
    "axes[1, 0].axhline(y=ARM_THRESHOLD, color='gray', linestyle='--', alpha=0.5, label=f'Seuil d={ARM_THRESHOLD}')\n",
    "axes[1, 0].set_xlabel('Index'); axes[1, 0].set_ylabel('d (curviligne)')\n",
    "axes[1, 0].set_title('Distance curviligne le long du U')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# 4. Carte de confusion de bras\n",
    "correct_mask = ~arm_confusion_mask\n",
    "axes[1, 1].scatter(y_test[correct_mask, 0], y_test[correct_mask, 1], \n",
    "                    c='green', s=1, alpha=0.2, label=f'Bras correct ({correct_mask.mean():.1%})')\n",
    "if arm_confusion_mask.any():\n",
    "    axes[1, 1].scatter(y_test[arm_confusion_mask, 0], y_test[arm_confusion_mask, 1], \n",
    "                        c='red', s=5, alpha=0.8, label=f'Bras confondu ({arm_confusion_rate:.1%})')\n",
    "for x1, y1, x2, y2 in SKELETON_SEGMENTS:\n",
    "    axes[1, 1].plot([x1, x2], [y1, y2], 'k--', linewidth=1, alpha=0.3)\n",
    "axes[1, 1].set_xlabel('X'); axes[1, 1].set_ylabel('Y')\n",
    "axes[1, 1].set_title('Confusion de bras (positions réelles)')\n",
    "axes[1, 1].legend(markerscale=5)\n",
    "axes[1, 1].set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Heatmaps : erreur + hors labyrinthe + P(droite) ---\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 7))\n",
    "\n",
    "nbins = 20\n",
    "x_edges = np.linspace(0, 1, nbins + 1)\n",
    "y_edges = np.linspace(0, 1, nbins + 1)\n",
    "\n",
    "for ax_idx, (title, values, cmap, pos_for_binning) in enumerate([\n",
    "    ('Erreur euclidienne moyenne', eucl_errors, 'RdYlGn_r', y_test),\n",
    "    ('Sigma moyen predit', (y_sigma[:, 0] + y_sigma[:, 1]) / 2, 'RdYlGn_r', y_test),\n",
    "    ('P(droite) moyenne', p_right_ensemble, 'RdBu_r', y_test)\n",
    "]):\n",
    "    val_map = np.full((nbins, nbins), np.nan)\n",
    "    count_map = np.zeros((nbins, nbins))\n",
    "    \n",
    "    for i in range(len(pos_for_binning)):\n",
    "        xi = np.clip(np.searchsorted(x_edges, pos_for_binning[i, 0]) - 1, 0, nbins - 1)\n",
    "        yi = np.clip(np.searchsorted(y_edges, pos_for_binning[i, 1]) - 1, 0, nbins - 1)\n",
    "        if np.isnan(val_map[yi, xi]):\n",
    "            val_map[yi, xi] = 0\n",
    "        val_map[yi, xi] += values[i]\n",
    "        count_map[yi, xi] += 1\n",
    "    \n",
    "    mean_map = np.where(count_map > 0, val_map / count_map, np.nan)\n",
    "    \n",
    "    im = axes[ax_idx].imshow(mean_map, origin='lower', aspect='equal', \n",
    "                              cmap=cmap, extent=[0, 1, 0, 1])\n",
    "    axes[ax_idx].set_xlabel('X'); axes[ax_idx].set_ylabel('Y')\n",
    "    axes[ax_idx].set_title(title)\n",
    "    plt.colorbar(im, ax=axes[ax_idx])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Calibration ---\n",
    "sigma_mean = (y_sigma[:, 0] + y_sigma[:, 1]) / 2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(sigma_mean, eucl_errors, s=1, alpha=0.3)\n",
    "ax.set_xlabel('Sigma moyen predit'); ax.set_ylabel('Erreur euclidienne reelle')\n",
    "ax.set_title('Calibration : incertitude vs erreur')\n",
    "sigma_range = np.linspace(0, sigma_mean.max(), 100)\n",
    "ax.plot(sigma_range, 2 * sigma_range, 'r--', label='y = 2*sigma', linewidth=1.5)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calibration\n",
    "in_1sigma = np.mean(eucl_errors < sigma_mean)\n",
    "in_2sigma = np.mean(eucl_errors < 2 * sigma_mean)\n",
    "in_3sigma = np.mean(eucl_errors < 3 * sigma_mean)\n",
    "print(f'Calibration de l\\'incertitude :')\n",
    "print(f'  Erreur < 1*sigma : {in_1sigma:.1%} (attendu ~39%)')\n",
    "print(f'  Erreur < 2*sigma : {in_2sigma:.1%} (attendu ~86%)')\n",
    "print(f'  Erreur < 3*sigma : {in_3sigma:.1%} (attendu ~99%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Sauvegarde des prédictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../outputs/preds_transformer_02h.npy', y_pred)\n",
    "np.save('../outputs/sigma_transformer_02h.npy', y_sigma)\n",
    "np.save('../outputs/d_pred_transformer_02h.npy', d_pred_ensemble)\n",
    "np.save('../outputs/y_test_transformer_02h.npy', y_test)\n",
    "np.save('../outputs/d_test_transformer_02h.npy', d_test_targets)\n",
    "print(f'Predictions ensemble ({N_FOLDS} folds) sauvegardees.')\n",
    "print(f'  preds_transformer_02h.npy : mu ensemble ({y_pred.shape})')\n",
    "print(f'  sigma_transformer_02h.npy : sigma ensemble ({y_sigma.shape})')\n",
    "print(f'  d_pred_transformer_02h.npy : d ensemble ({d_pred_ensemble.shape})')\n",
    "print(f'  y_test_transformer_02h.npy : targets ({y_test.shape})')\n",
    "print(f'  d_test_transformer_02h.npy : d targets ({d_test_targets.shape})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Interprétation\n",
    "\n",
    "### Approche hiérarchique vs approche directe (02g)\n",
    "\n",
    "Le notebook 02g prédit (x, y) et d simultanément avec une seule tête de régression. La distance curviligne d force indirectement le backbone à distinguer les bras, mais la tête de position reste unique et doit gérer les deux bras.\n",
    "\n",
    "Le notebook 02h sépare explicitement le problème :\n",
    "\n",
    "1. **Classification** : le head `cls_head` apprend directement à distinguer gauche vs droite. C'est une tâche binaire simple que le modèle devrait résoudre avec une accuracy très élevée.\n",
    "\n",
    "2. **Régression conditionnelle** : chaque head de position ne voit que les exemples de son bras pendant l'entraînement. Il peut donc se spécialiser :\n",
    "   - Le head gauche apprend que x ≈ 0.15 et y varie de 0 à 0.85\n",
    "   - Le head droit apprend que x ≈ 0.85 et y varie de 0 à 0.85\n",
    "\n",
    "3. **Mélange à l'inférence** : la prédiction finale est `mu = p * mu_right + (1-p) * mu_left`. Quand la classification est confiante (p ≈ 0 ou p ≈ 1), on obtient quasiment la prédiction du bon head. Dans la zone de transition (couloir haut), le mélange interpolate entre les deux heads.\n",
    "\n",
    "### Avantages attendus\n",
    "\n",
    "- **Réduction des erreurs de confusion de bras** : la classification explicite rend les \"sauts\" entre bras moins probables\n",
    "- **Spécialisation des heads** : chaque head peut apprendre une géométrie plus simple (essentiellement 1D le long de son bras)\n",
    "- **Incertitude mieux calibrée** : dans la zone de transition, l'incertitude augmente naturellement via la variance du mélange\n",
    "\n",
    "### Points d'attention\n",
    "\n",
    "- **Plus de paramètres** : 2 heads de régression au lieu de 1, plus le head de classification. Le risque d'overfitting augmente légèrement.\n",
    "- **Zone de transition** : le couloir haut est la zone la plus difficile car les deux heads doivent y contribuer via le mélange\n",
    "- **Propagation d'erreur** : si la classification se trompe, l'erreur de position est amplifiée car le mauvais head domine"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}