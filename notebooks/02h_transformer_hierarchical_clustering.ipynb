{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 02h : Transformer Hiérarchique (Classification 3 zones + Régression conditionnelle)\n",
    "\n",
    "## Modifications par rapport au notebook 02g\n",
    "\n",
    "1. **Classification explicite 3 zones** : un head de classification prédit la zone (gauche / haut / droite) à partir de la distance curviligne `d`. Les coins du U sont inclus dans le couloir haut.\n",
    "\n",
    "2. **Régression conditionnelle** : trois heads de régression spécialisés (un par zone) prédisent (x, y) + incertitude. La prédiction finale est un **mélange pondéré** par les probabilités de classification (softmax).\n",
    "\n",
    "3. **Loss hiérarchique** : chaque head de régression n'est entraîné que sur les exemples de sa zone (routage par les labels ground truth).\n",
    "\n",
    "4. **Spike dropout (15%) + Gaussian noise (std=0.5)** : data augmentation identique au notebook 02d.\n",
    "\n",
    "**Géométrie du U** (coordonnées normalisées [0, 1]) :\n",
    "- Bras gauche : x ∈ [0, 0.3], y ∈ [0, 1]\n",
    "- Bras droit : x ∈ [0.7, 1.0], y ∈ [0, 1]\n",
    "- Couloir haut : x ∈ [0, 1], y ∈ [0.7, 1.0]\n",
    "- Largeur du couloir : 0.3\n",
    "\n",
    "**Squelette central** : 3 segments formant un U :\n",
    "1. (0.15, 0) → (0.15, 0.85) — bras gauche, longueur 0.85\n",
    "2. (0.15, 0.85) → (0.85, 0.85) — couloir haut, longueur 0.70\n",
    "3. (0.85, 0.85) → (0.85, 0) — bras droit, longueur 0.85\n",
    "\n",
    "**Classification 3 zones** (basée sur d curviligne, total = 2.40) :\n",
    "- Gauche (classe 0) : d < 0.354 (bras gauche pur)\n",
    "- Haut (classe 1) : 0.354 ≤ d < 0.646 (couloir haut + coins)\n",
    "- Droite (classe 2) : d ≥ 0.646 (bras droit pur)\n",
    "\n",
    "**Loss combinée** : `L = L_cls (CrossEntropy) + Σ L_pos_zone (Gaussian NLL, masqué) + λ_d * L_curvilinear (MSE) + λ_feas * L_feasibility`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports et configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Reproductibilité\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Device\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device('mps')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "print(f'Device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Chargement des données ---\n",
    "LOCAL_DIR = os.path.join(os.path.abspath('..'), 'data')\n",
    "\n",
    "PARQUET_NAME = \"M1199_PAG_stride4_win108_test.parquet\"\n",
    "JSON_NAME = \"M1199_PAG.json\"\n",
    "\n",
    "PARQUET_FILE = os.path.join(LOCAL_DIR, PARQUET_NAME)\n",
    "JSON_FILE = os.path.join(LOCAL_DIR, JSON_NAME)\n",
    "\n",
    "if not os.path.exists(PARQUET_FILE):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Données introuvables dans {LOCAL_DIR}/\\n\"\n",
    "        f\"Lancez d'abord: python download_data.py\"\n",
    "    )\n",
    "\n",
    "print(f\"Chargement depuis {LOCAL_DIR}/\")\n",
    "df = pd.read_parquet(PARQUET_FILE)\n",
    "with open(JSON_FILE, \"r\") as f:\n",
    "    params = json.load(f)\n",
    "\n",
    "print(f\"Shape: {df.shape}\")\n",
    "\n",
    "nGroups = params['nGroups']\n",
    "nChannelsPerGroup = [params[f'group{g}']['nChannels'] for g in range(nGroups)]\n",
    "print(f\"nGroups={nGroups}, nChannelsPerGroup={nChannelsPerGroup}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement et filtrage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_masks = np.array([x[0] for x in df['speedMask']])\n",
    "df_moving = df[speed_masks].reset_index(drop=True)\n",
    "print(f'Exemples en mouvement : {len(df_moving)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Géométrie du U-maze, distance curviligne et labels de zone\n",
    "\n",
    "**3 zones** basées sur les segments du squelette :\n",
    "- Gauche (classe 0) : d < 0.354 — bras gauche pur (hors coin)\n",
    "- Haut (classe 1) : 0.354 ≤ d < 0.646 — couloir haut + coins\n",
    "- Droite (classe 2) : d ≥ 0.646 — bras droit pur (hors coin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Définition du squelette du U ---\n",
    "SKELETON_SEGMENTS = np.array([\n",
    "    [0.15, 0.0, 0.15, 0.85],   # Segment 1 : bras gauche (bas → haut)\n",
    "    [0.15, 0.85, 0.85, 0.85],  # Segment 2 : couloir haut (gauche → droite)\n",
    "    [0.85, 0.85, 0.85, 0.0],   # Segment 3 : bras droit (haut → bas)\n",
    "])\n",
    "\n",
    "CORRIDOR_HALF_WIDTH = 0.15\n",
    "\n",
    "SEGMENT_LENGTHS = np.array([\n",
    "    np.sqrt((s[2]-s[0])**2 + (s[3]-s[1])**2) for s in SKELETON_SEGMENTS\n",
    "])\n",
    "TOTAL_LENGTH = SEGMENT_LENGTHS.sum()  # 2.40\n",
    "CUMULATIVE_LENGTHS = np.concatenate([[0], np.cumsum(SEGMENT_LENGTHS)])\n",
    "\n",
    "# Seuils de zone (frontières naturelles entre segments)\n",
    "D_LEFT_END = CUMULATIVE_LENGTHS[1] / TOTAL_LENGTH    # 0.85 / 2.40 ≈ 0.354\n",
    "D_RIGHT_START = CUMULATIVE_LENGTHS[2] / TOTAL_LENGTH  # 1.55 / 2.40 ≈ 0.646\n",
    "\n",
    "N_ZONES = 3\n",
    "ZONE_NAMES = ['Gauche', 'Haut', 'Droite']\n",
    "\n",
    "print(f'Longueurs des segments : {SEGMENT_LENGTHS}')\n",
    "print(f'Longueur totale du U : {TOTAL_LENGTH:.2f}')\n",
    "print(f'\\nSeuils de zone :')\n",
    "print(f'  Gauche (classe 0) : d < {D_LEFT_END:.4f}')\n",
    "print(f'  Haut   (classe 1) : {D_LEFT_END:.4f} <= d < {D_RIGHT_START:.4f}')\n",
    "print(f'  Droite (classe 2) : d >= {D_RIGHT_START:.4f}')\n",
    "\n",
    "\n",
    "def project_point_on_segment(px, py, x1, y1, x2, y2):\n",
    "    \"\"\"Projette un point (px, py) sur le segment [(x1,y1), (x2,y2)].\"\"\"\n",
    "    dx, dy = x2 - x1, y2 - y1\n",
    "    seg_len_sq = dx**2 + dy**2\n",
    "    if seg_len_sq < 1e-12:\n",
    "        return 0.0, np.sqrt((px - x1)**2 + (py - y1)**2), x1, y1\n",
    "    t = ((px - x1) * dx + (py - y1) * dy) / seg_len_sq\n",
    "    t = np.clip(t, 0.0, 1.0)\n",
    "    proj_x = x1 + t * dx\n",
    "    proj_y = y1 + t * dy\n",
    "    dist = np.sqrt((px - proj_x)**2 + (py - proj_y)**2)\n",
    "    return t, dist, proj_x, proj_y\n",
    "\n",
    "\n",
    "def compute_curvilinear_distance(x, y):\n",
    "    \"\"\"Calcule la distance curviligne normalisée d ∈ [0, 1] le long du U.\"\"\"\n",
    "    best_dist = np.inf\n",
    "    best_d = 0.0\n",
    "    for i, (x1, y1, x2, y2) in enumerate(SKELETON_SEGMENTS):\n",
    "        t, dist, _, _ = project_point_on_segment(x, y, x1, y1, x2, y2)\n",
    "        if dist < best_dist:\n",
    "            best_dist = dist\n",
    "            best_d = (CUMULATIVE_LENGTHS[i] + t * SEGMENT_LENGTHS[i]) / TOTAL_LENGTH\n",
    "    return best_d\n",
    "\n",
    "\n",
    "def compute_distance_to_skeleton(x, y):\n",
    "    \"\"\"Distance minimale du point (x, y) au squelette du U.\"\"\"\n",
    "    best_dist = np.inf\n",
    "    for x1, y1, x2, y2 in SKELETON_SEGMENTS:\n",
    "        _, dist, _, _ = project_point_on_segment(x, y, x1, y1, x2, y2)\n",
    "        best_dist = min(best_dist, dist)\n",
    "    return best_dist\n",
    "\n",
    "\n",
    "def d_to_zone(d):\n",
    "    \"\"\"Convertit une distance curviligne d en label de zone (0, 1, 2).\"\"\"\n",
    "    if d < D_LEFT_END:\n",
    "        return 0  # Gauche\n",
    "    elif d < D_RIGHT_START:\n",
    "        return 1  # Haut\n",
    "    else:\n",
    "        return 2  # Droite\n",
    "\n",
    "\n",
    "# --- Calcul de d et zone_labels pour tous les exemples ---\n",
    "positions = np.array([[x[0], x[1]] for x in df_moving['pos']], dtype=np.float32)\n",
    "curvilinear_d = np.array([\n",
    "    compute_curvilinear_distance(x, y) for x, y in positions\n",
    "], dtype=np.float32)\n",
    "\n",
    "zone_labels = np.array([d_to_zone(d) for d in curvilinear_d], dtype=np.int64)\n",
    "\n",
    "print(f'\\nd curviligne : min={curvilinear_d.min():.4f}, max={curvilinear_d.max():.4f}, mean={curvilinear_d.mean():.4f}')\n",
    "print(f'\\nDistribution des zones :')\n",
    "for z in range(N_ZONES):\n",
    "    count = (zone_labels == z).sum()\n",
    "    print(f'  {ZONE_NAMES[z]:8s} (classe {z}) : {count} ({count / len(zone_labels):.1%})')\n",
    "\n",
    "dist_to_skel = np.array([compute_distance_to_skeleton(x, y) for x, y in positions])\n",
    "print(f'\\nDistance au squelette : mean={dist_to_skel.mean():.4f}, max={dist_to_skel.max():.4f}')\n",
    "print(f'  % dans le couloir (dist < {CORRIDOR_HALF_WIDTH}) : {(dist_to_skel < CORRIDOR_HALF_WIDTH).mean():.1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualisation : d + classification 3 zones ---\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 6))\n",
    "\n",
    "# 1. Squelette\n",
    "axes[0].scatter(positions[:, 0], positions[:, 1], c='lightgray', s=1, alpha=0.3)\n",
    "for x1, y1, x2, y2 in SKELETON_SEGMENTS:\n",
    "    axes[0].plot([x1, x2], [y1, y2], 'r-', linewidth=3)\n",
    "axes[0].set_xlabel('X'); axes[0].set_ylabel('Y')\n",
    "axes[0].set_title('Squelette du U sur les positions')\n",
    "axes[0].set_aspect('equal')\n",
    "\n",
    "# 2. d coloré\n",
    "sc = axes[1].scatter(positions[:, 0], positions[:, 1], c=curvilinear_d, s=1, alpha=0.5, cmap='viridis')\n",
    "plt.colorbar(sc, ax=axes[1], label='d (distance curviligne normalisée)')\n",
    "axes[1].set_xlabel('X'); axes[1].set_ylabel('Y')\n",
    "axes[1].set_title('Distance curviligne d le long du U')\n",
    "axes[1].set_aspect('equal')\n",
    "\n",
    "# 3. Classification 3 zones\n",
    "zone_colors = ['blue', 'green', 'red']\n",
    "for z in range(N_ZONES):\n",
    "    mask_z = zone_labels == z\n",
    "    axes[2].scatter(positions[mask_z, 0], positions[mask_z, 1], c=zone_colors[z], s=1, alpha=0.3, label=f'{ZONE_NAMES[z]}')\n",
    "axes[2].set_xlabel('X'); axes[2].set_ylabel('Y')\n",
    "axes[2].set_title(f'Classification 3 zones (d<{D_LEFT_END:.3f} / {D_LEFT_END:.3f}-{D_RIGHT_START:.3f} / >{D_RIGHT_START:.3f})')\n",
    "axes[2].legend(markerscale=10)\n",
    "axes[2].set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Histogramme de d avec seuils\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.hist(curvilinear_d, bins=100, alpha=0.7, edgecolor='black', linewidth=0.3)\n",
    "ax.axvline(x=D_LEFT_END, color='blue', linestyle='--', linewidth=2, label=f'Gauche/Haut ({D_LEFT_END:.3f})')\n",
    "ax.axvline(x=D_RIGHT_START, color='red', linestyle='--', linewidth=2, label=f'Haut/Droite ({D_RIGHT_START:.3f})')\n",
    "ax.set_xlabel('d (distance curviligne)'); ax.set_ylabel('Count')\n",
    "ax.set_title('Distribution de d + seuils de zone')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocessing : reconstruction de la séquence chronologique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_sequence(row, nGroups, nChannelsPerGroup, max_seq_len=128):\n",
    "    \"\"\"Reconstruit la séquence chronologique de spikes.\"\"\"\n",
    "    groups = row['groups']\n",
    "    length = min(len(groups), max_seq_len)\n",
    "    waveforms = {}\n",
    "    for g in range(nGroups):\n",
    "        nCh = nChannelsPerGroup[g]\n",
    "        raw = row[f'group{g}']\n",
    "        waveforms[g] = raw.reshape(-1, nCh, 32)\n",
    "    seq_waveforms = []\n",
    "    seq_shank_ids = []\n",
    "    for t in range(length):\n",
    "        g = int(groups[t])\n",
    "        idx = int(row[f'indices{g}'][t])\n",
    "        if idx > 0 and idx <= waveforms[g].shape[0]:\n",
    "            seq_waveforms.append((waveforms[g][idx - 1], g))\n",
    "            seq_shank_ids.append(g)\n",
    "    return seq_waveforms, seq_shank_ids\n",
    "\n",
    "wf, sids = reconstruct_sequence(df_moving.iloc[0], nGroups, nChannelsPerGroup)\n",
    "print(f'Premier exemple : {len(wf)} spikes, shanks={set(sids)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset PyTorch (avec zone_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 128\n",
    "MAX_CHANNELS = max(nChannelsPerGroup)  # 6\n",
    "\n",
    "class SpikeSequenceDataset(Dataset):\n",
    "    def __init__(self, dataframe, nGroups, nChannelsPerGroup, curvilinear_d, zone_labels, max_seq_len=MAX_SEQ_LEN):\n",
    "        self.df = dataframe\n",
    "        self.nGroups = nGroups\n",
    "        self.nChannelsPerGroup = nChannelsPerGroup\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.targets = np.array([[x[0], x[1]] for x in dataframe['pos']], dtype=np.float32)\n",
    "        self.curvilinear_d = curvilinear_d.astype(np.float32)\n",
    "        self.zone_labels = zone_labels.astype(np.int64)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        seq, shank_ids = reconstruct_sequence(row, self.nGroups, self.nChannelsPerGroup, self.max_seq_len)\n",
    "        seq_len = len(seq)\n",
    "        if seq_len == 0:\n",
    "            seq_len = 1\n",
    "            waveforms = np.zeros((1, MAX_CHANNELS, 32), dtype=np.float32)\n",
    "            shank_ids_arr = np.array([0], dtype=np.int64)\n",
    "        else:\n",
    "            waveforms = np.zeros((seq_len, MAX_CHANNELS, 32), dtype=np.float32)\n",
    "            shank_ids_arr = np.array(shank_ids, dtype=np.int64)\n",
    "            for t, (wf, g) in enumerate(seq):\n",
    "                nCh = wf.shape[0]\n",
    "                waveforms[t, :nCh, :] = wf\n",
    "        return {\n",
    "            'waveforms': torch.from_numpy(waveforms),\n",
    "            'shank_ids': torch.from_numpy(shank_ids_arr),\n",
    "            'seq_len': seq_len,\n",
    "            'target': torch.from_numpy(self.targets[idx]),\n",
    "            'd': torch.tensor(self.curvilinear_d[idx], dtype=torch.float32),\n",
    "            'zone': torch.tensor(self.zone_labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    max_len = max(item['seq_len'] for item in batch)\n",
    "    batch_size = len(batch)\n",
    "    waveforms = torch.zeros(batch_size, max_len, MAX_CHANNELS, 32)\n",
    "    shank_ids = torch.zeros(batch_size, max_len, dtype=torch.long)\n",
    "    mask = torch.ones(batch_size, max_len, dtype=torch.bool)\n",
    "    targets = torch.stack([item['target'] for item in batch])\n",
    "    d_targets = torch.stack([item['d'] for item in batch])\n",
    "    zone_targets = torch.stack([item['zone'] for item in batch])\n",
    "    for i, item in enumerate(batch):\n",
    "        sl = item['seq_len']\n",
    "        waveforms[i, :sl] = item['waveforms']\n",
    "        shank_ids[i, :sl] = item['shank_ids']\n",
    "        mask[i, :sl] = False\n",
    "    return {\n",
    "        'waveforms': waveforms, 'shank_ids': shank_ids, 'mask': mask,\n",
    "        'targets': targets, 'd_targets': d_targets, 'zone_targets': zone_targets\n",
    "    }\n",
    "\n",
    "print('Dataset et collate_fn définis (avec zone_label 3 classes).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feasibility Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeasibilityLoss(nn.Module):\n",
    "    \"\"\"Pénalise les prédictions (x, y) qui tombent hors du couloir du U.\"\"\"\n",
    "    def __init__(self, skeleton_segments, corridor_half_width):\n",
    "        super().__init__()\n",
    "        self.register_buffer('segments', torch.tensor(skeleton_segments, dtype=torch.float32))\n",
    "        self.corridor_half_width = corridor_half_width\n",
    "    \n",
    "    def forward(self, xy_pred):\n",
    "        px, py = xy_pred[:, 0], xy_pred[:, 1]\n",
    "        distances = []\n",
    "        for i in range(self.segments.shape[0]):\n",
    "            x1, y1, x2, y2 = self.segments[i]\n",
    "            dx, dy = x2 - x1, y2 - y1\n",
    "            seg_len_sq = dx**2 + dy**2\n",
    "            t = ((px - x1) * dx + (py - y1) * dy) / (seg_len_sq + 1e-8)\n",
    "            t = t.clamp(0.0, 1.0)\n",
    "            proj_x, proj_y = x1 + t * dx, y1 + t * dy\n",
    "            dist = torch.sqrt((px - proj_x)**2 + (py - proj_y)**2 + 1e-8)\n",
    "            distances.append(dist)\n",
    "        distances = torch.stack(distances, dim=1)\n",
    "        min_dist = distances.min(dim=1).values\n",
    "        return torch.relu(min_dist - self.corridor_half_width).pow(2).mean()\n",
    "\n",
    "feas_loss = FeasibilityLoss(SKELETON_SEGMENTS, CORRIDOR_HALF_WIDTH)\n",
    "print(f'Pénalité (dans le couloir) : {feas_loss(torch.tensor([[0.15, 0.5], [0.85, 0.3]])).item():.6f}')\n",
    "print(f'Pénalité (hors couloir)    : {feas_loss(torch.tensor([[0.5, 0.3], [0.5, 0.5]])).item():.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Architecture du modèle hiérarchique (3 zones)\n",
    "\n",
    "**Backbone partagé** : identique à 02g.\n",
    "\n",
    "**Heads spécialisés** :\n",
    "- `cls_head` : classification 3 classes → logits (3,) → softmax\n",
    "- `mu_heads[0..2]` + `log_sigma_heads[0..2]` : régression par zone\n",
    "- `d_head` : prédiction de d (tâche auxiliaire)\n",
    "\n",
    "**Combinaison à l'inférence** :\n",
    "```\n",
    "p = softmax(cls_logits)  # (batch, 3)\n",
    "mu = Σ p_z * mu_z\n",
    "sigma² = Σ p_z * (sigma_z² + mu_z²) - mu²\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpikeEncoder(nn.Module):\n",
    "    def __init__(self, n_channels, embed_dim):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(n_channels, 32, kernel_size=5, padding=2), nn.ReLU(),\n",
    "            nn.Conv1d(32, embed_dim, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv(x).squeeze(-1)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=256):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "class SpikeTransformerHierarchical(nn.Module):\n",
    "    \"\"\"Transformer hiérarchique : classification 3 zones + régression conditionnelle.\"\"\"\n",
    "    \n",
    "    def __init__(self, nGroups, nChannelsPerGroup, n_zones=3, embed_dim=64, nhead=4,\n",
    "                 num_layers=2, dropout=0.2, spike_dropout=0.15, noise_std=0.5,\n",
    "                 max_channels=MAX_CHANNELS):\n",
    "        super().__init__()\n",
    "        self.nGroups = nGroups\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_zones = n_zones\n",
    "        self.max_channels = max_channels\n",
    "        self.spike_dropout = spike_dropout\n",
    "        self.noise_std = noise_std\n",
    "        \n",
    "        self.spike_encoders = nn.ModuleList([\n",
    "            SpikeEncoder(max_channels, embed_dim) for _ in range(nGroups)\n",
    "        ])\n",
    "        self.shank_embedding = nn.Embedding(nGroups, embed_dim)\n",
    "        self.pos_encoding = PositionalEncoding(embed_dim)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, nhead=nhead, dim_feedforward=embed_dim * 4,\n",
    "            dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer, num_layers=num_layers, enable_nested_tensor=False\n",
    "        )\n",
    "        \n",
    "        # Classification 3 zones\n",
    "        self.cls_head = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim), nn.ReLU(), nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim, n_zones)\n",
    "        )\n",
    "        \n",
    "        # Régression par zone : mu + log_sigma\n",
    "        self.mu_heads = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(embed_dim, embed_dim), nn.ReLU(), nn.Dropout(dropout),\n",
    "                nn.Linear(embed_dim, 2)\n",
    "            ) for _ in range(n_zones)\n",
    "        ])\n",
    "        self.log_sigma_heads = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(embed_dim, embed_dim), nn.ReLU(), nn.Dropout(dropout),\n",
    "                nn.Linear(embed_dim, 2)\n",
    "            ) for _ in range(n_zones)\n",
    "        ])\n",
    "        \n",
    "        # Distance curviligne\n",
    "        self.d_head = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim), nn.ReLU(), nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim, 1), nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def _apply_spike_dropout(self, mask):\n",
    "        if not self.training or self.spike_dropout <= 0:\n",
    "            return mask\n",
    "        drop_mask = torch.rand_like(mask.float()) < self.spike_dropout\n",
    "        active = ~mask\n",
    "        new_drops = drop_mask & active\n",
    "        remaining = active & ~new_drops\n",
    "        all_dropped = remaining.sum(dim=1) == 0\n",
    "        if all_dropped.any():\n",
    "            new_drops[all_dropped] = False\n",
    "        return mask | new_drops\n",
    "    \n",
    "    def _apply_waveform_noise(self, waveforms):\n",
    "        if not self.training or self.noise_std <= 0:\n",
    "            return waveforms\n",
    "        return waveforms + torch.randn_like(waveforms) * self.noise_std\n",
    "    \n",
    "    def _encode(self, waveforms, shank_ids, mask):\n",
    "        \"\"\"Shared backbone : encode → transformer → pooling.\"\"\"\n",
    "        batch_size, seq_len = waveforms.shape[:2]\n",
    "        mask = self._apply_spike_dropout(mask)\n",
    "        waveforms = self._apply_waveform_noise(waveforms)\n",
    "        \n",
    "        embeddings = torch.zeros(batch_size, seq_len, self.embed_dim, device=waveforms.device)\n",
    "        for g in range(self.nGroups):\n",
    "            group_mask = (shank_ids == g) & (~mask)\n",
    "            if group_mask.any():\n",
    "                embeddings[group_mask] = self.spike_encoders[g](waveforms[group_mask])\n",
    "        \n",
    "        embeddings = embeddings + self.shank_embedding(shank_ids)\n",
    "        embeddings = self.pos_encoding(embeddings)\n",
    "        encoded = self.transformer(embeddings, src_key_padding_mask=mask)\n",
    "        \n",
    "        active_mask = (~mask).unsqueeze(-1).float()\n",
    "        pooled = (encoded * active_mask).sum(dim=1) / (active_mask.sum(dim=1) + 1e-8)\n",
    "        return pooled\n",
    "    \n",
    "    def forward(self, waveforms, shank_ids, mask):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            cls_logits: (batch, 3)\n",
    "            mus: list of 3 × (batch, 2)\n",
    "            sigmas: list of 3 × (batch, 2)\n",
    "            d_pred: (batch, 1)\n",
    "        \"\"\"\n",
    "        pooled = self._encode(waveforms, shank_ids, mask)\n",
    "        cls_logits = self.cls_head(pooled)\n",
    "        mus = [head(pooled) for head in self.mu_heads]\n",
    "        sigmas = [torch.exp(head(pooled)) for head in self.log_sigma_heads]\n",
    "        d_pred = self.d_head(pooled)\n",
    "        return cls_logits, mus, sigmas, d_pred\n",
    "    \n",
    "    def predict(self, waveforms, shank_ids, mask):\n",
    "        \"\"\"Prédiction combinée via mélange pondéré par softmax.\"\"\"\n",
    "        cls_logits, mus, sigmas, d_pred = self.forward(waveforms, shank_ids, mask)\n",
    "        probs = torch.softmax(cls_logits, dim=1)  # (batch, 3)\n",
    "        \n",
    "        # Mélange pondéré\n",
    "        mu_stack = torch.stack(mus, dim=1)       # (batch, 3, 2)\n",
    "        sigma_stack = torch.stack(sigmas, dim=1)  # (batch, 3, 2)\n",
    "        p = probs.unsqueeze(-1)                   # (batch, 3, 1)\n",
    "        \n",
    "        mu = (p * mu_stack).sum(dim=1)  # (batch, 2)\n",
    "        \n",
    "        # Variance totale\n",
    "        var_combined = (p * (sigma_stack ** 2 + mu_stack ** 2)).sum(dim=1) - mu ** 2\n",
    "        sigma = torch.sqrt(var_combined.clamp(min=1e-8))\n",
    "        \n",
    "        return mu, sigma, probs, d_pred\n",
    "\n",
    "\n",
    "model = SpikeTransformerHierarchical(nGroups, nChannelsPerGroup)\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Modèle créé : {n_params:,} paramètres')\n",
    "print(f'Têtes : classification 3 zones + 3 régressions conditionnelles + d curviligne')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Split et DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "split_idx = int(len(df_moving) * 0.9)\n",
    "df_train_full = df_moving.iloc[:split_idx].reset_index(drop=True)\n",
    "df_test = df_moving.iloc[split_idx:].reset_index(drop=True)\n",
    "d_train_full = curvilinear_d[:split_idx]\n",
    "d_test = curvilinear_d[split_idx:]\n",
    "zone_train_full = zone_labels[:split_idx]\n",
    "zone_test = zone_labels[split_idx:]\n",
    "\n",
    "print(f'Train : {len(df_train_full)} | Test : {len(df_test)}')\n",
    "for z in range(N_ZONES):\n",
    "    print(f'  {ZONE_NAMES[z]:8s} — train: {(zone_train_full == z).sum()}, test: {(zone_test == z).sum()}')\n",
    "\n",
    "N_FOLDS = 5\n",
    "kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=41)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "test_dataset = SpikeSequenceDataset(df_test, nGroups, nChannelsPerGroup, d_test, zone_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=0)\n",
    "print(f'Test: {len(test_dataset)} exemples, {len(test_loader)} batches')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Entraînement (5-Fold Cross-Validation)\n",
    "\n",
    "**Loss combinée** :\n",
    "```\n",
    "L = L_cls (CrossEntropy 3 classes)\n",
    "  + Σ_z L_pos_z (Gaussian NLL, masqué sur zone z)\n",
    "  + λ_d * L_curvilinear (MSE)\n",
    "  + λ_feas * L_feasibility\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 64\n",
    "NHEAD = 4\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.2\n",
    "SPIKE_DROPOUT = 0.15\n",
    "NOISE_STD = 0.5\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "EPOCHS = 30\n",
    "PATIENCE = 7\n",
    "LAMBDA_D = 1.0\n",
    "LAMBDA_FEAS = 10.0\n",
    "\n",
    "print(f'Loss : CrossEntropy(cls) + GaussianNLL(par zone) + {LAMBDA_D}*MSE(d) + {LAMBDA_FEAS}*Feasibility')\n",
    "print(f'Device : {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, scheduler, criterion_ce, criterion_nll, criterion_d, feas_loss, device):\n",
    "    model.train()\n",
    "    totals = {'loss': 0, 'cls': 0, 'pos': 0, 'd': 0, 'feas': 0, 'correct': 0, 'n': 0, 'batches': 0}\n",
    "    \n",
    "    for batch in loader:\n",
    "        wf = batch['waveforms'].to(device)\n",
    "        sid = batch['shank_ids'].to(device)\n",
    "        mask = batch['mask'].to(device)\n",
    "        targets = batch['targets'].to(device)\n",
    "        d_targets = batch['d_targets'].to(device)\n",
    "        zone_targets = batch['zone_targets'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        cls_logits, mus, sigmas, d_pred = model(wf, sid, mask)\n",
    "        \n",
    "        # Classification (CrossEntropy)\n",
    "        loss_cls = criterion_ce(cls_logits, zone_targets)\n",
    "        \n",
    "        # Régression par zone (masquée)\n",
    "        loss_pos = torch.tensor(0.0, device=device)\n",
    "        for z in range(N_ZONES):\n",
    "            zmask = (zone_targets == z)\n",
    "            if zmask.any():\n",
    "                loss_pos = loss_pos + criterion_nll(\n",
    "                    mus[z][zmask], targets[zmask], sigmas[z][zmask] ** 2\n",
    "                )\n",
    "        \n",
    "        # Curviligne + Feasibility\n",
    "        loss_d = criterion_d(d_pred.squeeze(-1), d_targets)\n",
    "        \n",
    "        probs = torch.softmax(cls_logits, dim=1).unsqueeze(-1)  # (batch, 3, 1)\n",
    "        mu_stack = torch.stack(mus, dim=1)  # (batch, 3, 2)\n",
    "        mu_combined = (probs * mu_stack).sum(dim=1)  # (batch, 2)\n",
    "        loss_feas = feas_loss(mu_combined)\n",
    "        \n",
    "        loss = loss_cls + loss_pos + LAMBDA_D * loss_d + LAMBDA_FEAS * loss_feas\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        totals['loss'] += loss.item()\n",
    "        totals['cls'] += loss_cls.item()\n",
    "        totals['pos'] += loss_pos.item()\n",
    "        totals['d'] += loss_d.item()\n",
    "        totals['feas'] += loss_feas.item()\n",
    "        with torch.no_grad():\n",
    "            totals['correct'] += (cls_logits.argmax(dim=1) == zone_targets).sum().item()\n",
    "            totals['n'] += len(zone_targets)\n",
    "        totals['batches'] += 1\n",
    "    \n",
    "    nb = totals['batches']\n",
    "    return {k: totals[k] / nb for k in ['loss', 'cls', 'pos', 'd', 'feas']}, totals['correct'] / totals['n']\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(model, loader, criterion_ce, criterion_nll, criterion_d, feas_loss, device):\n",
    "    model.eval()\n",
    "    totals = {'loss': 0, 'cls': 0, 'pos': 0, 'd': 0, 'feas': 0, 'correct': 0, 'n': 0, 'batches': 0}\n",
    "    all_mu, all_sigma, all_probs, all_d, all_targets, all_d_targets, all_zone_targets = [], [], [], [], [], [], []\n",
    "    \n",
    "    for batch in loader:\n",
    "        wf = batch['waveforms'].to(device)\n",
    "        sid = batch['shank_ids'].to(device)\n",
    "        mask = batch['mask'].to(device)\n",
    "        targets = batch['targets'].to(device)\n",
    "        d_targets = batch['d_targets'].to(device)\n",
    "        zone_targets = batch['zone_targets'].to(device)\n",
    "        \n",
    "        mu, sigma, probs, d_pred = model.predict(wf, sid, mask)\n",
    "        cls_logits, mus, sigmas, _ = model(wf, sid, mask)\n",
    "        \n",
    "        loss_cls = criterion_ce(cls_logits, zone_targets)\n",
    "        loss_pos = torch.tensor(0.0, device=device)\n",
    "        for z in range(N_ZONES):\n",
    "            zmask = (zone_targets == z)\n",
    "            if zmask.any():\n",
    "                loss_pos = loss_pos + criterion_nll(mus[z][zmask], targets[zmask], sigmas[z][zmask] ** 2)\n",
    "        loss_d = criterion_d(d_pred.squeeze(-1), d_targets)\n",
    "        loss_feas = feas_loss(mu)\n",
    "        loss = loss_cls + loss_pos + LAMBDA_D * loss_d + LAMBDA_FEAS * loss_feas\n",
    "        \n",
    "        totals['loss'] += loss.item(); totals['cls'] += loss_cls.item()\n",
    "        totals['pos'] += loss_pos.item(); totals['d'] += loss_d.item()\n",
    "        totals['feas'] += loss_feas.item()\n",
    "        totals['correct'] += (cls_logits.argmax(dim=1) == zone_targets).sum().item()\n",
    "        totals['n'] += len(zone_targets); totals['batches'] += 1\n",
    "        \n",
    "        all_mu.append(mu.cpu().numpy()); all_sigma.append(sigma.cpu().numpy())\n",
    "        all_probs.append(probs.cpu().numpy()); all_d.append(d_pred.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy()); all_d_targets.append(d_targets.cpu().numpy())\n",
    "        all_zone_targets.append(zone_targets.cpu().numpy())\n",
    "    \n",
    "    nb = totals['batches']\n",
    "    losses = {k: totals[k] / nb for k in ['loss', 'cls', 'pos', 'd', 'feas']}\n",
    "    acc = totals['correct'] / totals['n']\n",
    "    arrays = [np.concatenate(a) for a in [all_mu, all_sigma, all_probs, all_d, all_targets, all_d_targets, all_zone_targets]]\n",
    "    return losses, acc, arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_results = []\n",
    "all_train_losses = {}\n",
    "all_val_losses = {}\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(df_train_full)):\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'FOLD {fold+1}/{N_FOLDS}')\n",
    "    print(f'{\"=\"*60}')\n",
    "    \n",
    "    df_ft = df_train_full.iloc[train_idx].reset_index(drop=True)\n",
    "    df_fv = df_train_full.iloc[val_idx].reset_index(drop=True)\n",
    "    \n",
    "    ds_t = SpikeSequenceDataset(df_ft, nGroups, nChannelsPerGroup, d_train_full[train_idx], zone_train_full[train_idx])\n",
    "    ds_v = SpikeSequenceDataset(df_fv, nGroups, nChannelsPerGroup, d_train_full[val_idx], zone_train_full[val_idx])\n",
    "    dl_t = DataLoader(ds_t, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=0)\n",
    "    dl_v = DataLoader(ds_v, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=0)\n",
    "    \n",
    "    print(f'  Train: {len(ds_t)}, Val: {len(ds_v)}')\n",
    "    \n",
    "    model = SpikeTransformerHierarchical(\n",
    "        nGroups, nChannelsPerGroup, n_zones=N_ZONES,\n",
    "        embed_dim=EMBED_DIM, nhead=NHEAD, num_layers=NUM_LAYERS,\n",
    "        dropout=DROPOUT, spike_dropout=SPIKE_DROPOUT, noise_std=NOISE_STD\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=LR, epochs=EPOCHS, steps_per_epoch=len(dl_t))\n",
    "    criterion_ce = nn.CrossEntropyLoss()\n",
    "    criterion_nll = nn.GaussianNLLLoss()\n",
    "    criterion_d = nn.MSELoss()\n",
    "    feas_loss_fn = FeasibilityLoss(SKELETON_SEGMENTS, CORRIDOR_HALF_WIDTH).to(DEVICE)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    train_losses, val_losses = [], []\n",
    "    model_path = f'../outputs/best_transformer_02h_fold{fold+1}.pt'\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        t_losses, t_acc = train_epoch(model, dl_t, optimizer, scheduler, criterion_ce, criterion_nll, criterion_d, feas_loss_fn, DEVICE)\n",
    "        v_losses, v_acc, _ = eval_epoch(model, dl_v, criterion_ce, criterion_nll, criterion_d, feas_loss_fn, DEVICE)\n",
    "        \n",
    "        train_losses.append(t_losses['loss'])\n",
    "        val_losses.append(v_losses['loss'])\n",
    "        \n",
    "        if epoch % 5 == 0 or epoch == EPOCHS - 1:\n",
    "            print(f'  Epoch {epoch+1:02d}/{EPOCHS} | Train: {t_losses[\"loss\"]:.4f} (cls={t_losses[\"cls\"]:.4f} pos={t_losses[\"pos\"]:.4f} d={t_losses[\"d\"]:.5f} feas={t_losses[\"feas\"]:.6f} acc={t_acc:.1%}) | Val: {v_losses[\"loss\"]:.4f} (acc={v_acc:.1%})')\n",
    "        \n",
    "        if v_losses['loss'] < best_val_loss:\n",
    "            best_val_loss = v_losses['loss']\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f'  Early stopping a epoch {epoch+1}')\n",
    "                break\n",
    "    \n",
    "    all_train_losses[fold] = train_losses\n",
    "    all_val_losses[fold] = val_losses\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_path, map_location=DEVICE, weights_only=True))\n",
    "    _, val_acc, (val_mu, val_sigma, val_probs, val_d_pred, val_targets, val_d_targets, val_zone_targets) = eval_epoch(\n",
    "        model, dl_v, criterion_ce, criterion_nll, criterion_d, feas_loss_fn, DEVICE\n",
    "    )\n",
    "    val_eucl = np.sqrt(((val_targets - val_mu) ** 2).sum(axis=1))\n",
    "    val_d_mae = np.abs(val_d_targets - val_d_pred.squeeze()).mean()\n",
    "    \n",
    "    val_dist_to_skel = np.array([compute_distance_to_skeleton(val_mu[i, 0], val_mu[i, 1]) for i in range(len(val_mu))])\n",
    "    pct_outside = (val_dist_to_skel > CORRIDOR_HALF_WIDTH).mean()\n",
    "    \n",
    "    fold_results.append({\n",
    "        'fold': fold + 1, 'best_val_loss': best_val_loss,\n",
    "        'val_eucl_mean': val_eucl.mean(),\n",
    "        'val_r2_x': r2_score(val_targets[:, 0], val_mu[:, 0]),\n",
    "        'val_r2_y': r2_score(val_targets[:, 1], val_mu[:, 1]),\n",
    "        'val_d_mae': val_d_mae, 'val_cls_acc': val_acc, 'val_pct_outside': pct_outside,\n",
    "    })\n",
    "    print(f'  => Eucl={val_eucl.mean():.4f} | R2: X={fold_results[-1][\"val_r2_x\"]:.4f} Y={fold_results[-1][\"val_r2_y\"]:.4f} | cls_acc={val_acc:.1%} | hors={pct_outside:.1%}')\n",
    "\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print(f'RESUME ({N_FOLDS} folds)')\n",
    "print(f'{\"=\"*60}')\n",
    "for r in fold_results:\n",
    "    print(f'  Fold {r[\"fold\"]}: Eucl={r[\"val_eucl_mean\"]:.4f} | R2_X={r[\"val_r2_x\"]:.4f} R2_Y={r[\"val_r2_y\"]:.4f} | d_MAE={r[\"val_d_mae\"]:.4f} | cls={r[\"val_cls_acc\"]:.1%} | hors={r[\"val_pct_outside\"]:.1%}')\n",
    "print(f'\\n  Moyenne : Eucl={np.mean([r[\"val_eucl_mean\"] for r in fold_results]):.4f} | cls={np.mean([r[\"val_cls_acc\"] for r in fold_results]):.1%} | hors={np.mean([r[\"val_pct_outside\"] for r in fold_results]):.1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, N_FOLDS))\n",
    "for fold in range(N_FOLDS):\n",
    "    axes[0].plot(all_train_losses[fold], color=colors[fold], linewidth=1.5, label=f'Fold {fold+1}')\n",
    "    axes[1].plot(all_val_losses[fold], color=colors[fold], linewidth=1.5, label=f'Fold {fold+1}')\n",
    "axes[0].set_xlabel('Epoch'); axes[0].set_ylabel('Loss'); axes[0].set_title('Train'); axes[0].legend(); axes[0].grid(True, alpha=0.3)\n",
    "axes[1].set_xlabel('Epoch'); axes[1].set_ylabel('Loss'); axes[1].set_title('Validation'); axes[1].legend(); axes[1].grid(True, alpha=0.3)\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Évaluation finale sur le test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_ce = nn.CrossEntropyLoss()\n",
    "criterion_nll = nn.GaussianNLLLoss()\n",
    "criterion_d = nn.MSELoss()\n",
    "feas_loss_fn = FeasibilityLoss(SKELETON_SEGMENTS, CORRIDOR_HALF_WIDTH).to(DEVICE)\n",
    "\n",
    "all_fold_mu, all_fold_sigma, all_fold_probs, all_fold_d = [], [], [], []\n",
    "\n",
    "for fold in range(N_FOLDS):\n",
    "    model = SpikeTransformerHierarchical(\n",
    "        nGroups, nChannelsPerGroup, n_zones=N_ZONES,\n",
    "        embed_dim=EMBED_DIM, nhead=NHEAD, num_layers=NUM_LAYERS,\n",
    "        dropout=DROPOUT, spike_dropout=SPIKE_DROPOUT, noise_std=NOISE_STD\n",
    "    ).to(DEVICE)\n",
    "    model.load_state_dict(torch.load(f'../outputs/best_transformer_02h_fold{fold+1}.pt', map_location=DEVICE, weights_only=True))\n",
    "    \n",
    "    _, fold_acc, (fold_mu, fold_sigma, fold_probs, fold_d, y_test, d_test_targets, zone_test_targets) = eval_epoch(\n",
    "        model, test_loader, criterion_ce, criterion_nll, criterion_d, feas_loss_fn, DEVICE\n",
    "    )\n",
    "    all_fold_mu.append(fold_mu); all_fold_sigma.append(fold_sigma)\n",
    "    all_fold_probs.append(fold_probs); all_fold_d.append(fold_d)\n",
    "    fold_eucl = np.sqrt(((y_test - fold_mu) ** 2).sum(axis=1))\n",
    "    print(f'Fold {fold+1}: Eucl={fold_eucl.mean():.4f}, cls_acc={fold_acc:.1%}')\n",
    "\n",
    "# Ensemble\n",
    "all_fold_mu = np.stack(all_fold_mu)\n",
    "all_fold_sigma = np.stack(all_fold_sigma)\n",
    "all_fold_probs = np.stack(all_fold_probs)\n",
    "all_fold_d = np.stack(all_fold_d)\n",
    "\n",
    "y_pred = all_fold_mu.mean(axis=0)\n",
    "d_pred_ensemble = all_fold_d.mean(axis=0).squeeze()\n",
    "probs_ensemble = all_fold_probs.mean(axis=0)  # (n_test, 3)\n",
    "zone_pred = probs_ensemble.argmax(axis=1)\n",
    "\n",
    "mean_var = (all_fold_sigma ** 2).mean(axis=0)\n",
    "var_mu = all_fold_mu.var(axis=0)\n",
    "y_sigma = np.sqrt(mean_var + var_mu)\n",
    "\n",
    "# Métriques\n",
    "r2_x = r2_score(y_test[:, 0], y_pred[:, 0])\n",
    "r2_y = r2_score(y_test[:, 1], y_pred[:, 1])\n",
    "eucl_errors = np.sqrt(((y_test - y_pred) ** 2).sum(axis=1))\n",
    "d_mae = np.abs(d_test_targets - d_pred_ensemble).mean()\n",
    "d_r2 = r2_score(d_test_targets, d_pred_ensemble)\n",
    "cls_accuracy = (zone_pred == zone_test_targets).mean()\n",
    "\n",
    "test_dist_to_skel = np.array([compute_distance_to_skeleton(y_pred[i, 0], y_pred[i, 1]) for i in range(len(y_pred))])\n",
    "pct_outside = (test_dist_to_skel > CORRIDOR_HALF_WIDTH).mean()\n",
    "\n",
    "# Confusion par zone\n",
    "zone_confusion = zone_pred != zone_test_targets\n",
    "zone_confusion_rate = zone_confusion.mean()\n",
    "\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print(f'Transformer 02h : Hiérarchique 3 zones — Ensemble ({N_FOLDS} folds)')\n",
    "print(f'{\"=\"*60}')\n",
    "print(f'  R²   : X={r2_x:.4f}, Y={r2_y:.4f}')\n",
    "print(f'  Eucl : mean={eucl_errors.mean():.4f}, median={np.median(eucl_errors):.4f}, p90={np.percentile(eucl_errors, 90):.4f}')\n",
    "print(f'\\n  d curviligne : MAE={d_mae:.4f}, R²={d_r2:.4f}')\n",
    "print(f'  Classification zone : accuracy={cls_accuracy:.1%}')\n",
    "print(f'  Hors labyrinthe : {pct_outside:.1%}')\n",
    "print(f'  Confusion de zone : {zone_confusion_rate:.1%}')\n",
    "\n",
    "print(f'\\n  Erreur par zone :')\n",
    "for z in range(N_ZONES):\n",
    "    zmask = zone_test_targets == z\n",
    "    if zmask.any():\n",
    "        z_acc = (zone_pred[zmask] == z).mean()\n",
    "        print(f'    {ZONE_NAMES[z]:8s} : Eucl={eucl_errors[zmask].mean():.4f} | cls_acc={z_acc:.1%} ({zmask.sum()} pts)')\n",
    "\n",
    "print(f'\\n  Sigma moyen : X={y_sigma[:, 0].mean():.4f}, Y={y_sigma[:, 1].mean():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "axes[0].scatter(y_test[:, 0], y_pred[:, 0], s=1, alpha=0.3)\n",
    "axes[0].plot([0, 1], [0, 1], 'r--'); axes[0].set_title(f'X (R²={r2_x:.3f})'); axes[0].set_aspect('equal')\n",
    "axes[1].scatter(y_test[:, 1], y_pred[:, 1], s=1, alpha=0.3)\n",
    "axes[1].plot([0, 1], [0, 1], 'r--'); axes[1].set_title(f'Y (R²={r2_y:.3f})'); axes[1].set_aspect('equal')\n",
    "axes[2].scatter(d_test_targets, d_pred_ensemble, s=1, alpha=0.3)\n",
    "axes[2].plot([0, 1], [0, 1], 'r--'); axes[2].set_title(f'd (R²={d_r2:.3f})'); axes[2].set_aspect('equal')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment = slice(0, 500)\n",
    "seg_idx = np.arange(500)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Trajectoire 2D\n",
    "axes[0, 0].plot(y_test[segment, 0], y_test[segment, 1], 'b-', alpha=0.5, label='Vrai', linewidth=1)\n",
    "axes[0, 0].plot(y_pred[segment, 0], y_pred[segment, 1], 'r-', alpha=0.5, label='Pred', linewidth=1)\n",
    "for x1, y1, x2, y2 in SKELETON_SEGMENTS:\n",
    "    axes[0, 0].plot([x1, x2], [y1, y2], 'k--', linewidth=1, alpha=0.3)\n",
    "axes[0, 0].set_title('Trajectoire (500 pts)'); axes[0, 0].legend(); axes[0, 0].set_aspect('equal')\n",
    "\n",
    "# 2. Probabilités de zone\n",
    "for z in range(N_ZONES):\n",
    "    axes[0, 1].plot(seg_idx, probs_ensemble[segment, z], label=ZONE_NAMES[z], linewidth=1)\n",
    "axes[0, 1].plot(seg_idx, zone_test_targets[segment], 'k--', alpha=0.3, label='Vrai (0/1/2)')\n",
    "axes[0, 1].set_title('P(zone) vs temps'); axes[0, 1].legend(); axes[0, 1].set_ylim(-0.05, 1.05)\n",
    "\n",
    "# 3. Distance curviligne\n",
    "axes[1, 0].plot(seg_idx, d_test_targets[segment], 'b-', label='Vrai d', linewidth=1.5)\n",
    "axes[1, 0].plot(seg_idx, d_pred_ensemble[segment], 'r-', alpha=0.7, label='Pred d', linewidth=1)\n",
    "axes[1, 0].axhline(y=D_LEFT_END, color='blue', linestyle=':', alpha=0.5)\n",
    "axes[1, 0].axhline(y=D_RIGHT_START, color='red', linestyle=':', alpha=0.5)\n",
    "axes[1, 0].set_title('d curviligne'); axes[1, 0].legend()\n",
    "\n",
    "# 4. Carte confusion\n",
    "correct = ~zone_confusion\n",
    "axes[1, 1].scatter(y_test[correct, 0], y_test[correct, 1], c='green', s=1, alpha=0.2, label=f'Correct ({correct.mean():.1%})')\n",
    "if zone_confusion.any():\n",
    "    axes[1, 1].scatter(y_test[zone_confusion, 0], y_test[zone_confusion, 1], c='red', s=5, alpha=0.8, label=f'Erreur ({zone_confusion_rate:.1%})')\n",
    "for x1, y1, x2, y2 in SKELETON_SEGMENTS:\n",
    "    axes[1, 1].plot([x1, x2], [y1, y2], 'k--', linewidth=1, alpha=0.3)\n",
    "axes[1, 1].set_title('Confusion de zone'); axes[1, 1].legend(markerscale=5); axes[1, 1].set_aspect('equal')\n",
    "\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmaps\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 7))\n",
    "nbins = 20\n",
    "x_edges = np.linspace(0, 1, nbins + 1)\n",
    "y_edges = np.linspace(0, 1, nbins + 1)\n",
    "\n",
    "for ax_idx, (title, values, cmap) in enumerate([\n",
    "    ('Erreur euclidienne', eucl_errors, 'RdYlGn_r'),\n",
    "    ('Sigma moyen', (y_sigma[:, 0] + y_sigma[:, 1]) / 2, 'RdYlGn_r'),\n",
    "    ('Zone prédite (argmax)', zone_pred.astype(float), 'RdYlBu')\n",
    "]):\n",
    "    val_map = np.full((nbins, nbins), np.nan)\n",
    "    count_map = np.zeros((nbins, nbins))\n",
    "    for i in range(len(y_test)):\n",
    "        xi = np.clip(np.searchsorted(x_edges, y_test[i, 0]) - 1, 0, nbins - 1)\n",
    "        yi = np.clip(np.searchsorted(y_edges, y_test[i, 1]) - 1, 0, nbins - 1)\n",
    "        if np.isnan(val_map[yi, xi]): val_map[yi, xi] = 0\n",
    "        val_map[yi, xi] += values[i]; count_map[yi, xi] += 1\n",
    "    mean_map = np.where(count_map > 0, val_map / count_map, np.nan)\n",
    "    im = axes[ax_idx].imshow(mean_map, origin='lower', aspect='equal', cmap=cmap, extent=[0, 1, 0, 1])\n",
    "    axes[ax_idx].set_title(title); plt.colorbar(im, ax=axes[ax_idx])\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration\n",
    "sigma_mean = (y_sigma[:, 0] + y_sigma[:, 1]) / 2\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(sigma_mean, eucl_errors, s=1, alpha=0.3)\n",
    "sigma_range = np.linspace(0, sigma_mean.max(), 100)\n",
    "ax.plot(sigma_range, 2 * sigma_range, 'r--', label='y = 2*sigma')\n",
    "ax.set_xlabel('Sigma moyen'); ax.set_ylabel('Erreur euclidienne')\n",
    "ax.set_title('Calibration'); ax.legend()\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "for k, expected in [(1, '~39%'), (2, '~86%'), (3, '~99%')]:\n",
    "    pct = np.mean(eucl_errors < k * sigma_mean)\n",
    "    print(f'  Erreur < {k}*sigma : {pct:.1%} (attendu {expected})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Sauvegarde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../outputs/preds_transformer_02h.npy', y_pred)\n",
    "np.save('../outputs/sigma_transformer_02h.npy', y_sigma)\n",
    "np.save('../outputs/d_pred_transformer_02h.npy', d_pred_ensemble)\n",
    "np.save('../outputs/y_test_transformer_02h.npy', y_test)\n",
    "np.save('../outputs/d_test_transformer_02h.npy', d_test_targets)\n",
    "print(f'Sauvegardé : {y_pred.shape}, {y_sigma.shape}, {d_pred_ensemble.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Interprétation\n",
    "\n",
    "### 3 zones vs 2 bras\n",
    "\n",
    "Le passage de 2 classes (gauche/droite, seuil d=0.5) à 3 zones (gauche / haut / droite) a plusieurs avantages :\n",
    "\n",
    "1. **Coins traités séparément** : dans la version 2 classes, les coins du U (jonction bras-couloir) étaient assignés arbitrairement à gauche ou droite. C'est une zone de transition difficile où le signal est ambigu. Avec 3 zones, le couloir haut + coins forment une classe propre.\n",
    "\n",
    "2. **Meilleure classification** : la tâche est plus facile car chaque zone a une géométrie bien distincte (bras vertical gauche / couloir horizontal / bras vertical droit).\n",
    "\n",
    "3. **Head spécialisé pour le couloir** : le head du couloir haut peut apprendre que y ≈ 0.85 et x varie de 0.15 à 0.85, sans être perturbé par les bras verticaux.\n",
    "\n",
    "### Seuils naturels\n",
    "\n",
    "Les seuils d=0.354 et d=0.646 ne sont pas arbitraires : ils correspondent exactement aux jonctions entre les segments du squelette du U. C'est la frontière géométrique naturelle entre les zones.\n",
    "\n",
    "### Mélange 3 composantes\n",
    "\n",
    "La prédiction finale est un mélange de 3 gaussiennes pondérées par softmax. Par rapport au mélange 2 composantes :\n",
    "- Dans les bras purs, une seule composante domine (p ≈ 1)\n",
    "- Dans le couloir, la composante \"haut\" domine\n",
    "- Aux transitions, l'incertitude augmente naturellement via la variance du mélange"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
