{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 02d_bis : Transformer avec Spike Dropout + Gaussian Noise + Feasibility Loss\n",
    "\n",
    "## Modifications par rapport au notebook 02d\n",
    "\n",
    "1. **Pénalisation hors labyrinthe** : un terme de loss supplémentaire pénalise les prédictions (x, y) qui tombent en dehors du couloir du U. La distance au squelette du labyrinthe est calculée de manière différentiable.\n",
    "\n",
    "**Tout le reste est identique** au notebook 02d : même architecture, même loss principale (Gaussian NLL), même data augmentation (spike dropout 15%, gaussian noise std=0.5), même split.\n",
    "\n",
    "**Géométrie du U** (coordonnées normalisées [0, 1]) :\n",
    "- Bras gauche : x ∈ [0, 0.3], y ∈ [0, 1]\n",
    "- Bras droit : x ∈ [0.7, 1.0], y ∈ [0, 1]\n",
    "- Couloir haut : x ∈ [0, 1], y ∈ [0.7, 1.0]\n",
    "- Largeur du couloir : 0.3\n",
    "\n",
    "**Squelette central** : 3 segments formant un U :\n",
    "1. (0.15, 0) → (0.15, 0.85) — bras gauche\n",
    "2. (0.15, 0.85) → (0.85, 0.85) — couloir haut\n",
    "3. (0.85, 0.85) → (0.85, 0) — bras droit\n",
    "\n",
    "**Loss combinée** : `L = L_position (Gaussian NLL) + λ_feas * L_feasibility`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports et configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Reproductibilité\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Device\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device('mps')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "print(f'Device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Chargement des données ---\n",
    "LOCAL_DIR = os.path.join(os.path.abspath('..'), 'data')\n",
    "\n",
    "PARQUET_NAME = \"M1199_PAG_stride4_win108_test.parquet\"\n",
    "JSON_NAME = \"M1199_PAG.json\"\n",
    "\n",
    "PARQUET_FILE = os.path.join(LOCAL_DIR, PARQUET_NAME)\n",
    "JSON_FILE = os.path.join(LOCAL_DIR, JSON_NAME)\n",
    "\n",
    "if not os.path.exists(PARQUET_FILE):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Données introuvables dans {LOCAL_DIR}/\\n\"\n",
    "        f\"Lancez d'abord: python download_data.py\"\n",
    "    )\n",
    "\n",
    "print(f\"Chargement depuis {LOCAL_DIR}/\")\n",
    "df = pd.read_parquet(PARQUET_FILE)\n",
    "with open(JSON_FILE, \"r\") as f:\n",
    "    params = json.load(f)\n",
    "\n",
    "print(f\"Shape: {df.shape}\")\n",
    "\n",
    "nGroups = params['nGroups']\n",
    "nChannelsPerGroup = [params[f'group{g}']['nChannels'] for g in range(nGroups)]\n",
    "print(f\"nGroups={nGroups}, nChannelsPerGroup={nChannelsPerGroup}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement et filtrage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrage speedMask (on ne garde que les exemples en mouvement)\n",
    "speed_masks = np.array([x[0] for x in df['speedMask']])\n",
    "df_moving = df[speed_masks].reset_index(drop=True)\n",
    "print(f'Exemples en mouvement : {len(df_moving)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Géométrie du U-maze (pour la feasibility loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Définition du squelette du U ---\n",
    "SKELETON_SEGMENTS = np.array([\n",
    "    [0.15, 0.0, 0.15, 0.85],   # Segment 1 : bras gauche (bas → haut)\n",
    "    [0.15, 0.85, 0.85, 0.85],  # Segment 2 : couloir haut (gauche → droite)\n",
    "    [0.85, 0.85, 0.85, 0.0],   # Segment 3 : bras droit (haut → bas)\n",
    "])\n",
    "\n",
    "CORRIDOR_HALF_WIDTH = 0.15\n",
    "\n",
    "def compute_distance_to_skeleton(x, y):\n",
    "    \"\"\"Distance minimale du point (x, y) au squelette du U.\"\"\"\n",
    "    best_dist = np.inf\n",
    "    for x1, y1, x2, y2 in SKELETON_SEGMENTS:\n",
    "        dx, dy = x2 - x1, y2 - y1\n",
    "        seg_len_sq = dx**2 + dy**2\n",
    "        if seg_len_sq < 1e-12:\n",
    "            dist = np.sqrt((x - x1)**2 + (y - y1)**2)\n",
    "        else:\n",
    "            t = np.clip(((x - x1) * dx + (y - y1) * dy) / seg_len_sq, 0.0, 1.0)\n",
    "            proj_x = x1 + t * dx\n",
    "            proj_y = y1 + t * dy\n",
    "            dist = np.sqrt((x - proj_x)**2 + (y - proj_y)**2)\n",
    "        best_dist = min(best_dist, dist)\n",
    "    return best_dist\n",
    "\n",
    "# Vérification\n",
    "positions = np.array([[x[0], x[1]] for x in df_moving['pos']], dtype=np.float32)\n",
    "dist_to_skel = np.array([compute_distance_to_skeleton(x, y) for x, y in positions])\n",
    "print(f'Distance au squelette : mean={dist_to_skel.mean():.4f}, max={dist_to_skel.max():.4f}')\n",
    "print(f'  % dans le couloir (dist < {CORRIDOR_HALF_WIDTH}) : {(dist_to_skel < CORRIDOR_HALF_WIDTH).mean():.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocessing : reconstruction de la séquence chronologique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_sequence(row, nGroups, nChannelsPerGroup, max_seq_len=128):\n",
    "    \"\"\"Reconstruit la séquence chronologique de spikes.\"\"\"\n",
    "    groups = row['groups']\n",
    "    length = min(len(groups), max_seq_len)\n",
    "    \n",
    "    waveforms = {}\n",
    "    for g in range(nGroups):\n",
    "        nCh = nChannelsPerGroup[g]\n",
    "        raw = row[f'group{g}']\n",
    "        waveforms[g] = raw.reshape(-1, nCh, 32)\n",
    "    \n",
    "    seq_waveforms = []\n",
    "    seq_shank_ids = []\n",
    "    \n",
    "    for t in range(length):\n",
    "        g = int(groups[t])\n",
    "        idx = int(row[f'indices{g}'][t])\n",
    "        if idx > 0 and idx <= waveforms[g].shape[0]:\n",
    "            seq_waveforms.append((waveforms[g][idx - 1], g))\n",
    "            seq_shank_ids.append(g)\n",
    "    \n",
    "    return seq_waveforms, seq_shank_ids\n",
    "\n",
    "# Test rapide\n",
    "wf, sids = reconstruct_sequence(df_moving.iloc[0], nGroups, nChannelsPerGroup)\n",
    "print(f'Premier exemple : {len(wf)} spikes réels dans la séquence')\n",
    "print(f'Shanks utilisés : {set(sids)}')\n",
    "print(f'Premier spike : shank={wf[0][1]}, shape={wf[0][0].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 128\n",
    "MAX_CHANNELS = max(nChannelsPerGroup)  # 6\n",
    "\n",
    "class SpikeSequenceDataset(Dataset):\n",
    "    def __init__(self, dataframe, nGroups, nChannelsPerGroup, max_seq_len=MAX_SEQ_LEN):\n",
    "        self.df = dataframe\n",
    "        self.nGroups = nGroups\n",
    "        self.nChannelsPerGroup = nChannelsPerGroup\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.targets = np.array([[x[0], x[1]] for x in dataframe['pos']], dtype=np.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        seq, shank_ids = reconstruct_sequence(row, self.nGroups, self.nChannelsPerGroup, self.max_seq_len)\n",
    "        \n",
    "        seq_len = len(seq)\n",
    "        if seq_len == 0:\n",
    "            seq_len = 1\n",
    "            waveforms = np.zeros((1, MAX_CHANNELS, 32), dtype=np.float32)\n",
    "            shank_ids_arr = np.array([0], dtype=np.int64)\n",
    "        else:\n",
    "            waveforms = np.zeros((seq_len, MAX_CHANNELS, 32), dtype=np.float32)\n",
    "            shank_ids_arr = np.array(shank_ids, dtype=np.int64)\n",
    "            for t, (wf, g) in enumerate(seq):\n",
    "                nCh = wf.shape[0]\n",
    "                waveforms[t, :nCh, :] = wf\n",
    "        \n",
    "        target = self.targets[idx]\n",
    "        return {\n",
    "            'waveforms': torch.from_numpy(waveforms),\n",
    "            'shank_ids': torch.from_numpy(shank_ids_arr),\n",
    "            'seq_len': seq_len,\n",
    "            'target': torch.from_numpy(target)\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Collate avec padding dynamique.\"\"\"\n",
    "    max_len = max(item['seq_len'] for item in batch)\n",
    "    batch_size = len(batch)\n",
    "    \n",
    "    waveforms = torch.zeros(batch_size, max_len, MAX_CHANNELS, 32)\n",
    "    shank_ids = torch.zeros(batch_size, max_len, dtype=torch.long)\n",
    "    mask = torch.ones(batch_size, max_len, dtype=torch.bool)\n",
    "    targets = torch.stack([item['target'] for item in batch])\n",
    "    \n",
    "    for i, item in enumerate(batch):\n",
    "        sl = item['seq_len']\n",
    "        waveforms[i, :sl] = item['waveforms']\n",
    "        shank_ids[i, :sl] = item['shank_ids']\n",
    "        mask[i, :sl] = False\n",
    "    \n",
    "    return {\n",
    "        'waveforms': waveforms,\n",
    "        'shank_ids': shank_ids,\n",
    "        'mask': mask,\n",
    "        'targets': targets\n",
    "    }\n",
    "\n",
    "print('Dataset et collate_fn définis.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feasibility Loss (pénalisation hors labyrinthe)\n",
    "\n",
    "La distance d'un point au squelette du U est calculée de manière **différentiable** :\n",
    "pour chaque segment, on projette le point et on calcule la distance euclidienne.\n",
    "On prend le minimum.\n",
    "\n",
    "La pénalité est `ReLU(distance - corridor_half_width)²` : nulle si le point est dans le couloir, quadratique sinon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeasibilityLoss(nn.Module):\n",
    "    \"\"\"Pénalise les prédictions (x, y) qui tombent hors du couloir du U.\"\"\"\n",
    "    \n",
    "    def __init__(self, skeleton_segments, corridor_half_width):\n",
    "        super().__init__()\n",
    "        self.register_buffer('segments', torch.tensor(skeleton_segments, dtype=torch.float32))\n",
    "        self.corridor_half_width = corridor_half_width\n",
    "    \n",
    "    def forward(self, xy_pred):\n",
    "        px = xy_pred[:, 0]\n",
    "        py = xy_pred[:, 1]\n",
    "        \n",
    "        distances = []\n",
    "        for i in range(self.segments.shape[0]):\n",
    "            x1, y1, x2, y2 = self.segments[i]\n",
    "            dx, dy = x2 - x1, y2 - y1\n",
    "            seg_len_sq = dx**2 + dy**2\n",
    "            \n",
    "            t = ((px - x1) * dx + (py - y1) * dy) / (seg_len_sq + 1e-8)\n",
    "            t = t.clamp(0.0, 1.0)\n",
    "            \n",
    "            proj_x = x1 + t * dx\n",
    "            proj_y = y1 + t * dy\n",
    "            \n",
    "            dist = torch.sqrt((px - proj_x)**2 + (py - proj_y)**2 + 1e-8)\n",
    "            distances.append(dist)\n",
    "        \n",
    "        distances = torch.stack(distances, dim=1)\n",
    "        min_dist = distances.min(dim=1).values\n",
    "        \n",
    "        penalty = torch.relu(min_dist - self.corridor_half_width) ** 2\n",
    "        \n",
    "        return penalty.mean()\n",
    "\n",
    "\n",
    "# Test\n",
    "feas_loss = FeasibilityLoss(SKELETON_SEGMENTS, CORRIDOR_HALF_WIDTH)\n",
    "in_corridor = torch.tensor([[0.15, 0.5], [0.85, 0.3], [0.5, 0.85]], dtype=torch.float32)\n",
    "outside = torch.tensor([[0.5, 0.3], [0.5, 0.5], [0.5, 0.0]], dtype=torch.float32)\n",
    "print(f'Pénalité (dans le couloir) : {feas_loss(in_corridor).item():.6f}')\n",
    "print(f'Pénalité (hors couloir)    : {feas_loss(outside).item():.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Architecture du modèle\n",
    "\n",
    "Identique au notebook 02d. La feasibility loss est externe au modèle (calculée dans la boucle d'entraînement)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpikeEncoder(nn.Module):\n",
    "    \"\"\"Encode un waveform (MAX_CH, 32) en un vecteur de dimension embed_dim.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_channels, embed_dim):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(n_channels, 32, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, embed_dim, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x).squeeze(-1)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Sinusoidal positional encoding.\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, max_len=256):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "class SpikeTransformer(nn.Module):\n",
    "    def __init__(self, nGroups, nChannelsPerGroup, embed_dim=64, nhead=4, \n",
    "                 num_layers=2, dropout=0.2, spike_dropout=0.15, noise_std=0.5,\n",
    "                 max_channels=MAX_CHANNELS):\n",
    "        super().__init__()\n",
    "        self.nGroups = nGroups\n",
    "        self.embed_dim = embed_dim\n",
    "        self.max_channels = max_channels\n",
    "        self.spike_dropout = spike_dropout\n",
    "        self.noise_std = noise_std\n",
    "        \n",
    "        self.spike_encoders = nn.ModuleList([\n",
    "            SpikeEncoder(max_channels, embed_dim) for _ in range(nGroups)\n",
    "        ])\n",
    "        \n",
    "        self.shank_embedding = nn.Embedding(nGroups, embed_dim)\n",
    "        self.pos_encoding = PositionalEncoding(embed_dim)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, nhead=nhead, dim_feedforward=embed_dim * 4,\n",
    "            dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer, num_layers=num_layers, enable_nested_tensor=False\n",
    "        )\n",
    "        \n",
    "        self.mu_head = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim, 2)\n",
    "        )\n",
    "        self.log_sigma_head = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim, 2)\n",
    "        )\n",
    "    \n",
    "    def _apply_spike_dropout(self, mask):\n",
    "        if not self.training or self.spike_dropout <= 0:\n",
    "            return mask\n",
    "        drop_mask = torch.rand_like(mask.float()) < self.spike_dropout\n",
    "        active = ~mask\n",
    "        new_drops = drop_mask & active\n",
    "        remaining = active & ~new_drops\n",
    "        n_remaining = remaining.sum(dim=1)\n",
    "        all_dropped = n_remaining == 0\n",
    "        if all_dropped.any():\n",
    "            new_drops[all_dropped] = False\n",
    "        return mask | new_drops\n",
    "    \n",
    "    def _apply_waveform_noise(self, waveforms):\n",
    "        if not self.training or self.noise_std <= 0:\n",
    "            return waveforms\n",
    "        noise = torch.randn_like(waveforms) * self.noise_std\n",
    "        return waveforms + noise\n",
    "    \n",
    "    def forward(self, waveforms, shank_ids, mask):\n",
    "        batch_size, seq_len = waveforms.shape[:2]\n",
    "        \n",
    "        mask = self._apply_spike_dropout(mask)\n",
    "        waveforms = self._apply_waveform_noise(waveforms)\n",
    "        \n",
    "        embeddings = torch.zeros(batch_size, seq_len, self.embed_dim, device=waveforms.device)\n",
    "        for g in range(self.nGroups):\n",
    "            group_mask = (shank_ids == g) & (~mask)\n",
    "            if group_mask.any():\n",
    "                group_wf = waveforms[group_mask]\n",
    "                group_emb = self.spike_encoders[g](group_wf)\n",
    "                embeddings[group_mask] = group_emb\n",
    "        \n",
    "        shank_emb = self.shank_embedding(shank_ids)\n",
    "        embeddings = embeddings + shank_emb\n",
    "        embeddings = self.pos_encoding(embeddings)\n",
    "        \n",
    "        encoded = self.transformer(embeddings, src_key_padding_mask=mask)\n",
    "        \n",
    "        active_mask = (~mask).unsqueeze(-1).float()\n",
    "        pooled = (encoded * active_mask).sum(dim=1) / (active_mask.sum(dim=1) + 1e-8)\n",
    "        \n",
    "        mu = self.mu_head(pooled)\n",
    "        log_sigma = self.log_sigma_head(pooled)\n",
    "        sigma = torch.exp(log_sigma)\n",
    "        \n",
    "        return mu, sigma\n",
    "\n",
    "\n",
    "# Test rapide\n",
    "SPIKE_DROPOUT = 0.15\n",
    "NOISE_STD = 0.5\n",
    "model = SpikeTransformer(nGroups, nChannelsPerGroup, embed_dim=64, nhead=4, num_layers=2,\n",
    "                         spike_dropout=SPIKE_DROPOUT, noise_std=NOISE_STD)\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Modèle créé : {n_params:,} paramètres')\n",
    "print(f'Data augmentation : spike dropout={SPIKE_DROPOUT:.0%}, gaussian noise std={NOISE_STD}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Split et DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Split temporel 90/10\n",
    "split_idx = int(len(df_moving) * 0.9)\n",
    "df_train_full = df_moving.iloc[:split_idx].reset_index(drop=True)\n",
    "df_test = df_moving.iloc[split_idx:].reset_index(drop=True)\n",
    "\n",
    "print(f'Train (full) : {len(df_train_full)} exemples')\n",
    "print(f'Test         : {len(df_test)} exemples')\n",
    "\n",
    "# KFold\n",
    "N_FOLDS = 5\n",
    "kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=41)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Test loader\n",
    "test_dataset = SpikeSequenceDataset(df_test, nGroups, nChannelsPerGroup)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                         collate_fn=collate_fn, num_workers=0)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(df_train_full)):\n",
    "    print(f'  Fold {fold+1}: train={len(train_idx)}, val={len(val_idx)}')\n",
    "\n",
    "print(f'\\nTest: {len(test_dataset)} exemples, {len(test_loader)} batches')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Entraînement (5-Fold Cross-Validation)\n",
    "\n",
    "**Loss combinée** :\n",
    "```\n",
    "L = L_position (Gaussian NLL sur x, y)\n",
    "  + λ_feas * L_feasibility (pénalité hors labyrinthe)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparamètres\n",
    "EMBED_DIM = 64\n",
    "NHEAD = 4\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.2\n",
    "SPIKE_DROPOUT = 0.15\n",
    "NOISE_STD = 0.5\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "EPOCHS = 30\n",
    "PATIENCE = 7\n",
    "\n",
    "LAMBDA_FEAS = 10.0   # poids de la pénalité hors labyrinthe\n",
    "\n",
    "print(f'Hyperparamètres : embed_dim={EMBED_DIM}, nhead={NHEAD}, layers={NUM_LAYERS}, dropout={DROPOUT}')\n",
    "print(f'Data augmentation : spike dropout={SPIKE_DROPOUT:.0%}, gaussian noise std={NOISE_STD}')\n",
    "print(f'Loss : GaussianNLL + {LAMBDA_FEAS}*Feasibility')\n",
    "print(f'Entraînement : {EPOCHS} epochs max, patience={PATIENCE}, LR={LR}')\n",
    "print(f'Device : {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, scheduler, criterion, feas_loss, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_pos_loss = 0\n",
    "    total_feas_loss = 0\n",
    "    n_batches = 0\n",
    "    \n",
    "    for batch in loader:\n",
    "        wf = batch['waveforms'].to(device)\n",
    "        sid = batch['shank_ids'].to(device)\n",
    "        mask = batch['mask'].to(device)\n",
    "        targets = batch['targets'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        mu, sigma = model(wf, sid, mask)\n",
    "        \n",
    "        loss_pos = criterion(mu, targets, sigma ** 2)\n",
    "        loss_feas = feas_loss(mu)\n",
    "        loss = loss_pos + LAMBDA_FEAS * loss_feas\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_pos_loss += loss_pos.item()\n",
    "        total_feas_loss += loss_feas.item()\n",
    "        n_batches += 1\n",
    "    \n",
    "    return total_loss / n_batches, total_pos_loss / n_batches, total_feas_loss / n_batches\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(model, loader, criterion, feas_loss, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_pos_loss = 0\n",
    "    total_feas_loss = 0\n",
    "    n_batches = 0\n",
    "    all_mu = []\n",
    "    all_sigma = []\n",
    "    all_targets = []\n",
    "    \n",
    "    for batch in loader:\n",
    "        wf = batch['waveforms'].to(device)\n",
    "        sid = batch['shank_ids'].to(device)\n",
    "        mask = batch['mask'].to(device)\n",
    "        targets = batch['targets'].to(device)\n",
    "        \n",
    "        mu, sigma = model(wf, sid, mask)\n",
    "        \n",
    "        loss_pos = criterion(mu, targets, sigma ** 2)\n",
    "        loss_feas = feas_loss(mu)\n",
    "        loss = loss_pos + LAMBDA_FEAS * loss_feas\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_pos_loss += loss_pos.item()\n",
    "        total_feas_loss += loss_feas.item()\n",
    "        n_batches += 1\n",
    "        all_mu.append(mu.cpu().numpy())\n",
    "        all_sigma.append(sigma.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "    \n",
    "    return (total_loss / n_batches, total_pos_loss / n_batches, total_feas_loss / n_batches,\n",
    "            np.concatenate(all_mu), np.concatenate(all_sigma), np.concatenate(all_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boucle d'entraînement avec KFold\n",
    "fold_results = []\n",
    "all_train_losses = {}\n",
    "all_val_losses = {}\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(df_train_full)):\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'FOLD {fold+1}/{N_FOLDS}')\n",
    "    print(f'{\"=\"*60}')\n",
    "    \n",
    "    df_fold_train = df_train_full.iloc[train_idx].reset_index(drop=True)\n",
    "    df_fold_val = df_train_full.iloc[val_idx].reset_index(drop=True)\n",
    "    \n",
    "    fold_train_dataset = SpikeSequenceDataset(df_fold_train, nGroups, nChannelsPerGroup)\n",
    "    fold_val_dataset = SpikeSequenceDataset(df_fold_val, nGroups, nChannelsPerGroup)\n",
    "    \n",
    "    fold_train_loader = DataLoader(fold_train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                                   collate_fn=collate_fn, num_workers=0)\n",
    "    fold_val_loader = DataLoader(fold_val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                                 collate_fn=collate_fn, num_workers=0)\n",
    "    \n",
    "    print(f'  Train: {len(fold_train_dataset)}, Val: {len(fold_val_dataset)}')\n",
    "    \n",
    "    model = SpikeTransformer(\n",
    "        nGroups, nChannelsPerGroup,\n",
    "        embed_dim=EMBED_DIM, nhead=NHEAD, num_layers=NUM_LAYERS,\n",
    "        dropout=DROPOUT, spike_dropout=SPIKE_DROPOUT, noise_std=NOISE_STD\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=LR, epochs=EPOCHS, steps_per_epoch=len(fold_train_loader)\n",
    "    )\n",
    "    criterion = nn.GaussianNLLLoss()\n",
    "    feas_loss_fn = FeasibilityLoss(SKELETON_SEGMENTS, CORRIDOR_HALF_WIDTH).to(DEVICE)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    model_path = f'../outputs/best_transformer_02d_bis_fold{fold+1}.pt'\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        t_loss, t_pos, t_feas = train_epoch(\n",
    "            model, fold_train_loader, optimizer, scheduler, criterion, feas_loss_fn, DEVICE\n",
    "        )\n",
    "        v_loss, v_pos, v_feas, _, _, _ = eval_epoch(\n",
    "            model, fold_val_loader, criterion, feas_loss_fn, DEVICE\n",
    "        )\n",
    "        \n",
    "        train_losses.append(t_loss)\n",
    "        val_losses.append(v_loss)\n",
    "        \n",
    "        if epoch % 5 == 0 or epoch == EPOCHS - 1:\n",
    "            print(f'  Epoch {epoch+1:02d}/{EPOCHS} | Train: {t_loss:.5f} (pos={t_pos:.5f}, feas={t_feas:.6f}) | Val: {v_loss:.5f}')\n",
    "        \n",
    "        if v_loss < best_val_loss:\n",
    "            best_val_loss = v_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f'  Early stopping a epoch {epoch+1}')\n",
    "                break\n",
    "    \n",
    "    all_train_losses[fold] = train_losses\n",
    "    all_val_losses[fold] = val_losses\n",
    "    \n",
    "    # Évaluer sur la validation de ce fold\n",
    "    model.load_state_dict(torch.load(model_path, map_location=DEVICE, weights_only=True))\n",
    "    _, _, _, val_mu, val_sigma, val_targets = eval_epoch(\n",
    "        model, fold_val_loader, criterion, feas_loss_fn, DEVICE\n",
    "    )\n",
    "    val_eucl = np.sqrt((val_targets[:, 0] - val_mu[:, 0])**2 + (val_targets[:, 1] - val_mu[:, 1])**2)\n",
    "    \n",
    "    # % de prédictions hors labyrinthe\n",
    "    val_dist_to_skel = np.array([\n",
    "        compute_distance_to_skeleton(val_mu[i, 0], val_mu[i, 1]) for i in range(len(val_mu))\n",
    "    ])\n",
    "    pct_outside = (val_dist_to_skel > CORRIDOR_HALF_WIDTH).mean()\n",
    "    \n",
    "    fold_results.append({\n",
    "        'fold': fold + 1,\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'val_eucl_mean': val_eucl.mean(),\n",
    "        'val_r2_x': r2_score(val_targets[:, 0], val_mu[:, 0]),\n",
    "        'val_r2_y': r2_score(val_targets[:, 1], val_mu[:, 1]),\n",
    "        'val_pct_outside': pct_outside,\n",
    "        'epochs': len(train_losses),\n",
    "    })\n",
    "    print(f'  Best val loss: {best_val_loss:.5f} | Eucl: {val_eucl.mean():.4f} | R2: X={fold_results[-1][\"val_r2_x\"]:.4f}, Y={fold_results[-1][\"val_r2_y\"]:.4f} | Hors: {pct_outside:.1%}')\n",
    "\n",
    "# Résumé\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print(f'RESUME CROSS-VALIDATION ({N_FOLDS} folds)')\n",
    "print(f'{\"=\"*60}')\n",
    "for r in fold_results:\n",
    "    print(f'  Fold {r[\"fold\"]}: NLL={r[\"best_val_loss\"]:.5f} | Eucl={r[\"val_eucl_mean\"]:.4f} | R2_X={r[\"val_r2_x\"]:.4f} | R2_Y={r[\"val_r2_y\"]:.4f} | hors={r[\"val_pct_outside\"]:.1%}')\n",
    "\n",
    "mean_eucl = np.mean([r['val_eucl_mean'] for r in fold_results])\n",
    "std_eucl = np.std([r['val_eucl_mean'] for r in fold_results])\n",
    "mean_r2_x = np.mean([r['val_r2_x'] for r in fold_results])\n",
    "mean_r2_y = np.mean([r['val_r2_y'] for r in fold_results])\n",
    "mean_outside = np.mean([r['val_pct_outside'] for r in fold_results])\n",
    "print(f'\\n  Moyenne : Eucl={mean_eucl:.4f} (+/- {std_eucl:.4f}) | R2_X={mean_r2_x:.4f} | R2_Y={mean_r2_y:.4f} | hors={mean_outside:.1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courbes d'entraînement par fold\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, N_FOLDS))\n",
    "\n",
    "for fold in range(N_FOLDS):\n",
    "    axes[0].plot(all_train_losses[fold], color=colors[fold], linewidth=1.5, label=f'Fold {fold+1}')\n",
    "    axes[1].plot(all_val_losses[fold], color=colors[fold], linewidth=1.5, label=f'Fold {fold+1}')\n",
    "\n",
    "axes[0].set_xlabel('Epoch'); axes[0].set_ylabel('Loss totale')\n",
    "axes[0].set_title('Train Loss par fold'); axes[0].legend(); axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].set_xlabel('Epoch'); axes[1].set_ylabel('Loss totale')\n",
    "axes[1].set_title('Validation Loss par fold'); axes[1].legend(); axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Évaluation finale sur le test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluation ensemble des 5 folds sur le test set\n",
    "criterion = nn.GaussianNLLLoss()\n",
    "feas_loss_fn = FeasibilityLoss(SKELETON_SEGMENTS, CORRIDOR_HALF_WIDTH).to(DEVICE)\n",
    "\n",
    "all_fold_mu = []\n",
    "all_fold_sigma = []\n",
    "\n",
    "for fold in range(N_FOLDS):\n",
    "    model_path = f'../outputs/best_transformer_02d_bis_fold{fold+1}.pt'\n",
    "    model = SpikeTransformer(\n",
    "        nGroups, nChannelsPerGroup,\n",
    "        embed_dim=EMBED_DIM, nhead=NHEAD, num_layers=NUM_LAYERS,\n",
    "        dropout=DROPOUT, spike_dropout=SPIKE_DROPOUT, noise_std=NOISE_STD\n",
    "    ).to(DEVICE)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=DEVICE, weights_only=True))\n",
    "    \n",
    "    _, _, _, fold_mu, fold_sigma, y_test = eval_epoch(\n",
    "        model, test_loader, criterion, feas_loss_fn, DEVICE\n",
    "    )\n",
    "    all_fold_mu.append(fold_mu)\n",
    "    all_fold_sigma.append(fold_sigma)\n",
    "    \n",
    "    fold_eucl = np.sqrt((y_test[:, 0] - fold_mu[:, 0])**2 + (y_test[:, 1] - fold_mu[:, 1])**2)\n",
    "    print(f'Fold {fold+1} sur test: Eucl={fold_eucl.mean():.4f}')\n",
    "\n",
    "# Ensemble\n",
    "all_fold_mu = np.stack(all_fold_mu)\n",
    "all_fold_sigma = np.stack(all_fold_sigma)\n",
    "\n",
    "y_pred = all_fold_mu.mean(axis=0)\n",
    "\n",
    "# Sigma ensemble (loi de la variance totale)\n",
    "mean_var = (all_fold_sigma ** 2).mean(axis=0)\n",
    "var_mu = all_fold_mu.var(axis=0)\n",
    "y_sigma = np.sqrt(mean_var + var_mu)\n",
    "\n",
    "# Métriques\n",
    "mse_x = mean_squared_error(y_test[:, 0], y_pred[:, 0])\n",
    "mse_y = mean_squared_error(y_test[:, 1], y_pred[:, 1])\n",
    "mae_x = mean_absolute_error(y_test[:, 0], y_pred[:, 0])\n",
    "mae_y = mean_absolute_error(y_test[:, 1], y_pred[:, 1])\n",
    "r2_x = r2_score(y_test[:, 0], y_pred[:, 0])\n",
    "r2_y = r2_score(y_test[:, 1], y_pred[:, 1])\n",
    "eucl_errors = np.sqrt((y_test[:, 0] - y_pred[:, 0])**2 + (y_test[:, 1] - y_pred[:, 1])**2)\n",
    "\n",
    "# % hors labyrinthe\n",
    "test_dist_to_skel = np.array([\n",
    "    compute_distance_to_skeleton(y_pred[i, 0], y_pred[i, 1]) for i in range(len(y_pred))\n",
    "])\n",
    "pct_outside = (test_dist_to_skel > CORRIDOR_HALF_WIDTH).mean()\n",
    "\n",
    "print(f'\\n=== Transformer 02d_bis : Spike Dropout + Noise + Feasibility — Ensemble ({N_FOLDS} folds) ===')\n",
    "print(f'  MSE  : X={mse_x:.5f}, Y={mse_y:.5f}')\n",
    "print(f'  MAE  : X={mae_x:.4f}, Y={mae_y:.4f}')\n",
    "print(f'  R²   : X={r2_x:.4f}, Y={r2_y:.4f}')\n",
    "print(f'  Eucl : mean={eucl_errors.mean():.4f}, median={np.median(eucl_errors):.4f}, p90={np.percentile(eucl_errors, 90):.4f}')\n",
    "print(f'  Hors labyrinthe : {pct_outside:.1%}')\n",
    "print(f'\\n  Sigma moyen : X={y_sigma[:, 0].mean():.4f}, Y={y_sigma[:, 1].mean():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Scatter pred vs true ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axes[0].scatter(y_test[:, 0], y_pred[:, 0], s=1, alpha=0.3)\n",
    "axes[0].plot([0, 1], [0, 1], 'r--', linewidth=2)\n",
    "axes[0].set_xlabel('True X'); axes[0].set_ylabel('Predicted X')\n",
    "axes[0].set_title(f'02d_bis - Position X (R²={r2_x:.3f})')\n",
    "axes[0].set_aspect('equal')\n",
    "\n",
    "axes[1].scatter(y_test[:, 1], y_pred[:, 1], s=1, alpha=0.3)\n",
    "axes[1].plot([0, 1], [0, 1], 'r--', linewidth=2)\n",
    "axes[1].set_xlabel('True Y'); axes[1].set_ylabel('Predicted Y')\n",
    "axes[1].set_title(f'02d_bis - Position Y (R²={r2_y:.3f})')\n",
    "axes[1].set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Points prédits vs vrais avec incertitude ---\n",
    "segment = slice(0, 500)\n",
    "seg_idx = np.arange(500)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Points 2D colorés par ordre chronologique\n",
    "colors = np.arange(500)\n",
    "axes[0, 0].scatter(y_test[segment, 0], y_test[segment, 1], c=colors, cmap='winter', s=8, alpha=0.6, label='Vraie position')\n",
    "sc = axes[0, 0].scatter(y_pred[segment, 0], y_pred[segment, 1], c=colors, cmap='autumn', s=8, alpha=0.6, marker='x', label='Prediction (mu)')\n",
    "for x1, y1, x2, y2 in SKELETON_SEGMENTS:\n",
    "    axes[0, 0].plot([x1, x2], [y1, y2], 'k--', linewidth=1, alpha=0.3)\n",
    "axes[0, 0].set_xlabel('X'); axes[0, 0].set_ylabel('Y')\n",
    "axes[0, 0].set_title('Positions (500 premiers points test)')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].set_aspect('equal')\n",
    "cbar = plt.colorbar(sc, ax=axes[0, 0])\n",
    "cbar.set_label('Index temporel')\n",
    "\n",
    "# 2. Position X avec bande d'incertitude\n",
    "axes[0, 1].plot(seg_idx, y_test[segment, 0], 'b-', label='Vrai X', linewidth=1.5)\n",
    "axes[0, 1].plot(seg_idx, y_pred[segment, 0], 'r-', alpha=0.7, label='Prediction (mu)', linewidth=1)\n",
    "axes[0, 1].fill_between(seg_idx, \n",
    "                         y_pred[segment, 0] - 2 * y_sigma[segment, 0],\n",
    "                         y_pred[segment, 0] + 2 * y_sigma[segment, 0],\n",
    "                         alpha=0.2, color='red', label='Incertitude (2 sigma)')\n",
    "axes[0, 1].set_xlabel('Index'); axes[0, 1].set_ylabel('Position X')\n",
    "axes[0, 1].set_title('Position X avec incertitude')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# 3. Position Y avec bande d'incertitude\n",
    "axes[1, 0].plot(seg_idx, y_test[segment, 1], 'b-', label='Vrai Y', linewidth=1.5)\n",
    "axes[1, 0].plot(seg_idx, y_pred[segment, 1], 'r-', alpha=0.7, label='Prediction (mu)', linewidth=1)\n",
    "axes[1, 0].fill_between(seg_idx,\n",
    "                         y_pred[segment, 1] - 2 * y_sigma[segment, 1],\n",
    "                         y_pred[segment, 1] + 2 * y_sigma[segment, 1],\n",
    "                         alpha=0.2, color='red', label='Incertitude (2 sigma)')\n",
    "axes[1, 0].set_xlabel('Index'); axes[1, 0].set_ylabel('Position Y')\n",
    "axes[1, 0].set_title('Position Y avec incertitude')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# 4. Calibration\n",
    "sigma_mean = (y_sigma[:, 0] + y_sigma[:, 1]) / 2\n",
    "axes[1, 1].scatter(sigma_mean, eucl_errors, s=1, alpha=0.3)\n",
    "axes[1, 1].set_xlabel('Sigma moyen predit'); axes[1, 1].set_ylabel('Erreur euclidienne reelle')\n",
    "axes[1, 1].set_title('Calibration : incertitude vs erreur')\n",
    "sigma_range = np.linspace(0, sigma_mean.max(), 100)\n",
    "axes[1, 1].plot(sigma_range, 2 * sigma_range, 'r--', label='y = 2*sigma', linewidth=1.5)\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calibration\n",
    "in_1sigma = np.mean(eucl_errors < sigma_mean)\n",
    "in_2sigma = np.mean(eucl_errors < 2 * sigma_mean)\n",
    "in_3sigma = np.mean(eucl_errors < 3 * sigma_mean)\n",
    "print(f'Calibration de l\\'incertitude :')\n",
    "print(f'  Erreur < 1*sigma : {in_1sigma:.1%} (attendu ~39%)')\n",
    "print(f'  Erreur < 2*sigma : {in_2sigma:.1%} (attendu ~86%)')\n",
    "print(f'  Erreur < 3*sigma : {in_3sigma:.1%} (attendu ~99%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Heatmaps : erreur + distance au squelette ---\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 7))\n",
    "\n",
    "nbins = 20\n",
    "x_edges = np.linspace(0, 1, nbins + 1)\n",
    "y_edges = np.linspace(0, 1, nbins + 1)\n",
    "\n",
    "for ax_idx, (title, values, cmap, pos_for_binning) in enumerate([\n",
    "    ('Erreur euclidienne moyenne', eucl_errors, 'RdYlGn_r', y_test),\n",
    "    ('Sigma moyen predit', (y_sigma[:, 0] + y_sigma[:, 1]) / 2, 'RdYlGn_r', y_test),\n",
    "    ('Distance au squelette (predictions)', test_dist_to_skel, 'Reds', y_pred)\n",
    "]):\n",
    "    val_map = np.full((nbins, nbins), np.nan)\n",
    "    count_map = np.zeros((nbins, nbins))\n",
    "    \n",
    "    for i in range(len(pos_for_binning)):\n",
    "        xi = np.clip(np.searchsorted(x_edges, pos_for_binning[i, 0]) - 1, 0, nbins - 1)\n",
    "        yi = np.clip(np.searchsorted(y_edges, pos_for_binning[i, 1]) - 1, 0, nbins - 1)\n",
    "        if np.isnan(val_map[yi, xi]):\n",
    "            val_map[yi, xi] = 0\n",
    "        val_map[yi, xi] += values[i]\n",
    "        count_map[yi, xi] += 1\n",
    "    \n",
    "    mean_map = np.where(count_map > 0, val_map / count_map, np.nan)\n",
    "    \n",
    "    im = axes[ax_idx].imshow(mean_map, origin='lower', aspect='equal', \n",
    "                              cmap=cmap, extent=[0, 1, 0, 1])\n",
    "    axes[ax_idx].set_xlabel('X'); axes[ax_idx].set_ylabel('Y')\n",
    "    axes[ax_idx].set_title(title)\n",
    "    plt.colorbar(im, ax=axes[ax_idx])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Sauvegarde des prédictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../outputs/preds_transformer_02d_bis.npy', y_pred)\n",
    "np.save('../outputs/sigma_transformer_02d_bis.npy', y_sigma)\n",
    "np.save('../outputs/y_test_transformer_02d_bis.npy', y_test)\n",
    "print(f'Predictions ensemble ({N_FOLDS} folds) sauvegardees.')\n",
    "print(f'  preds_transformer_02d_bis.npy : mu ensemble ({y_pred.shape})')\n",
    "print(f'  sigma_transformer_02d_bis.npy : sigma ensemble ({y_sigma.shape})')\n",
    "print(f'  y_test_transformer_02d_bis.npy : targets ({y_test.shape})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Interprétation\n",
    "\n",
    "### Modification apportée (02d_bis vs 02d)\n",
    "\n",
    "**Feasibility loss** : pénalise quadratiquement les prédictions (x, y) qui tombent en dehors du couloir du U. La distance au squelette est calculée de manière différentiable via projection sur les 3 segments.\n",
    "\n",
    "- `λ_feas = 10.0` : poids élevé pour fortement décourager les prédictions hors couloir\n",
    "- Pénalité nulle si la prédiction est dans le couloir (distance < 0.15)\n",
    "- Pénalité quadratique au-delà : `ReLU(dist - 0.15)²`\n",
    "\n",
    "### Impact attendu\n",
    "\n",
    "- **Moins de prédictions au centre du U** : la feasibility loss pénalise les points qui tombent dans le \"trou\" du U (zone hors couloir)\n",
    "- **Prédictions plus réalistes** : toutes les prédictions devraient rester sur le tracé du labyrinthe\n",
    "- **Potentiel compromis sur la NLL** : le modèle sacrifie un peu de vraisemblance pour la faisabilité géométrique\n",
    "\n",
    "### Comparaison avec 02d\n",
    "\n",
    "La métrique clé est le **% de prédictions hors labyrinthe** : si la feasibility loss fonctionne, ce pourcentage devrait baisser significativement par rapport au 02d."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
