{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 5 : Améliorations avancées\n",
    "\n",
    "Ce notebook regroupe 3 pistes d'amélioration :\n",
    "\n",
    "1. **Kalman avec modèle de vitesse constante** : au lieu de supposer que la souris est immobile, on modélise l'état comme `[x, y, vx, vy]` — le filtre anticipe le mouvement\n",
    "2. **Ensemble de modèles** : on combine les prédictions XGBoost + Transformer (+ CNN) par moyenne pondérée ou stacking\n",
    "3. **GRU multi-fenêtre** : on utilise les N fenêtres précédentes comme contexte temporel — un GRU au-dessus du Transformer apprend la continuité de la trajectoire\n",
    "\n",
    "**Prérequis** : avoir exécuté les notebooks 1, 2, 3 (les fichiers `.npy` et `.pt` doivent exister)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports et configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device('mps')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "print(f'Device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "source": "# --- Connexion S3 (Onyxia) ou chargement local ---\nS3_ENDPOINT = \"https://minio.lab.sspcloud.fr\"\nS3_BUCKET = \"gmarguier\"\nS3_PREFIX = \"hacktion-potential\"\n\nPARQUET_NAME = \"M1199_PAG_stride4_win108_test.parquet\"\nJSON_NAME = \"M1199_PAG.json\"\n\nLOCAL_DIR = os.path.join(os.path.abspath('..'), 'data')\nLOCAL_PARQUET = os.path.join(LOCAL_DIR, PARQUET_NAME)\nLOCAL_JSON = os.path.join(LOCAL_DIR, JSON_NAME)\n\nUSE_S3 = False\nfs = None\n\ntry:\n    import s3fs\n    fs = s3fs.S3FileSystem(\n        client_kwargs={\"endpoint_url\": S3_ENDPOINT},\n        key=os.environ.get(\"AWS_ACCESS_KEY_ID\"),\n        secret=os.environ.get(\"AWS_SECRET_ACCESS_KEY\"),\n        token=os.environ.get(\"AWS_SESSION_TOKEN\"),\n    )\n    s3_parquet = f\"{S3_BUCKET}/{S3_PREFIX}/{PARQUET_NAME}\"\n    if fs.exists(s3_parquet):\n        USE_S3 = True\n        print(f\"[S3] Connexion OK — lecture depuis s3://{S3_BUCKET}/{S3_PREFIX}/\")\n    else:\n        print(f\"[S3] Bucket accessible mais fichier introuvable ({s3_parquet})\")\nexcept Exception as e:\n    print(f\"[S3] Non disponible ({type(e).__name__}: {e})\")\n\nif not USE_S3:\n    if os.path.exists(LOCAL_PARQUET):\n        print(f\"[LOCAL] Chargement depuis {LOCAL_DIR}/\")\n    else:\n        raise FileNotFoundError(f\"Données introuvables ni sur S3 ni en local ({LOCAL_PARQUET})\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(name, y_true, y_pred):\n",
    "    \"\"\"Calcule et affiche les métriques.\"\"\"\n",
    "    r2_x = r2_score(y_true[:, 0], y_pred[:, 0])\n",
    "    r2_y = r2_score(y_true[:, 1], y_pred[:, 1])\n",
    "    eucl = np.sqrt((y_true[:, 0] - y_pred[:, 0])**2 + (y_true[:, 1] - y_pred[:, 1])**2)\n",
    "    print(f'=== {name} ===')\n",
    "    print(f'  R²   : X={r2_x:.4f}, Y={r2_y:.4f}')\n",
    "    print(f'  Eucl : mean={eucl.mean():.4f}, median={np.median(eucl):.4f}, p90={np.percentile(eucl, 90):.4f}')\n",
    "    return {'r2_x': r2_x, 'r2_y': r2_y, 'eucl': eucl}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Partie 1 : Filtre de Kalman avec modèle de vitesse constante\n",
    "\n",
    "## Pourquoi ?\n",
    "\n",
    "Le Kalman du notebook 1-3 utilise un modèle de **position constante** : il suppose que la souris ne bouge pas entre deux observations. C'est une approximation très grossière — une souris en mouvement parcourt une distance significative entre deux fenêtres de 108ms.\n",
    "\n",
    "Le modèle de **vitesse constante** utilise un état étendu `[x, y, vx, vy]` :\n",
    "- **Prédiction** : `x(t+1) = x(t) + vx(t)*dt`, `vx(t+1) = vx(t)` (la vitesse persiste)\n",
    "- **Mesure** : on observe `[x, y]` (les prédictions du modèle ML)\n",
    "\n",
    "Cela permet au filtre d'**anticiper** le mouvement plutôt que de simplement lisser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kalman_velocity(observations, process_noise_pos=0.001, process_noise_vel=0.01,\n",
    "                     measurement_noise=0.01, dt=1.0):\n",
    "    \"\"\"\n",
    "    Filtre de Kalman avec modèle de vitesse constante.\n",
    "    \n",
    "    État : [x, y, vx, vy]\n",
    "    Transition : x += vx*dt, y += vy*dt, vx = vx, vy = vy\n",
    "    Observation : [x, y] (prédictions du modèle ML)\n",
    "    \n",
    "    Args:\n",
    "        observations: (N, 2) - prédictions brutes\n",
    "        process_noise_pos: bruit sur la position (Q_pos)\n",
    "        process_noise_vel: bruit sur la vitesse (Q_vel)\n",
    "        measurement_noise: bruit de mesure (R)\n",
    "        dt: pas de temps normalisé entre observations\n",
    "    \n",
    "    Returns:\n",
    "        smoothed: (N, 2) - positions lissées\n",
    "    \"\"\"\n",
    "    N = len(observations)\n",
    "    smoothed = np.zeros((N, 2), dtype=np.float64)\n",
    "    \n",
    "    # Matrice de transition : vitesse constante\n",
    "    F = np.array([\n",
    "        [1, 0, dt, 0],\n",
    "        [0, 1, 0, dt],\n",
    "        [0, 0, 1,  0],\n",
    "        [0, 0, 0,  1]\n",
    "    ], dtype=np.float64)\n",
    "    \n",
    "    # Matrice d'observation : on observe seulement (x, y)\n",
    "    H = np.array([\n",
    "        [1, 0, 0, 0],\n",
    "        [0, 1, 0, 0]\n",
    "    ], dtype=np.float64)\n",
    "    \n",
    "    # Bruit du processus\n",
    "    Q = np.diag([process_noise_pos, process_noise_pos,\n",
    "                 process_noise_vel, process_noise_vel])\n",
    "    \n",
    "    # Bruit de mesure\n",
    "    R = np.eye(2) * measurement_noise\n",
    "    \n",
    "    # État initial : position = première observation, vitesse = 0\n",
    "    x_est = np.array([observations[0, 0], observations[0, 1], 0.0, 0.0], dtype=np.float64)\n",
    "    P = np.diag([measurement_noise, measurement_noise, 0.1, 0.1])\n",
    "    \n",
    "    I4 = np.eye(4)\n",
    "    \n",
    "    for t in range(N):\n",
    "        # --- Prédiction ---\n",
    "        x_pred = F @ x_est\n",
    "        P_pred = F @ P @ F.T + Q\n",
    "        \n",
    "        # --- Mise à jour ---\n",
    "        z = observations[t].astype(np.float64)\n",
    "        y_innov = z - H @ x_pred\n",
    "        S = H @ P_pred @ H.T + R\n",
    "        K = P_pred @ H.T @ np.linalg.inv(S)\n",
    "        \n",
    "        x_est = x_pred + K @ y_innov\n",
    "        P = (I4 - K @ H) @ P_pred\n",
    "        \n",
    "        smoothed[t] = x_est[:2]  # on ne retourne que (x, y)\n",
    "    \n",
    "    return smoothed.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les prédictions des notebooks précédents\n",
    "preds = {}\n",
    "y_tests = {}\n",
    "\n",
    "for name, pred_file, true_file in [\n",
    "    ('XGBoost', '../outputs/preds_xgboost.npy', '../outputs/y_test.npy'),\n",
    "    ('Transformer', '../outputs/preds_transformer.npy', '../outputs/y_test_transformer.npy'),\n",
    "    ('CNN', '../outputs/preds_cnn.npy', '../outputs/y_test_cnn.npy'),\n",
    "]:\n",
    "    if os.path.exists(pred_file) and os.path.exists(true_file):\n",
    "        preds[name] = np.load(pred_file)\n",
    "        y_tests[name] = np.load(true_file)\n",
    "        print(f'{name}: {preds[name].shape}')\n",
    "\n",
    "print(f'\\nModèles disponibles : {list(preds.keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ancien Kalman (position constante) pour comparaison\n",
    "def kalman_position(observations, process_noise=0.001, measurement_noise=0.01):\n",
    "    \"\"\"Kalman avec modèle de position constante (comme dans les notebooks 1-3).\"\"\"\n",
    "    N = len(observations)\n",
    "    smoothed = np.zeros_like(observations)\n",
    "    x_est = observations[0].copy()\n",
    "    P = np.eye(2) * measurement_noise\n",
    "    Q = np.eye(2) * process_noise\n",
    "    R = np.eye(2) * measurement_noise\n",
    "    for t in range(N):\n",
    "        x_pred = x_est\n",
    "        P_pred = P + Q\n",
    "        z = observations[t]\n",
    "        y_innov = z - x_pred\n",
    "        S = P_pred + R\n",
    "        K = P_pred @ np.linalg.inv(S)\n",
    "        x_est = x_pred + K @ y_innov\n",
    "        P = (np.eye(2) - K) @ P_pred\n",
    "        smoothed[t] = x_est\n",
    "    return smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tester le Kalman vitesse sur chaque modèle\n",
    "# On compare : brut, Kalman position, Kalman vitesse\n",
    "\n",
    "kalman_vel_configs = {\n",
    "    'Vel léger': {'process_noise_pos': 0.005, 'process_noise_vel': 0.05, 'measurement_noise': 0.005},\n",
    "    'Vel moyen': {'process_noise_pos': 0.001, 'process_noise_vel': 0.02, 'measurement_noise': 0.02},\n",
    "    'Vel fort':  {'process_noise_pos': 0.0005, 'process_noise_vel': 0.01, 'measurement_noise': 0.05},\n",
    "}\n",
    "\n",
    "kalman_vel_results = {}  # {model_name: {config_name: y_smooth}}\n",
    "\n",
    "for model_name in preds:\n",
    "    y_pred = preds[model_name]\n",
    "    y_true = y_tests[model_name]\n",
    "    \n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'Modèle : {model_name}')\n",
    "    print(f'{\"=\"*60}')\n",
    "    \n",
    "    # Brut\n",
    "    evaluate(f'{model_name} brut', y_true, y_pred)\n",
    "    \n",
    "    # Kalman position (meilleur config des notebooks)\n",
    "    y_pos = kalman_position(y_pred, process_noise=0.001, measurement_noise=0.02)\n",
    "    evaluate(f'{model_name} + Kalman position', y_true, y_pos)\n",
    "    \n",
    "    # Kalman vitesse\n",
    "    kalman_vel_results[model_name] = {}\n",
    "    best_eucl = float('inf')\n",
    "    best_config = None\n",
    "    \n",
    "    for config_name, params in kalman_vel_configs.items():\n",
    "        y_vel = kalman_velocity(y_pred, **params)\n",
    "        res = evaluate(f'{model_name} + Kalman {config_name}', y_true, y_vel)\n",
    "        kalman_vel_results[model_name][config_name] = y_vel\n",
    "        \n",
    "        if res['eucl'].mean() < best_eucl:\n",
    "            best_eucl = res['eucl'].mean()\n",
    "            best_config = config_name\n",
    "    \n",
    "    print(f'\\n  >> Meilleur Kalman vitesse : {best_config} (eucl={best_eucl:.4f})')\n",
    "    \n",
    "    # Sauvegarder le meilleur\n",
    "    fname = f'../outputs/preds_{model_name.lower()}_kalman_vel.npy'\n",
    "    np.save(fname, kalman_vel_results[model_name][best_config])\n",
    "    print(f'  >> Sauvegardé : {fname}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation : Kalman position vs Kalman vitesse (sur le Transformer)\n",
    "if 'Transformer' in preds:\n",
    "    y_pred_t = preds['Transformer']\n",
    "    y_true_t = y_tests['Transformer']\n",
    "    \n",
    "    y_pos = kalman_position(y_pred_t, 0.001, 0.02)\n",
    "    y_vel = kalman_velocity(y_pred_t, 0.001, 0.02, 0.02)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    seg = slice(0, 300)\n",
    "    \n",
    "    # Trajectoire\n",
    "    ax = axes[0, 0]\n",
    "    ax.plot(y_true_t[seg, 0], y_true_t[seg, 1], 'b-', alpha=0.5, label='Vérité', linewidth=1.5)\n",
    "    ax.plot(y_pred_t[seg, 0], y_pred_t[seg, 1], 'r-', alpha=0.3, label='Brut', linewidth=0.8)\n",
    "    ax.plot(y_pos[seg, 0], y_pos[seg, 1], 'orange', alpha=0.6, label='Kalman position', linewidth=1.2)\n",
    "    ax.plot(y_vel[seg, 0], y_vel[seg, 1], 'g-', alpha=0.7, label='Kalman vitesse', linewidth=1.5)\n",
    "    ax.set_xlabel('X'); ax.set_ylabel('Y')\n",
    "    ax.set_title('Trajectoire (300 points test)')\n",
    "    ax.legend(); ax.set_aspect('equal')\n",
    "    \n",
    "    # Position X\n",
    "    ax = axes[0, 1]\n",
    "    t_idx = np.arange(200)\n",
    "    ax.plot(t_idx, y_true_t[t_idx, 0], 'b-', label='Vérité', linewidth=2)\n",
    "    ax.plot(t_idx, y_pred_t[t_idx, 0], 'r-', alpha=0.3, label='Brut', linewidth=0.8)\n",
    "    ax.plot(t_idx, y_pos[t_idx, 0], 'orange', alpha=0.6, label='Kalman position', linewidth=1.2)\n",
    "    ax.plot(t_idx, y_vel[t_idx, 0], 'g-', label='Kalman vitesse', linewidth=1.5)\n",
    "    ax.set_xlabel('Index'); ax.set_ylabel('X')\n",
    "    ax.set_title('Position X - Zoom 200 points')\n",
    "    ax.legend()\n",
    "    \n",
    "    # CDF\n",
    "    ax = axes[1, 0]\n",
    "    for label, yp, color in [\n",
    "        ('Brut', y_pred_t, 'red'),\n",
    "        ('Kalman position', y_pos, 'orange'),\n",
    "        ('Kalman vitesse', y_vel, 'green'),\n",
    "    ]:\n",
    "        e = np.sort(np.sqrt((y_true_t[:, 0] - yp[:, 0])**2 + (y_true_t[:, 1] - yp[:, 1])**2))\n",
    "        ax.plot(e, np.linspace(0, 1, len(e)), label=label, linewidth=2, color=color)\n",
    "    ax.set_xlabel('Erreur euclidienne'); ax.set_ylabel('CDF')\n",
    "    ax.set_title('CDF des erreurs - Transformer')\n",
    "    ax.legend(); ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Barplot comparatif\n",
    "    ax = axes[1, 1]\n",
    "    eucl_brut = np.sqrt((y_true_t[:, 0] - y_pred_t[:, 0])**2 + (y_true_t[:, 1] - y_pred_t[:, 1])**2)\n",
    "    eucl_pos = np.sqrt((y_true_t[:, 0] - y_pos[:, 0])**2 + (y_true_t[:, 1] - y_pos[:, 1])**2)\n",
    "    eucl_vel = np.sqrt((y_true_t[:, 0] - y_vel[:, 0])**2 + (y_true_t[:, 1] - y_vel[:, 1])**2)\n",
    "    \n",
    "    names = ['Brut', 'Kalman\\nposition', 'Kalman\\nvitesse']\n",
    "    means = [eucl_brut.mean(), eucl_pos.mean(), eucl_vel.mean()]\n",
    "    p90s = [np.percentile(eucl_brut, 90), np.percentile(eucl_pos, 90), np.percentile(eucl_vel, 90)]\n",
    "    \n",
    "    x_pos = np.arange(3)\n",
    "    ax.bar(x_pos - 0.15, means, 0.3, label='Moyenne', color='steelblue')\n",
    "    ax.bar(x_pos + 0.15, p90s, 0.3, label='P90', color='coral')\n",
    "    ax.set_xticks(x_pos); ax.set_xticklabels(names)\n",
    "    ax.set_ylabel('Erreur euclidienne')\n",
    "    ax.set_title('Comparaison Kalman - Transformer')\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.suptitle('Kalman position vs vitesse constante (Transformer)', fontsize=14, fontweight='bold', y=1.01)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Partie 2 : Ensemble de modèles\n",
    "\n",
    "## Pourquoi ?\n",
    "\n",
    "Chaque modèle capture des aspects différents des données :\n",
    "- **XGBoost** : features statistiques manuelles, robuste mais perd l'info fine\n",
    "- **Transformer** : waveforms bruts, co-activations, mais peu de données d'entraînement\n",
    "- **CNN** : patterns spatio-temporels, mais agrège les spikes en bins\n",
    "\n",
    "En combinant leurs prédictions, on exploite leur complémentarité.\n",
    "\n",
    "**3 stratégies :**\n",
    "1. Moyenne simple\n",
    "2. Moyenne pondérée (poids inversement proportionnels à l'erreur)\n",
    "3. Stacking : un méta-modèle (Ridge) apprend la meilleure combinaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier que tous les modèles ont le même test set\n",
    "available_models = list(preds.keys())\n",
    "print(f'Modèles disponibles : {available_models}')\n",
    "\n",
    "# Vérifier la cohérence des y_test\n",
    "ref_true = y_tests[available_models[0]]\n",
    "for name in available_models[1:]:\n",
    "    diff = np.abs(y_tests[name] - ref_true[:len(y_tests[name])]).max()\n",
    "    print(f'  Diff max y_test {available_models[0]} vs {name}: {diff:.8f}')\n",
    "    \n",
    "# Utiliser le y_test de référence\n",
    "y_true_ensemble = ref_true\n",
    "n_test = len(y_true_ensemble)\n",
    "print(f'\\nTaille test set : {n_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Stratégie 1 : Moyenne simple ---\n",
    "pred_stack = np.stack([preds[name][:n_test] for name in available_models])  # (n_models, N, 2)\n",
    "y_mean = pred_stack.mean(axis=0)\n",
    "res_mean = evaluate('Ensemble moyenne simple', y_true_ensemble, y_mean)\n",
    "print()\n",
    "\n",
    "# --- Stratégie 2 : Moyenne pondérée (poids ∝ 1/erreur) ---\n",
    "weights = []\n",
    "for name in available_models:\n",
    "    eucl = np.sqrt((y_true_ensemble[:, 0] - preds[name][:n_test, 0])**2 + \n",
    "                   (y_true_ensemble[:, 1] - preds[name][:n_test, 1])**2).mean()\n",
    "    weights.append(1.0 / eucl)\n",
    "    \n",
    "weights = np.array(weights)\n",
    "weights = weights / weights.sum()\n",
    "print(f'Poids calculés : {dict(zip(available_models, [f\"{w:.3f}\" for w in weights]))}')\n",
    "\n",
    "y_weighted = np.zeros_like(y_true_ensemble)\n",
    "for i, name in enumerate(available_models):\n",
    "    y_weighted += weights[i] * preds[name][:n_test]\n",
    "\n",
    "res_weighted = evaluate('Ensemble moyenne pondérée', y_true_ensemble, y_weighted)\n",
    "print()\n",
    "\n",
    "# --- Stratégie 3 : Stacking avec Ridge ---\n",
    "# On utilise la première moitié du test set pour entraîner le méta-modèle,\n",
    "# et la seconde moitié pour évaluer.\n",
    "# (Idéalement on utiliserait un val set séparé, mais on fait avec ce qu'on a)\n",
    "half = n_test // 2\n",
    "\n",
    "# Construire la matrice de features : concaténer les prédictions de chaque modèle\n",
    "X_stack = np.concatenate([preds[name][:n_test] for name in available_models], axis=1)  # (N, 2*n_models)\n",
    "\n",
    "X_stack_train = X_stack[:half]\n",
    "y_stack_train = y_true_ensemble[:half]\n",
    "X_stack_test = X_stack[half:]\n",
    "y_stack_test = y_true_ensemble[half:]\n",
    "\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_stack_train, y_stack_train)\n",
    "y_stacked = ridge.predict(X_stack_test)\n",
    "\n",
    "res_stacked = evaluate('Ensemble stacking (Ridge, 2e moitié test)', y_stack_test, y_stacked)\n",
    "\n",
    "# Afficher les coefficients du méta-modèle\n",
    "print(f'\\nCoefficients Ridge :')\n",
    "for i, name in enumerate(available_models):\n",
    "    print(f'  {name}: coef_x={ridge.coef_[0, 2*i]:.3f}, coef_y={ridge.coef_[1, 2*i+1]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Stratégie bonus : Ensemble + Kalman vitesse ---\n",
    "# On applique le meilleur Kalman vitesse sur la moyenne pondérée\n",
    "y_ensemble_kalman = kalman_velocity(y_weighted, \n",
    "                                     process_noise_pos=0.001, \n",
    "                                     process_noise_vel=0.02,\n",
    "                                     measurement_noise=0.02)\n",
    "res_ensemble_kalman = evaluate('Ensemble pondéré + Kalman vitesse', y_true_ensemble, y_ensemble_kalman)\n",
    "\n",
    "# Sauvegarder les prédictions ensemble\n",
    "np.save('../outputs/preds_ensemble_weighted.npy', y_weighted)\n",
    "np.save('../outputs/preds_ensemble_kalman_vel.npy', y_ensemble_kalman)\n",
    "print('\\nPrédictions ensemble sauvegardées.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de l'ensemble\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "seg = slice(0, 300)\n",
    "\n",
    "# Trajectoire\n",
    "ax = axes[0]\n",
    "ax.plot(y_true_ensemble[seg, 0], y_true_ensemble[seg, 1], 'b-', alpha=0.5, label='Vérité', linewidth=1.5)\n",
    "for name in available_models:\n",
    "    ax.plot(preds[name][seg, 0], preds[name][seg, 1], '-', alpha=0.2, linewidth=0.5, label=name)\n",
    "ax.plot(y_weighted[seg, 0], y_weighted[seg, 1], 'orange', alpha=0.6, label='Ensemble pondéré', linewidth=1.2)\n",
    "ax.plot(y_ensemble_kalman[seg, 0], y_ensemble_kalman[seg, 1], 'g-', label='Ensemble + Kalman vel', linewidth=1.5)\n",
    "ax.set_xlabel('X'); ax.set_ylabel('Y')\n",
    "ax.set_title('Trajectoires')\n",
    "ax.legend(fontsize=8); ax.set_aspect('equal')\n",
    "\n",
    "# CDF\n",
    "ax = axes[1]\n",
    "for name, yp, color in [\n",
    "    *[(n, preds[n][:n_test], None) for n in available_models],\n",
    "    ('Ensemble pondéré', y_weighted, 'orange'),\n",
    "    ('Ensemble + Kalman vel', y_ensemble_kalman, 'green'),\n",
    "]:\n",
    "    e = np.sort(np.sqrt((y_true_ensemble[:, 0] - yp[:, 0])**2 + (y_true_ensemble[:, 1] - yp[:, 1])**2))\n",
    "    kwargs = {'color': color} if color else {}\n",
    "    lw = 2.5 if color else 1\n",
    "    ax.plot(e, np.linspace(0, 1, len(e)), label=name, linewidth=lw, **kwargs)\n",
    "ax.set_xlabel('Erreur euclidienne'); ax.set_ylabel('CDF')\n",
    "ax.set_title('CDF des erreurs')\n",
    "ax.legend(fontsize=8); ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Barplot\n",
    "ax = axes[2]\n",
    "all_names = available_models + ['Ensemble\\npondéré', 'Ensemble\\n+ Kalman vel']\n",
    "all_preds_list = [preds[n][:n_test] for n in available_models] + [y_weighted, y_ensemble_kalman]\n",
    "all_means = []\n",
    "for yp in all_preds_list:\n",
    "    e = np.sqrt((y_true_ensemble[:, 0] - yp[:, 0])**2 + (y_true_ensemble[:, 1] - yp[:, 1])**2)\n",
    "    all_means.append(e.mean())\n",
    "\n",
    "colors = ['#66c2a5', '#fc8d62', '#8da0cb', '#e78ac3', '#a6d854']\n",
    "bars = ax.bar(range(len(all_names)), all_means, color=colors[:len(all_names)], edgecolor='black')\n",
    "ax.set_xticks(range(len(all_names))); ax.set_xticklabels(all_names, fontsize=9)\n",
    "ax.set_ylabel('Erreur euclidienne moyenne')\n",
    "ax.set_title('Comparaison')\n",
    "for bar, val in zip(bars, all_means):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.002,\n",
    "            f'{val:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.suptitle('Ensemble de modèles', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Partie 3 : GRU multi-fenêtre (contexte temporel)\n",
    "\n",
    "## Pourquoi ?\n",
    "\n",
    "Tous les modèles précédents traitent chaque fenêtre de 108ms **indépendamment**. Or la position à l'instant t est fortement corrélée à la position à t-1, t-2, etc. Un GRU (Gated Recurrent Unit) peut apprendre cette continuité temporelle.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "1. **Encodeur de fenêtre** : on réutilise le Transformer pré-entraîné du notebook 2 comme extracteur de features. Pour chaque fenêtre, il produit un vecteur de dimension D=64.\n",
    "2. **GRU** : prend la séquence de K vecteurs consécutifs → apprend la dynamique temporelle\n",
    "3. **Readout** : Dense → (x, y)\n",
    "\n",
    "On freeze le Transformer (pas de fine-tuning) pour éviter l'overfitting, et on n'entraîne que le GRU + readout.\n",
    "\n",
    "**Alternative plus légère** : au lieu de réutiliser le Transformer, on peut utiliser les features XGBoost (déjà calculées) comme entrée du GRU. C'est beaucoup plus rapide et ne nécessite pas de GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Chargement des données et extraction des features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Charger les données brutes\nprint('Chargement des données...')\nif USE_S3:\n    with fs.open(f\"{S3_BUCKET}/{S3_PREFIX}/{PARQUET_NAME}\", \"rb\") as f:\n        df = pd.read_parquet(f)\n    with fs.open(f\"{S3_BUCKET}/{S3_PREFIX}/{JSON_NAME}\", \"r\") as f:\n        params = json.load(f)\nelse:\n    df = pd.read_parquet(LOCAL_PARQUET)\n    with open(LOCAL_JSON, 'r') as f:\n        params = json.load(f)\n\nnGroups = params['nGroups']\nnChannelsPerGroup = [params[f'group{g}']['nChannels'] for g in range(nGroups)]\n\nspeed_masks = np.array([x[0] for x in df['speedMask']])\ndf_moving = df[speed_masks].reset_index(drop=True)\nprint(f'Exemples en mouvement : {len(df_moving)}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraire les features (même code que notebook 1)\n",
    "def extract_features(row, nGroups, nChannelsPerGroup):\n",
    "    features = {}\n",
    "    total_spikes = 0\n",
    "    all_amplitudes = []\n",
    "    \n",
    "    for g in range(nGroups):\n",
    "        nCh = nChannelsPerGroup[g]\n",
    "        raw = row[f'group{g}']\n",
    "        waveforms = raw.reshape(-1, nCh, 32)\n",
    "        n_spikes = waveforms.shape[0]\n",
    "        \n",
    "        features[f'shank{g}_n_spikes'] = n_spikes\n",
    "        total_spikes += n_spikes\n",
    "        \n",
    "        if n_spikes > 0:\n",
    "            amplitudes = waveforms.max(axis=2) - waveforms.min(axis=2)  # (n_spikes, nCh)\n",
    "            max_amp_per_spike = amplitudes.max(axis=1)  # (n_spikes,)\n",
    "            all_amplitudes.extend(max_amp_per_spike)\n",
    "            \n",
    "            features[f'shank{g}_amp_mean'] = amplitudes.mean()\n",
    "            features[f'shank{g}_amp_max'] = amplitudes.max()\n",
    "            features[f'shank{g}_amp_std'] = amplitudes.std()\n",
    "            features[f'shank{g}_energy'] = (waveforms ** 2).mean()\n",
    "            features[f'shank{g}_dominant_ch'] = amplitudes.mean(axis=0).argmax()\n",
    "            \n",
    "            for ch in range(nCh):\n",
    "                features[f'shank{g}_ch{ch}_amp'] = amplitudes[:, ch].mean()\n",
    "        else:\n",
    "            features[f'shank{g}_amp_mean'] = 0\n",
    "            features[f'shank{g}_amp_max'] = 0\n",
    "            features[f'shank{g}_amp_std'] = 0\n",
    "            features[f'shank{g}_energy'] = 0\n",
    "            features[f'shank{g}_dominant_ch'] = 0\n",
    "            for ch in range(nCh):\n",
    "                features[f'shank{g}_ch{ch}_amp'] = 0\n",
    "    \n",
    "    features['total_spikes'] = total_spikes\n",
    "    features['length'] = len(row['groups'])\n",
    "    \n",
    "    idx = row['indexInDat']\n",
    "    if len(idx) > 1:\n",
    "        diffs = np.diff(idx.astype(float))\n",
    "        features['isi_mean'] = diffs.mean()\n",
    "        features['isi_std'] = diffs.std()\n",
    "        features['isi_median'] = np.median(diffs)\n",
    "        features['temporal_spread'] = idx[-1] - idx[0]\n",
    "    else:\n",
    "        features['isi_mean'] = 0\n",
    "        features['isi_std'] = 0\n",
    "        features['isi_median'] = 0\n",
    "        features['temporal_spread'] = 0\n",
    "    \n",
    "    if total_spikes > 0:\n",
    "        for g in range(nGroups):\n",
    "            features[f'ratio_shank{g}'] = features[f'shank{g}_n_spikes'] / total_spikes\n",
    "    else:\n",
    "        for g in range(nGroups):\n",
    "            features[f'ratio_shank{g}'] = 0\n",
    "    \n",
    "    return features\n",
    "\n",
    "print('Extraction des features...')\n",
    "features_list = []\n",
    "for idx in range(len(df_moving)):\n",
    "    if idx % 5000 == 0:\n",
    "        print(f'  {idx}/{len(df_moving)}')\n",
    "    features_list.append(extract_features(df_moving.iloc[idx], nGroups, nChannelsPerGroup))\n",
    "\n",
    "features_df = pd.DataFrame(features_list).fillna(0)\n",
    "print(f'Features : {features_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparer les targets\n",
    "targets = np.array([[row['pos'][0], row['pos'][1]] for _, row in df_moving.iterrows()], dtype=np.float32)\n",
    "\n",
    "# Normaliser les features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "features_array = scaler.fit_transform(features_df.values).astype(np.float32)\n",
    "\n",
    "print(f'Features: {features_array.shape}, Targets: {targets.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Dataset séquentiel (fenêtres glissantes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_LEN = 10  # Nombre de fenêtres consécutives en entrée\n",
    "\n",
    "class SequentialWindowDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Pour chaque index t, retourne les features des fenêtres [t-K+1, ..., t]\n",
    "    et le target de la fenêtre t.\n",
    "    \"\"\"\n",
    "    def __init__(self, features, targets, context_len=CONTEXT_LEN):\n",
    "        self.features = features  # (N, D)\n",
    "        self.targets = targets    # (N, 2)\n",
    "        self.context_len = context_len\n",
    "        self.n_samples = len(features) - context_len + 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Séquence de features : [idx, idx+1, ..., idx+context_len-1]\n",
    "        feat_seq = self.features[idx : idx + self.context_len]  # (context_len, D)\n",
    "        target = self.targets[idx + self.context_len - 1]       # target de la dernière fenêtre\n",
    "        return torch.from_numpy(feat_seq), torch.from_numpy(target)\n",
    "\n",
    "\n",
    "# Split temporel\n",
    "split_idx = int(len(features_array) * 0.8)\n",
    "\n",
    "train_features = features_array[:split_idx]\n",
    "train_targets = targets[:split_idx]\n",
    "test_features = features_array[split_idx:]\n",
    "test_targets = targets[split_idx:]\n",
    "\n",
    "# Le scaler doit être fit seulement sur le train set\n",
    "scaler_train = StandardScaler()\n",
    "train_features = scaler_train.fit_transform(train_features).astype(np.float32)\n",
    "test_features = scaler_train.transform(test_features).astype(np.float32)\n",
    "\n",
    "train_dataset = SequentialWindowDataset(train_features, train_targets, CONTEXT_LEN)\n",
    "test_dataset = SequentialWindowDataset(test_features, test_targets, CONTEXT_LEN)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f'Context length: {CONTEXT_LEN} fenêtres (~{CONTEXT_LEN * 108}ms)')\n",
    "print(f'Feature dim: {train_features.shape[1]}')\n",
    "print(f'Train: {len(train_dataset)} séquences, Test: {len(test_dataset)} séquences')\n",
    "\n",
    "# Vérification\n",
    "feat_seq, tgt = train_dataset[0]\n",
    "print(f'\\nBatch test: features={feat_seq.shape}, target={tgt.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c. Architecture GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalGRU(nn.Module):\n",
    "    \"\"\"\n",
    "    GRU qui prend une séquence de vecteurs de features (une par fenêtre de 108ms)\n",
    "    et prédit la position (x, y) de la dernière fenêtre.\n",
    "    \n",
    "    Architecture :\n",
    "    - Projection linéaire des features vers dim cachée\n",
    "    - GRU bidirectionnel (2 couches)\n",
    "    - On prend le dernier hidden state → Dense → (x, y)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=128, num_layers=2, dropout=0.3, bidirectional=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_proj = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        \n",
    "        self.gru = nn.GRU(\n",
    "            input_size=hidden_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "        \n",
    "        gru_output_dim = hidden_dim * (2 if bidirectional else 1)\n",
    "        \n",
    "        self.output_head = nn.Sequential(\n",
    "            nn.Linear(gru_output_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 2),  # (x, y)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, context_len, input_dim)\n",
    "        x = self.input_proj(x)          # (batch, context_len, hidden_dim)\n",
    "        output, h_n = self.gru(x)       # output: (batch, context_len, gru_output_dim)\n",
    "        last_output = output[:, -1, :]  # Dernier timestep (batch, gru_output_dim)\n",
    "        return self.output_head(last_output)  # (batch, 2)\n",
    "\n",
    "\n",
    "input_dim = train_features.shape[1]\n",
    "model_gru = TemporalGRU(input_dim=input_dim, hidden_dim=128, num_layers=2, dropout=0.3)\n",
    "n_params = sum(p.numel() for p in model_gru.parameters())\n",
    "print(f'GRU : {n_params:,} paramètres')\n",
    "print(model_gru)\n",
    "\n",
    "# Test forward\n",
    "test_input = torch.randn(4, CONTEXT_LEN, input_dim)\n",
    "test_output = model_gru(test_input)\n",
    "print(f'\\nTest: input {test_input.shape} → output {test_output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3d. Entraînement du GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-3\n",
    "EPOCHS = 50\n",
    "PATIENCE = 10\n",
    "\n",
    "model_gru = TemporalGRU(input_dim=input_dim, hidden_dim=128, num_layers=2, dropout=0.3).to(DEVICE)\n",
    "\n",
    "optimizer = optim.AdamW(model_gru.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=LR, epochs=EPOCHS, steps_per_epoch=len(train_loader)\n",
    ")\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(f'Entraînement GRU sur {DEVICE} pour {EPOCHS} epochs (patience={PATIENCE})')\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Train\n",
    "    model_gru.train()\n",
    "    epoch_loss = 0\n",
    "    n_batches = 0\n",
    "    for feat_seq, tgt in train_loader:\n",
    "        feat_seq = feat_seq.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        pred = model_gru(feat_seq)\n",
    "        loss = criterion(pred, tgt)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model_gru.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        n_batches += 1\n",
    "    \n",
    "    train_loss = epoch_loss / n_batches\n",
    "    \n",
    "    # Eval\n",
    "    model_gru.eval()\n",
    "    val_loss_sum = 0\n",
    "    val_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for feat_seq, tgt in test_loader:\n",
    "            feat_seq = feat_seq.to(DEVICE)\n",
    "            tgt = tgt.to(DEVICE)\n",
    "            pred = model_gru(feat_seq)\n",
    "            val_loss_sum += criterion(pred, tgt).item()\n",
    "            val_batches += 1\n",
    "    \n",
    "    val_loss = val_loss_sum / val_batches\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    lr_current = optimizer.param_groups[0]['lr']\n",
    "    print(f'Epoch {epoch+1:02d}/{EPOCHS} | Train: {train_loss:.5f} | Val: {val_loss:.5f} | LR: {lr_current:.6f}')\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model_gru.state_dict(), '../outputs/best_gru.pt')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f'Early stopping à epoch {epoch+1}')\n",
    "            break\n",
    "\n",
    "print(f'\\nMeilleure val loss: {best_val_loss:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courbes d'entraînement\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(train_losses, label='Train', linewidth=2)\n",
    "ax.plot(val_losses, label='Validation', linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('MSE Loss')\n",
    "ax.set_title('Courbes d\\'entraînement - GRU multi-fenêtre')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3e. Évaluation du GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le meilleur modèle\n",
    "model_gru.load_state_dict(torch.load('../outputs/best_gru.pt', map_location=DEVICE, weights_only=True))\n",
    "model_gru.eval()\n",
    "\n",
    "all_preds_gru = []\n",
    "all_true_gru = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for feat_seq, tgt in test_loader:\n",
    "        feat_seq = feat_seq.to(DEVICE)\n",
    "        pred = model_gru(feat_seq)\n",
    "        all_preds_gru.append(pred.cpu().numpy())\n",
    "        all_true_gru.append(tgt.numpy())\n",
    "\n",
    "y_pred_gru = np.concatenate(all_preds_gru)\n",
    "y_true_gru = np.concatenate(all_true_gru)\n",
    "\n",
    "res_gru = evaluate('GRU multi-fenêtre', y_true_gru, y_pred_gru)\n",
    "\n",
    "# Appliquer Kalman vitesse\n",
    "y_pred_gru_kalman = kalman_velocity(y_pred_gru, 0.001, 0.02, 0.02)\n",
    "res_gru_kalman = evaluate('GRU + Kalman vitesse', y_true_gru, y_pred_gru_kalman)\n",
    "\n",
    "# Sauvegarder\n",
    "np.save('../outputs/preds_gru.npy', y_pred_gru)\n",
    "np.save('../outputs/y_test_gru.npy', y_true_gru)\n",
    "np.save('../outputs/preds_gru_kalman_vel.npy', y_pred_gru_kalman)\n",
    "print('\\nPrédictions GRU sauvegardées.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisations GRU\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "seg = slice(0, 500)\n",
    "\n",
    "eucl_gru = res_gru['eucl']\n",
    "eucl_gru_k = res_gru_kalman['eucl'] if 'eucl' in res_gru_kalman else np.sqrt(\n",
    "    (y_true_gru[:, 0] - y_pred_gru_kalman[:, 0])**2 + (y_true_gru[:, 1] - y_pred_gru_kalman[:, 1])**2)\n",
    "\n",
    "# Scatter\n",
    "ax = axes[0, 0]\n",
    "ax.scatter(y_true_gru[:, 0], y_pred_gru[:, 0], s=1, alpha=0.3)\n",
    "ax.plot([0, 1], [0, 1], 'r--', linewidth=2)\n",
    "ax.set_xlabel('True X'); ax.set_ylabel('Pred X')\n",
    "ax.set_title(f'GRU - Position X (R²={res_gru[\"r2_x\"]:.3f})')\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "ax = axes[0, 1]\n",
    "ax.scatter(y_true_gru[:, 1], y_pred_gru[:, 1], s=1, alpha=0.3)\n",
    "ax.plot([0, 1], [0, 1], 'r--', linewidth=2)\n",
    "ax.set_xlabel('True Y'); ax.set_ylabel('Pred Y')\n",
    "ax.set_title(f'GRU - Position Y (R²={res_gru[\"r2_y\"]:.3f})')\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "# Trajectoire\n",
    "ax = axes[1, 0]\n",
    "ax.plot(y_true_gru[seg, 0], y_true_gru[seg, 1], 'b-', alpha=0.5, label='Vérité', linewidth=1.5)\n",
    "ax.plot(y_pred_gru[seg, 0], y_pred_gru[seg, 1], 'r-', alpha=0.4, label='GRU brut', linewidth=1)\n",
    "ax.plot(y_pred_gru_kalman[seg, 0], y_pred_gru_kalman[seg, 1], 'g-', alpha=0.7, label='GRU + Kalman vel', linewidth=1.5)\n",
    "ax.set_xlabel('X'); ax.set_ylabel('Y')\n",
    "ax.set_title('Trajectoire (500 points test)')\n",
    "ax.legend(); ax.set_aspect('equal')\n",
    "\n",
    "# CDF\n",
    "ax = axes[1, 1]\n",
    "for label, errors, color in [\n",
    "    ('GRU brut', eucl_gru, 'red'),\n",
    "    ('GRU + Kalman vel', eucl_gru_k, 'green'),\n",
    "]:\n",
    "    sorted_e = np.sort(errors)\n",
    "    ax.plot(sorted_e, np.linspace(0, 1, len(sorted_e)), label=label, linewidth=2, color=color)\n",
    "ax.set_xlabel('Erreur euclidienne'); ax.set_ylabel('CDF')\n",
    "ax.set_title('CDF des erreurs')\n",
    "ax.legend(); ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'GRU multi-fenêtre (contexte = {CONTEXT_LEN} × 108ms = {CONTEXT_LEN * 108}ms)',\n",
    "             fontsize=14, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Résumé et sauvegarde finale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=== RÉSUMÉ DES AMÉLIORATIONS ===')\n",
    "print()\n",
    "\n",
    "# Partie 1 : Kalman vitesse\n",
    "print('--- Partie 1 : Kalman vitesse constante ---')\n",
    "for model_name in preds:\n",
    "    y_true = y_tests[model_name]\n",
    "    y_pred_brut = preds[model_name]\n",
    "    y_pos = kalman_position(y_pred_brut, 0.001, 0.02)\n",
    "    y_vel = kalman_velocity(y_pred_brut, 0.001, 0.02, 0.02)\n",
    "    \n",
    "    e_brut = np.sqrt((y_true[:, 0] - y_pred_brut[:, 0])**2 + (y_true[:, 1] - y_pred_brut[:, 1])**2).mean()\n",
    "    e_pos = np.sqrt((y_true[:, 0] - y_pos[:, 0])**2 + (y_true[:, 1] - y_pos[:, 1])**2).mean()\n",
    "    e_vel = np.sqrt((y_true[:, 0] - y_vel[:, 0])**2 + (y_true[:, 1] - y_vel[:, 1])**2).mean()\n",
    "    \n",
    "    print(f'  {model_name:15s} : brut={e_brut:.4f}, Kalman pos={e_pos:.4f}, Kalman vel={e_vel:.4f}')\n",
    "\n",
    "print()\n",
    "print('--- Partie 2 : Ensemble ---')\n",
    "e_weighted = np.sqrt((y_true_ensemble[:, 0] - y_weighted[:, 0])**2 + (y_true_ensemble[:, 1] - y_weighted[:, 1])**2).mean()\n",
    "e_ens_kalman = np.sqrt((y_true_ensemble[:, 0] - y_ensemble_kalman[:, 0])**2 + (y_true_ensemble[:, 1] - y_ensemble_kalman[:, 1])**2).mean()\n",
    "print(f'  Moyenne pondérée           : {e_weighted:.4f}')\n",
    "print(f'  Moyenne pondérée + Kalman  : {e_ens_kalman:.4f}')\n",
    "\n",
    "print()\n",
    "print('--- Partie 3 : GRU multi-fenêtre ---')\n",
    "e_gru = res_gru['eucl'].mean()\n",
    "e_gru_k = np.sqrt((y_true_gru[:, 0] - y_pred_gru_kalman[:, 0])**2 + (y_true_gru[:, 1] - y_pred_gru_kalman[:, 1])**2).mean()\n",
    "print(f'  GRU brut                   : {e_gru:.4f}')\n",
    "print(f'  GRU + Kalman vitesse       : {e_gru_k:.4f}')\n",
    "\n",
    "print()\n",
    "print('Fichiers sauvegardés :')\n",
    "for f in ['../outputs/preds_xgboost_kalman_vel.npy', '../outputs/preds_transformer_kalman_vel.npy', '../outputs/preds_cnn_kalman_vel.npy',\n",
    "          '../outputs/preds_ensemble_weighted.npy', '../outputs/preds_ensemble_kalman_vel.npy',\n",
    "          '../outputs/preds_gru.npy', '../outputs/y_test_gru.npy', '../outputs/preds_gru_kalman_vel.npy',\n",
    "          '../outputs/best_gru.pt']:\n",
    "    exists = 'OK' if os.path.exists(f) else 'MANQUANT'\n",
    "    print(f'  {f}: {exists}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interprétation\n",
    "\n",
    "### Kalman vitesse vs position\n",
    "\n",
    "Le modèle de vitesse constante devrait mieux gérer les phases de mouvement rapide, car il **anticipe** la position suivante au lieu de simplement lisser. Le gain est surtout visible sur :\n",
    "- Les virages (le modèle de position constante a du retard dans les virages)\n",
    "- Les accélérations/décélérations\n",
    "\n",
    "### Ensemble\n",
    "\n",
    "La moyenne pondérée est simple mais efficace : chaque modèle contribue proportionnellement à sa fiabilité. Le stacking (Ridge) peut aller plus loin en apprenant des pondérations non-uniformes et des corrections de biais.\n",
    "\n",
    "### GRU multi-fenêtre\n",
    "\n",
    "C'est conceptuellement l'amélioration la plus profonde : le modèle **apprend** la continuité temporelle au lieu de la poser comme hypothèse (Kalman). Le GRU peut capturer des patterns complexes comme :\n",
    "- Décélération à l'approche d'un mur\n",
    "- Schémas de navigation récurrents (aller-retour dans le U-maze)\n",
    "- Corrélation entre l'activité neuronale récente et la trajectoire future\n",
    "\n",
    "### Limites\n",
    "\n",
    "- Le GRU utilise les features XGBoost → il hérite de leurs limitations (pas de waveforms bruts)\n",
    "- Le contexte de 10 fenêtres (~1 seconde) est arbitraire — on pourrait tester d'autres valeurs\n",
    "- L'ensemble nécessite que tous les modèles aient le même test set et les mêmes indices\n",
    "\n",
    "### Amélioration ultime : GRU sur embeddings Transformer\n",
    "\n",
    "Pour aller plus loin, on pourrait extraire les embeddings du Transformer (le vecteur après masked average pooling, dim=64) pour chaque fenêtre, puis les passer au GRU. Cela combinerait la richesse des waveforms bruts (Transformer) avec la continuité temporelle (GRU)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}