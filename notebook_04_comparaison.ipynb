{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Notebook 4 : Comparaison des modèles\n\nCe notebook charge les prédictions sauvegardées par les notebooks 1 à 5 et les compare sur les mêmes métriques.\n\n**Prérequis** : avoir exécuté les notebooks 1, 2, 3 et 5 en entier. Chacun sauvegarde ses prédictions dans des fichiers `.npy`."
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import os\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement des prédictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Charger toutes les prédictions\nmodels = {}\n\n# --- Notebook 1 : ML classique ---\ny_test_ml = np.load('y_test.npy')\nfor name, fname in [('XGBoost', 'preds_xgboost.npy'), \n                     ('RandomForest', 'preds_rf.npy'),\n                     ('GradientBoosting', 'preds_gb.npy')]:\n    if os.path.exists(fname):\n        models[name] = {'pred': np.load(fname), 'true': y_test_ml}\n        print(f'{name}: chargé ({models[name][\"pred\"].shape})')\n\n# Notebook 1 : Kalman position\nif os.path.exists('preds_xgboost_kalman.npy'):\n    models['XGBoost + Kalman pos'] = {\n        'pred': np.load('preds_xgboost_kalman.npy'),\n        'true': y_test_ml\n    }\n    print(f'XGBoost + Kalman pos: chargé')\n\n# --- Notebook 2 : Transformer ---\nif os.path.exists('preds_transformer.npy'):\n    y_test_tr = np.load('y_test_transformer.npy')\n    models['Transformer'] = {\n        'pred': np.load('preds_transformer.npy'),\n        'true': y_test_tr\n    }\n    print(f'Transformer: chargé')\n\nif os.path.exists('preds_transformer_kalman.npy'):\n    models['Transformer + Kalman pos'] = {\n        'pred': np.load('preds_transformer_kalman.npy'),\n        'true': y_test_tr\n    }\n    print(f'Transformer + Kalman pos: chargé')\n\n# --- Notebook 3 : CNN ---\nif os.path.exists('preds_cnn.npy'):\n    y_test_cnn = np.load('y_test_cnn.npy')\n    models['CNN'] = {\n        'pred': np.load('preds_cnn.npy'),\n        'true': y_test_cnn\n    }\n    print(f'CNN: chargé')\n\nif os.path.exists('preds_cnn_kalman.npy'):\n    models['CNN + Kalman pos'] = {\n        'pred': np.load('preds_cnn_kalman.npy'),\n        'true': y_test_cnn\n    }\n    print(f'CNN + Kalman pos: chargé')\n\n# --- Notebook 5 : Améliorations ---\n# Kalman vitesse\nfor name, fname, true_file in [\n    ('XGBoost + Kalman vel', 'preds_xgboost_kalman_vel.npy', 'y_test.npy'),\n    ('Transformer + Kalman vel', 'preds_transformer_kalman_vel.npy', 'y_test_transformer.npy'),\n    ('CNN + Kalman vel', 'preds_cnn_kalman_vel.npy', 'y_test_cnn.npy'),\n]:\n    if os.path.exists(fname):\n        models[name] = {'pred': np.load(fname), 'true': np.load(true_file)}\n        print(f'{name}: chargé')\n\n# Ensemble\nif os.path.exists('preds_ensemble_weighted.npy'):\n    models['Ensemble pondéré'] = {\n        'pred': np.load('preds_ensemble_weighted.npy'),\n        'true': y_test_ml  # même test set\n    }\n    print(f'Ensemble pondéré: chargé')\n\nif os.path.exists('preds_ensemble_kalman_vel.npy'):\n    models['Ensemble + Kalman vel'] = {\n        'pred': np.load('preds_ensemble_kalman_vel.npy'),\n        'true': y_test_ml\n    }\n    print(f'Ensemble + Kalman vel: chargé')\n\n# GRU\nif os.path.exists('preds_gru.npy'):\n    y_test_gru = np.load('y_test_gru.npy')\n    models['GRU multi-fenêtre'] = {\n        'pred': np.load('preds_gru.npy'),\n        'true': y_test_gru\n    }\n    print(f'GRU multi-fenêtre: chargé ({y_test_gru.shape})')\n\nif os.path.exists('preds_gru_kalman_vel.npy'):\n    models['GRU + Kalman vel'] = {\n        'pred': np.load('preds_gru_kalman_vel.npy'),\n        'true': y_test_gru\n    }\n    print(f'GRU + Kalman vel: chargé')\n\nprint(f'\\n{len(models)} modèles chargés : {list(models.keys())}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tableau comparatif des métriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "results = []\n\nfor name, data in models.items():\n    y_true = data['true']\n    y_pred = data['pred']\n    \n    # Gérer les tailles différentes (GRU a un test set plus court à cause du contexte)\n    n = min(len(y_true), len(y_pred))\n    y_true = y_true[:n]\n    y_pred = y_pred[:n]\n    \n    eucl = np.sqrt((y_true[:, 0] - y_pred[:, 0])**2 + (y_true[:, 1] - y_pred[:, 1])**2)\n    \n    results.append({\n        'Modèle': name,\n        'N': n,\n        'MSE_X': mean_squared_error(y_true[:, 0], y_pred[:, 0]),\n        'MSE_Y': mean_squared_error(y_true[:, 1], y_pred[:, 1]),\n        'MAE_X': mean_absolute_error(y_true[:, 0], y_pred[:, 0]),\n        'MAE_Y': mean_absolute_error(y_true[:, 1], y_pred[:, 1]),\n        'R²_X': r2_score(y_true[:, 0], y_pred[:, 0]),\n        'R²_Y': r2_score(y_true[:, 1], y_pred[:, 1]),\n        'Eucl_mean': eucl.mean(),\n        'Eucl_median': np.median(eucl),\n        'Eucl_p90': np.percentile(eucl, 90),\n    })\n\n# Trier par erreur euclidienne moyenne (meilleur en haut)\nresults.sort(key=lambda r: r['Eucl_mean'])\n\n# Affichage formaté\nprint(f'{\"Modèle\":<28} {\"R²_X\":>7} {\"R²_Y\":>7} {\"Eucl_mean\":>10} {\"Eucl_med\":>10} {\"Eucl_p90\":>10}  {\"N\":>5}')\nprint('-' * 90)\nfor r in results:\n    print(f'{r[\"Modèle\"]:<28} {r[\"R²_X\"]:>7.4f} {r[\"R²_Y\"]:>7.4f} {r[\"Eucl_mean\"]:>10.4f} {r[\"Eucl_median\"]:>10.4f} {r[\"Eucl_p90\"]:>10.4f}  {r[\"N\"]:>5}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Barplot des métriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "model_names = [r['Modèle'] for r in results]\nn_models = len(model_names)\ncolors = plt.cm.tab20(np.linspace(0, 1, n_models))\n\nfig, axes = plt.subplots(1, 3, figsize=(20, 6))\n\n# R² moyen (X + Y) / 2\nr2_avg = [(r['R²_X'] + r['R²_Y']) / 2 for r in results]\nbars = axes[0].barh(model_names, r2_avg, color=colors, edgecolor='black', linewidth=0.5)\naxes[0].set_xlabel('R² moyen (X, Y)')\naxes[0].set_title('R² moyen')\nfor bar, val in zip(bars, r2_avg):\n    axes[0].text(bar.get_width() + 0.005, bar.get_y() + bar.get_height()/2.,\n                 f'{val:.3f}', ha='left', va='center', fontsize=8)\n\n# Erreur euclidienne moyenne\neucl_means = [r['Eucl_mean'] for r in results]\nbars = axes[1].barh(model_names, eucl_means, color=colors, edgecolor='black', linewidth=0.5)\naxes[1].set_xlabel('Erreur euclidienne moyenne')\naxes[1].set_title('Erreur euclidienne moyenne')\nfor bar, val in zip(bars, eucl_means):\n    axes[1].text(bar.get_width() + 0.002, bar.get_y() + bar.get_height()/2.,\n                 f'{val:.4f}', ha='left', va='center', fontsize=8)\n\n# Erreur euclidienne p90\neucl_p90s = [r['Eucl_p90'] for r in results]\nbars = axes[2].barh(model_names, eucl_p90s, color=colors, edgecolor='black', linewidth=0.5)\naxes[2].set_xlabel('Erreur euclidienne p90')\naxes[2].set_title('Erreur euclidienne (90e percentile)')\nfor bar, val in zip(bars, eucl_p90s):\n    axes[2].text(bar.get_width() + 0.002, bar.get_y() + bar.get_height()/2.,\n                 f'{val:.4f}', ha='left', va='center', fontsize=8)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. CDF des erreurs euclidiennes (tous modèles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "fig, ax = plt.subplots(figsize=(12, 7))\n\nfor i, (name, data) in enumerate(models.items()):\n    y_true = data['true']\n    y_pred = data['pred']\n    n = min(len(y_true), len(y_pred))\n    errors = np.sqrt((y_true[:n, 0] - y_pred[:n, 0])**2 + (y_true[:n, 1] - y_pred[:n, 1])**2)\n    sorted_errors = np.sort(errors)\n    cdf = np.arange(1, len(sorted_errors) + 1) / len(sorted_errors)\n    ax.plot(sorted_errors, cdf, label=name, linewidth=2, color=colors[i])\n\nax.set_xlabel('Erreur euclidienne')\nax.set_ylabel('CDF (proportion ≤ erreur)')\nax.set_title('Distribution cumulée des erreurs - Comparaison de tous les modèles')\nax.legend(fontsize=8, loc='lower right')\nax.grid(True, alpha=0.3)\nax.set_xlim(0, 0.7)\nax.axhline(0.5, color='gray', linestyle=':', alpha=0.5)\nax.axhline(0.9, color='gray', linestyle=':', alpha=0.5)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Trajectoires prédites superposées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# On prend les 300 premiers points du test set\n# On sélectionne les modèles les plus intéressants pour la trajectoire\nseg = slice(0, 300)\nref_true = list(models.values())[0]['true']\n\n# Sélectionner un sous-ensemble pour lisibilité\nhighlight_models = ['XGBoost', 'Transformer', 'CNN']\nkalman_models = [n for n in models if 'Kalman vel' in n or 'Ensemble' in n or 'GRU' in n]\n# On affiche les bruts + les améliorations\n\nfig, axes = plt.subplots(1, 2, figsize=(18, 7))\n\n# Trajectoire 2D\naxes[0].plot(ref_true[seg, 0], ref_true[seg, 1], 'k-', linewidth=2.5, alpha=0.8, label='Vrai', zorder=10)\nfor i, (name, data) in enumerate(models.items()):\n    pred = data['pred']\n    n = min(len(pred), len(ref_true))\n    if seg.stop <= n:\n        lw = 1.8 if name in kalman_models else 0.8\n        alpha = 0.7 if name in kalman_models else 0.3\n        axes[0].plot(pred[seg, 0], pred[seg, 1], '-', linewidth=lw, alpha=alpha, \n                     color=colors[i], label=name)\n\naxes[0].set_xlabel('X')\naxes[0].set_ylabel('Y')\naxes[0].set_title('Trajectoire - 300 premiers points test')\naxes[0].legend(fontsize=7, loc='best')\naxes[0].set_aspect('equal')\n\n# Erreur au cours du temps (top 5 modèles seulement)\ntop5 = [r['Modèle'] for r in results[:5]]\nfor i, (name, data) in enumerate(models.items()):\n    if name in top5:\n        y_true = data['true']\n        y_pred = data['pred']\n        n = min(len(y_true), len(y_pred))\n        if seg.stop <= n:\n            errors = np.sqrt((y_true[:n, 0] - y_pred[:n, 0])**2 + (y_true[:n, 1] - y_pred[:n, 1])**2)\n            axes[1].plot(errors[seg], linewidth=1, alpha=0.8, color=colors[i], label=name)\n\naxes[1].set_xlabel('Index (test set)')\naxes[1].set_ylabel('Erreur euclidienne')\naxes[1].set_title('Erreur au cours du temps - Top 5 modèles')\naxes[1].legend(fontsize=8)\naxes[1].grid(True, alpha=0.2)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Heatmaps d'erreur spatiale (tous modèles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Heatmaps limitées aux modèles clés (un par famille + meilleurs)\nheatmap_models = ['XGBoost', 'Transformer', 'CNN', 'Transformer + Kalman pos']\n\n# Ajouter les modèles du notebook 5 s'ils existent\nfor name in ['Transformer + Kalman vel', 'Ensemble + Kalman vel', 'GRU multi-fenêtre']:\n    if name in models:\n        heatmap_models.append(name)\n\n# Filtrer ceux qui existent\nheatmap_models = [n for n in heatmap_models if n in models]\n\nn_hm = len(heatmap_models)\nfig, axes = plt.subplots(1, n_hm, figsize=(4.5 * n_hm, 5))\nif n_hm == 1:\n    axes = [axes]\n\nnbins = 20\nx_edges = np.linspace(0, 1, nbins + 1)\ny_edges = np.linspace(0, 1, nbins + 1)\n\n# Calculer les error maps\nall_error_maps = []\nfor name in heatmap_models:\n    data = models[name]\n    y_true = data['true']\n    y_pred = data['pred']\n    n = min(len(y_true), len(y_pred))\n    eucl = np.sqrt((y_true[:n, 0] - y_pred[:n, 0])**2 + (y_true[:n, 1] - y_pred[:n, 1])**2)\n    \n    error_map = np.full((nbins, nbins), np.nan)\n    count_map = np.zeros((nbins, nbins))\n    for i in range(n):\n        xi = np.clip(np.searchsorted(x_edges, y_true[i, 0]) - 1, 0, nbins - 1)\n        yi = np.clip(np.searchsorted(y_edges, y_true[i, 1]) - 1, 0, nbins - 1)\n        if np.isnan(error_map[yi, xi]):\n            error_map[yi, xi] = 0\n        error_map[yi, xi] += eucl[i]\n        count_map[yi, xi] += 1\n    \n    mean_map = np.where(count_map > 0, error_map / count_map, np.nan)\n    all_error_maps.append(mean_map)\n\n# Limites communes\nall_vals = np.concatenate([m[~np.isnan(m)] for m in all_error_maps])\nvmin, vmax = np.percentile(all_vals, 5), np.percentile(all_vals, 95)\n\nfor idx, (name, error_map) in enumerate(zip(heatmap_models, all_error_maps)):\n    im = axes[idx].imshow(error_map, origin='lower', aspect='equal', cmap='RdYlGn_r',\n                           extent=[0, 1, 0, 1], vmin=vmin, vmax=vmax)\n    axes[idx].set_xlabel('X')\n    axes[idx].set_ylabel('Y') if idx == 0 else None\n    axes[idx].set_title(name, fontsize=10)\n\nplt.colorbar(im, ax=axes, label='Erreur euclidienne moyenne', shrink=0.8)\nplt.suptitle('Erreur spatiale par position - Modèles clés', fontsize=14)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Scatter pred vs true (tous modèles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Scatter plots limités aux modèles clés\nscatter_models = ['XGBoost', 'Transformer', 'CNN']\nfor name in ['Transformer + Kalman vel', 'Ensemble + Kalman vel', 'GRU multi-fenêtre']:\n    if name in models:\n        scatter_models.append(name)\n\nn_sc = len(scatter_models)\nfig, axes = plt.subplots(n_sc, 2, figsize=(10, 4 * n_sc))\nif n_sc == 1:\n    axes = axes.reshape(1, -1)\n\nfor idx, name in enumerate(scatter_models):\n    if name not in models:\n        continue\n    data = models[name]\n    y_true = data['true']\n    y_pred = data['pred']\n    n = min(len(y_true), len(y_pred))\n    r2x = r2_score(y_true[:n, 0], y_pred[:n, 0])\n    r2y = r2_score(y_true[:n, 1], y_pred[:n, 1])\n    \n    axes[idx, 0].scatter(y_true[:n, 0], y_pred[:n, 0], s=0.5, alpha=0.2)\n    axes[idx, 0].plot([0, 1], [0, 1], 'r--', linewidth=2)\n    axes[idx, 0].set_xlabel('True X')\n    axes[idx, 0].set_ylabel('Pred X')\n    axes[idx, 0].set_title(f'{name} - X (R²={r2x:.3f})')\n    axes[idx, 0].set_aspect('equal')\n    \n    axes[idx, 1].scatter(y_true[:n, 1], y_pred[:n, 1], s=0.5, alpha=0.2)\n    axes[idx, 1].plot([0, 1], [0, 1], 'r--', linewidth=2)\n    axes[idx, 1].set_xlabel('True Y')\n    axes[idx, 1].set_ylabel('Pred Y')\n    axes[idx, 1].set_title(f'{name} - Y (R²={r2y:.3f})')\n    axes[idx, 1].set_aspect('equal')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Discussion et conclusion\n\n### Résumé des approches\n\n| Notebook | Approche | Entrée | Points forts |\n|----------|----------|--------|-------------|\n| **NB1** | Feature Eng. + XGBoost/RF | ~35 features manuelles | Rapide, interprétable |\n| **NB2** | Transformer | Séquence de waveforms bruts | Capture co-activations via self-attention |\n| **NB3** | CNN bins temporels | Image (20ch × 22 bins) | Entrée fixe, patterns spatio-temporels |\n| **NB5** | Kalman vitesse | Post-processing (état = [x,y,vx,vy]) | Anticipe le mouvement, meilleur que Kalman position |\n| **NB5** | Ensemble pondéré | Combinaison de tous les modèles | Exploite la complémentarité |\n| **NB5** | GRU multi-fenêtre | Séquence de 10 fenêtres de features | Apprend la continuité temporelle |\n\n### Analyse des résultats\n\n- **R²** : proportion de la variance expliquée (1 = parfait, 0 = aussi bon que la moyenne)\n- **Erreur euclidienne** : en unités normalisées (0-1). L'arène ~40cm → erreur 0.1 ≈ 4cm\n- **p90** : 90% des prédictions sous ce seuil — mesure les worst cases\n\n### Impact des améliorations (notebook 5)\n\n1. **Kalman vitesse vs position** : le modèle de vitesse constante anticipe les mouvements au lieu de simplement lisser. Le gain est surtout visible sur le p90 (pires cas) et en R² pendant les phases de mouvement rapide.\n\n2. **Ensemble** : la moyenne pondérée exploite la complémentarité entre modèles. Même une simple moyenne réduit la variance des prédictions. L'ensemble + Kalman vitesse combine le meilleur des deux mondes.\n\n3. **GRU multi-fenêtre** : en utilisant le contexte temporel (~1 seconde), le GRU peut apprendre la dynamique de la trajectoire de manière data-driven, sans hypothèse de modèle physique comme le Kalman.\n\n### Classement attendu (du meilleur au moins bon)\n\n1. GRU multi-fenêtre + Kalman vitesse (ou Ensemble + Kalman vitesse)\n2. Transformer + Kalman vitesse\n3. Transformer + Kalman position\n4. Ensemble pondéré\n5. Transformer brut\n6. XGBoost + Kalman\n7. XGBoost / GradientBoosting brut\n8. CNN brut\n\n### Pistes restantes\n\n- **GRU sur embeddings Transformer** : extraire le vecteur dim=64 du Transformer (au lieu des features XGBoost) pour combiner richesse des waveforms + continuité temporelle\n- **Multi-tâche** : prédire aussi vitesse et direction de tête\n- **Data augmentation** : bruit sur waveforms, masquage de spikes/shanks\n- **Plus d'epochs** : le Transformer passe de 5 à 30 epochs (à exécuter sur GPU)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}